import os
import re
import statistics
import tempfile
import logging
import subprocess
from typing import List, Optional, Dict, Any, Tuple, Union
from dataclasses import dataclass, field
from pathlib import Path
from collections import namedtuple
from os import PathLike
import numpy as np
import iree.runtime as rt
from iree.runtime import VmModule
from iree.runtime.benchmark import (
    benchmark_module,
    benchmark_exe,
    BenchmarkTimeoutError,
    BenchmarkToolError,
)


__all__ = ["BenchmarkConfig", "BenchmarkResult", "BenchmarkRunner"]

DTYPE_TO_ABI_TYPE = {
    np.dtype(np.float32): "f32",
    np.dtype(np.int32): "i32",
    np.dtype(np.int64): "i64",
    np.dtype(np.float64): "f64",
    np.dtype(np.int16): "i16",
    np.dtype(np.float16): "f16",
    np.dtype(np.int8): "i8",
    np.dtype(np.bool_): "i1",
}


@dataclass
class BenchmarkConfig:
    """configuration for benchmark runs"""

    num_runs: int = 10
    timeout: Optional[float] = None
    entry_function: Optional[str] = None
    data_dir: str = "./benchmark_data"
    cleanup_after: bool = False
    verbose: bool = False
    min_runs_for_stats: int = 3
    benchmark_flags: Dict[str, Any] = field(default_factory=dict)
    statistical_analysis: bool = True


@dataclass
class BenchmarkResult:
    """container for benchmark results with statistical analysis"""

    # åŸºç¡€ä¿¡æ¯
    benchmark_name: str
    config_used: BenchmarkConfig

    # è¿è¡Œç»“æžœ
    num_runs: int
    successful_runs: int
    failed_runs: int

    # åŽŸå§‹IREEç»“æžœ
    raw_iree_results: List[rt.benchmark.BenchmarkResult]

    # æ—¶é—´ç»Ÿè®¡ (è§£æžåŽçš„æ•°å€¼)
    time_values: List[float]
    cpu_time_values: List[float]

    # ç»Ÿè®¡æŒ‡æ ‡
    mean_time: float = 0.0
    median_time: float = 0.0
    min_time: float = 0.0
    max_time: float = 0.0
    std_dev_time: Optional[float] = None
    cv_time: Optional[float] = None

    mean_cpu_time: float = 0.0
    median_cpu_time: float = 0.0
    min_cpu_time: float = 0.0
    max_cpu_time: float = 0.0
    std_dev_cpu_time: Optional[float] = None
    cv_cpu_time: Optional[float] = None

    # æ€§èƒ½æŒ‡æ ‡
    throughput: float = 0.0
    cpu_throughput: float = 0.0
    stability_rating: str = "N/A"

    # é”™è¯¯ä¿¡æ¯
    error_details: List[str] = field(default_factory=list)


class BenchmarkRunner:
    """benchmark runner with IREE compatibility and advanced features"""

    def __init__(self, config: Optional[BenchmarkConfig] = None):
        self.config = config or BenchmarkConfig()
        self.logger = logging.getLogger(__name__)
        self._setup_logging()

    def _setup_logging(self):
        """Setup logging configuration"""
        if self.config.verbose:
            logging.basicConfig(level=logging.INFO)
        else:
            logging.basicConfig(level=logging.WARNING)

    def _get_benchmark_exe(self) -> str:
        """Get the path to the IREE benchmark executable"""
        return benchmark_exe()

    def _prepare_inputs_for_iree(self, inputs: List[Union[str]]) -> List[str]:
        """
        Prepare inputs in IREE benchmark format
        Compatible with official IREE benchmark_module input format
        """
        formatted_inputs = []

        for inp in inputs:
            if isinstance(inp, str):
                # å­—ç¬¦ä¸²è¾“å…¥ï¼Œå¯èƒ½æ˜¯æ–‡ä»¶è·¯å¾„æˆ–æ ¼å¼åŒ–å­—ç¬¦ä¸²
                formatted_inputs.append(f"@{inp}")
                continue

            # numpyæ•°ç»„è¾“å…¥ï¼ŒæŠ¥é”™ï¼Œbenchmarkåªèƒ½æŽ¥æ”¶åœ°å€
            else:
                raise ValueError(f"Unsupported input type: {type(inp)}")

        return formatted_inputs

    def _save_inputs_to_files(self, inputs: List[np.ndarray]) -> List[str]:
        """Save numpy arrays to files and return file paths"""

        save_dir = self.config.data_dir
        os.makedirs(save_dir, exist_ok=True)

        file_paths = []
        for i, inp in enumerate(inputs):
            if isinstance(inp, np.ndarray):
                file_path = os.path.join(save_dir, f"input_{i}.npy")
                np.save(file_path, inp)
                file_paths.append(os.path.abspath(file_path))

                if self.config.verbose:
                    self.logger.info(f"Saved input {i} to: {file_path}")

        return file_paths

    def _run_single_benchmark(
        self,
        module: Union[VmModule, PathLike],
        entry_function: str = None,
        inputs: List[str] = [],
        device: str = "local-task",
    ) -> list[rt.benchmark.BenchmarkResult]:
        """
        Run a single benchmark using IREE's benchmark tool
        """
        return benchmark_module(module, entry_function, inputs, device=device)

    def _parse_time_value(self, time_str: str) -> Optional[float]:
        """
        Parse time value from IREE benchmark output
        Handles various formats like "123.45 ms", "1.23 us", etc.
        """
        if not isinstance(time_str, str):
            return None

        # åŒ¹é…æ•°å­—å’Œå•ä½çš„æ­£åˆ™è¡¨è¾¾å¼
        pattern = r"(\d+\.?\d*)\s*(ns|us|ms|s)?"
        match = re.search(pattern, time_str.lower())

        if not match:
            return None

        value = float(match.group(1))
        unit = match.group(2) or "s"  # é»˜è®¤ä¸ºç§’

        # è½¬æ¢ä¸ºç§’
        unit_multipliers = {"ns": 1e-9, "us": 1e-6, "ms": 1e-3, "s": 1.0}

        return value * unit_multipliers.get(unit, 1.0)

    def _calculate_statistics(self, values: List[float]) -> Dict[str, float]:
        """Calculate statistical measures"""
        if not values:
            return {}

        stats = {
            "mean": statistics.mean(values),
            "median": statistics.median(values),
            "min": min(values),
            "max": max(values),
        }

        if len(values) > 1:
            stats["std_dev"] = statistics.stdev(values)
            stats["cv"] = stats["std_dev"] / stats["mean"] if stats["mean"] > 0 else 0

        stats["throughput"] = 1.0 / stats["mean"] if stats["mean"] > 0 else 0
        return stats

    def _get_stability_rating(self, cv: Optional[float]) -> str:
        """Get stability rating based on coefficient of variation"""
        if cv is None:
            return "N/A"

        if cv < 0.05:
            return "ä¼˜ç§€"
        elif cv < 0.1:
            return "è‰¯å¥½"
        elif cv < 0.2:
            return "ä¸€èˆ¬"
        else:
            return "è¾ƒå·®"

    def run_benchmark(
        self,
        module: Union[VmModule, PathLike],
        entry_function: str = None,
        inputs: Union[List[np.ndarray], List[str]] = [],
        benchmark_name: str = "benchmark",
        device: str = "local-task",
    ) -> BenchmarkResult:
        """
        Run enhanced benchmark with statistical analysis

        Args:
            moduleIREE VmModule or path to module file:
            inputs: Input data (numpy arrays or formatted strings)
            benchmark_name: Name for this benchmark

        Returns:
            EnhancedBenchmarkResult with detailed statistics
        """
        if self.config.verbose:
            self.logger.info(f"å¼€å§‹åŸºå‡†æµ‹è¯•: {benchmark_name}")
            self.logger.info(f"é…ç½®: {self.config.num_runs} æ¬¡è¿è¡Œ")

        # å‡†å¤‡è¾“å…¥
        if inputs and isinstance(inputs[0], np.ndarray):
            # ä¿å­˜æ–‡ä»¶
            saved_files = self._save_inputs_to_files(inputs)
            # è½¬æ¢ä¸ºIREEæ ¼å¼
            formatted_inputs = self._prepare_inputs_for_iree(saved_files)
        elif inputs and isinstance(inputs[0], str):
            formatted_inputs = inputs
            saved_files = []
        else:
            raise ValueError(f"Unsupported type: {type(inputs[0])}")

        # è¿è¡ŒåŸºå‡†æµ‹è¯•
        raw_results = []
        successful_runs = 0
        error_details = []

        for i in range(self.config.num_runs):
            if self.config.verbose:
                self.logger.info(f"è¿è¡Œ {i+1}/{self.config.num_runs}, ...")

            try:
                result = self._run_single_benchmark(
                    module,
                    entry_function=entry_function,
                    inputs=formatted_inputs,
                    device=device,
                )[0]
                # print(f"result: {result}")
                raw_results.append(result)
                successful_runs += 1

                if self.config.verbose:
                    self.logger.info(f"å®Œæˆ (æ—¶é—´: {result.time})")

            except Exception as e:
                error_msg = f"è¿è¡Œ {i+1} å¤±è´¥: {str(e)}"
                error_details.append(error_msg)

                if self.config.verbose:
                    self.logger.error(f"å¤±è´¥: {e}")

                self.logger.error(error_msg)

        # è§£æžæ—¶é—´å€¼
        time_values = []
        cpu_time_values = []

        for result in raw_results:
            time_val = self._parse_time_value(result.time)
            cpu_time_val = self._parse_time_value(result.cpu_time)

            if time_val is not None:
                time_values.append(time_val)
            if cpu_time_val is not None:
                cpu_time_values.append(cpu_time_val)

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        time_stats = self._calculate_statistics(time_values)
        cpu_time_stats = self._calculate_statistics(cpu_time_values)

        # åˆ›å»ºç»“æžœå¯¹è±¡
        result = BenchmarkResult(
            benchmark_name=benchmark_name,
            config_used=self.config,
            num_runs=self.config.num_runs,
            successful_runs=successful_runs,
            failed_runs=self.config.num_runs - successful_runs,
            raw_iree_results=raw_results,
            time_values=time_values,
            cpu_time_values=cpu_time_values,
            # æ—¶é—´ç»Ÿè®¡
            mean_time=time_stats.get("mean", 0),
            median_time=time_stats.get("median", 0),
            min_time=time_stats.get("min", 0),
            max_time=time_stats.get("max", 0),
            std_dev_time=time_stats.get("std_dev"),
            cv_time=time_stats.get("cv"),
            # CPUæ—¶é—´ç»Ÿè®¡
            mean_cpu_time=cpu_time_stats.get("mean", 0),
            median_cpu_time=cpu_time_stats.get("median", 0),
            min_cpu_time=cpu_time_stats.get("min", 0),
            max_cpu_time=cpu_time_stats.get("max", 0),
            std_dev_cpu_time=cpu_time_stats.get("std_dev"),
            cv_cpu_time=cpu_time_stats.get("cv"),
            # æ€§èƒ½æŒ‡æ ‡
            throughput=time_stats.get("throughput", 0),
            cpu_throughput=cpu_time_stats.get("throughput", 0),
            stability_rating=self._get_stability_rating(time_stats.get("cv")),
            error_details=error_details,
        )

        # æ¸…ç†æ–‡ä»¶
        if self.config.cleanup_after and saved_files:
            self._cleanup_files(saved_files)

        return result

    def _cleanup_files(self, file_paths: List[str]):
        """Clean up saved files"""
        for file_path in file_paths:
            try:
                os.remove(file_path)
                if self.config.verbose:
                    self.logger.info(f"å·²æ¸…ç†æ–‡ä»¶: {file_path}")
            except OSError as e:
                self.logger.warning(f"æ¸…ç†æ–‡ä»¶å¤±è´¥ {file_path}: {e}")

    def print_results(self, result: BenchmarkResult):
        """Print formatted enhanced benchmark results"""
        print("\n" + "=" * 70)
        print("å¢žå¼ºåŸºå‡†æµ‹è¯•ç»“æžœ:")
        print("=" * 70)

        print(f"ðŸ“‹ åŸºå‡†æµ‹è¯•åç§°: {result.benchmark_name}")
        print(f"âœ… æˆåŠŸè¿è¡Œ: {result.successful_runs}/{result.num_runs}")

        if result.failed_runs > 0:
            print(f"âŒ å¤±è´¥è¿è¡Œ: {result.failed_runs}")
            print("ðŸ” é”™è¯¯è¯¦æƒ…:")
            for error in result.error_details:
                print(f"   {error}")

        if result.successful_runs == 0:
            print("âŒ æ‰€æœ‰åŸºå‡†æµ‹è¯•è¿è¡Œéƒ½å¤±è´¥äº†")
            return

        print()
        print("â±ï¸  å¢™é’Ÿæ—¶é—´ç»Ÿè®¡:")
        print(f"   å¹³å‡å€¼: {result.mean_time:.6f} ç§’")
        print(f"   ä¸­ä½æ•°: {result.median_time:.6f} ç§’")
        print(f"   æœ€å°å€¼: {result.min_time:.6f} ç§’")
        print(f"   æœ€å¤§å€¼: {result.max_time:.6f} ç§’")

        if result.std_dev_time is not None:
            print(f"   æ ‡å‡†å·®: {result.std_dev_time:.6f} ç§’")
            print(f"   å˜å¼‚ç³»æ•°: {result.cv_time*100:.2f}%")

        print()
        print("ðŸ–¥ï¸  CPUæ—¶é—´ç»Ÿè®¡:")
        print(f"   å¹³å‡å€¼: {result.mean_cpu_time:.6f} ç§’")
        print(f"   ä¸­ä½æ•°: {result.median_cpu_time:.6f} ç§’")
        print(f"   æœ€å°å€¼: {result.min_cpu_time:.6f} ç§’")
        print(f"   æœ€å¤§å€¼: {result.max_cpu_time:.6f} ç§’")

        if result.std_dev_cpu_time is not None:
            print(f"   æ ‡å‡†å·®: {result.std_dev_cpu_time:.6f} ç§’")
            print(f"   å˜å¼‚ç³»æ•°: {result.cv_cpu_time*100:.2f}%")

        print()
        print("ðŸ“Š æ€§èƒ½æŒ‡æ ‡:")
        print(f"   å¢™é’Ÿåžåé‡: {result.throughput:.2f} æ¬¡/ç§’")
        print(f"   CPUåžåé‡: {result.cpu_throughput:.2f} æ¬¡/ç§’")
        print(f"   ç¨³å®šæ€§è¯„çº§: {result.stability_rating}")

    def print_result_simple(self, result: BenchmarkResult):
        """Print simplified benchmark results focusing on average times"""
        print(
            f"ðŸ“‹ {result.benchmark_name} | âœ… {result.successful_runs}/{result.num_runs}"
        )

        if result.successful_runs == 0:
            print("âŒ æ‰€æœ‰åŸºå‡†æµ‹è¯•è¿è¡Œéƒ½å¤±è´¥äº†")
            return

        print(f"â±ï¸  å¹³å‡å¢™é’Ÿæ—¶é—´: {result.mean_time:.6f} ç§’")
        print(f"ðŸ–¥ï¸  å¹³å‡CPUæ—¶é—´: {result.mean_cpu_time:.6f} ç§’")

        if result.failed_runs > 0:
            print(f"âŒ å¤±è´¥è¿è¡Œ: {result.failed_runs}")
        print("-" * 50)


class ComparisonBenchmark:
    """performance comparison tools"""

    @staticmethod
    def compare_results(
        results: List[BenchmarkResult], metric: str = "mean_time"
    ) -> Dict[str, Any]:
        """Compare multiplebenchmark results"""
        if not results:
            return {}

        values = []
        names = []

        for result in results:
            if hasattr(result, metric):
                values.append(getattr(result, metric))
                names.append(result.benchmark_name)

        if not values:
            return {}

        # å¯¹äºŽæ—¶é—´æŒ‡æ ‡ï¼Œè¶Šå°è¶Šå¥½ï¼›å¯¹äºŽåžåé‡æŒ‡æ ‡ï¼Œè¶Šå¤§è¶Šå¥½
        is_time_metric = "time" in metric.lower()
        best_idx = values.index(min(values) if is_time_metric else max(values))

        comparison = {
            "metric": metric,
            "results": list(zip(names, values)),
            "best": names[best_idx],
            "best_value": values[best_idx],
            "improvement_ratios": [],
            "is_time_metric": is_time_metric,
        }

        # è®¡ç®—æ”¹è¿›æ¯”çŽ‡
        base_value = values[best_idx]
        for i, value in enumerate(values):
            if value > 0 and base_value > 0:
                if is_time_metric:
                    ratio = value / base_value
                else:
                    ratio = base_value / value
                comparison["improvement_ratios"].append((names[i], ratio))

        return comparison

    @staticmethod
    def print_comparison(comparison: Dict[str, Any]):
        """Print formatted enhanced comparison results"""
        if not comparison:
            print("æ²¡æœ‰å¯æ¯”è¾ƒçš„ç»“æžœ")
            return

        metric_name = comparison["metric"]
        is_time_metric = comparison["is_time_metric"]

        print("\n" + "=" * 70)
        print(f"å¢žå¼ºæ€§èƒ½æ¯”è¾ƒåˆ†æž (åŸºäºŽ {metric_name}):")
        print("=" * 70)

        print(f"ðŸ† æœ€ä¼˜ç»“æžœ: {comparison['best']}")
        print(f"   {metric_name}: {comparison['best_value']:.6f}")
        print()

        print("ðŸ“ˆ è¯¦ç»†æ¯”è¾ƒ:")
        sorted_results = sorted(
            comparison["results"], key=lambda x: x[1], reverse=not is_time_metric
        )

        for i, (name, value) in enumerate(sorted_results):
            rank_emoji = (
                "ðŸ¥‡" if i == 0 else "ðŸ¥ˆ" if i == 1 else "ðŸ¥‰" if i == 2 else f"{i+1}."
            )
            print(f"   {rank_emoji} {name}: {value:.6f}")

        if comparison["improvement_ratios"]:
            print(f"\nâš¡ æ€§èƒ½æ¯”è¾ƒ (ç›¸å¯¹äºŽæœ€ä¼˜ {comparison['best']}):")
            sorted_ratios = sorted(comparison["improvement_ratios"], key=lambda x: x[1])

            for name, ratio in sorted_ratios:
                if ratio == 1.0:
                    print(f"   âœ¨ {name}: åŸºå‡† (1.00x)")
                elif ratio > 1.0:
                    if is_time_metric:
                        print(f"   ðŸŒ {name}: æ…¢ {ratio:.2f}x")
                    else:
                        print(f"   ðŸš€ {name}: å¿« {ratio:.2f}x")
                else:
                    if is_time_metric:
                        print(f"   ðŸš€ {name}: å¿« {1/ratio:.2f}x")
                    else:
                        print(f"   ðŸŒ {name}: æ…¢ {1/ratio:.2f}x")


# ä¾¿æ·å‡½æ•°
def quick_benchmark(
    module: Union[VmModule, PathLike],
    inputs: List[Union[np.ndarray, str]],
    benchmark_name: str = "quick_test",
    num_runs: int = 10,
    entry_function: Optional[str] = None,
    timeout: Optional[float] = None,
    verbose: bool = True,
) -> BenchmarkResult:
    """
    Quick enhanced benchmark function

    Args:
        module: IREE VmModule or path to module file
        inputs: Input data
        benchmark_name: Name for the benchmark
        num_runs: Number of runs
        entry_function: Entry function name
        timeout: Timeout in seconds
        verbose: Whether to print results

    Returns:
        EnhancedBenchmarkResult
    """
    config = BenchmarkConfig(
        num_runs=num_runs,
        entry_function=entry_function,
        timeout=timeout,
        verbose=verbose,
    )

    runner = BenchmarkRunner(config)
    result = runner.run_benchmark(module, inputs, benchmark_name)

    if verbose:
        runner.print_results(result)

    return result
