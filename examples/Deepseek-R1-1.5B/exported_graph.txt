ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, p_model_layers_0_input_layernorm_weight: "bf16[1536]", p_model_layers_0_post_attention_layernorm_weight: "bf16[1536]", p_model_layers_1_input_layernorm_weight: "bf16[1536]", p_model_layers_1_post_attention_layernorm_weight: "bf16[1536]", p_model_layers_2_input_layernorm_weight: "bf16[1536]", p_model_layers_2_post_attention_layernorm_weight: "bf16[1536]", p_model_layers_3_input_layernorm_weight: "bf16[1536]", p_model_layers_3_post_attention_layernorm_weight: "bf16[1536]", p_model_layers_4_input_layernorm_weight: "bf16[1536]", p_model_layers_4_post_attention_layernorm_weight: "bf16[1536]", p_model_layers_5_input_layernorm_weight: "bf16[1536]", p_model_layers_5_post_attention_layernorm_weight: "bf16[1536]", p_model_layers_6_input_layernorm_weight: "bf16[1536]", p_model_layers_6_post_attention_layernorm_weight: "bf16[1536]", p_model_layers_7_input_layernorm_weight: "bf16[1536]", p_model_layers_7_post_attention_layernorm_weight: "bf16[1536]", p_model_layers_8_input_layernorm_weight: "bf16[1536]", p_model_layers_8_post_attention_layernorm_weight: "bf16[1536]", p_model_layers_9_input_layernorm_weight: "bf16[1536]", p_model_layers_9_post_attention_layernorm_weight: "bf16[1536]", p_model_layers_10_input_layernorm_weight: "bf16[1536]", p_model_layers_10_post_attention_layernorm_weight: "bf16[1536]", p_model_layers_11_input_layernorm_weight: "bf16[1536]", p_model_layers_11_post_attention_layernorm_weight: "bf16[1536]", p_model_layers_12_input_layernorm_weight: "bf16[1536]", p_model_layers_12_post_attention_layernorm_weight: "bf16[1536]", p_model_layers_13_input_layernorm_weight: "bf16[1536]", p_model_layers_13_post_attention_layernorm_weight: "bf16[1536]", p_model_layers_14_input_layernorm_weight: "bf16[1536]", p_model_layers_14_post_attention_layernorm_weight: "bf16[1536]", p_model_layers_15_input_layernorm_weight: "bf16[1536]", p_model_layers_15_post_attention_layernorm_weight: "bf16[1536]", p_model_layers_16_input_layernorm_weight: "bf16[1536]", p_model_layers_16_post_attention_layernorm_weight: "bf16[1536]", p_model_layers_17_input_layernorm_weight: "bf16[1536]", p_model_layers_17_post_attention_layernorm_weight: "bf16[1536]", p_model_layers_18_input_layernorm_weight: "bf16[1536]", p_model_layers_18_post_attention_layernorm_weight: "bf16[1536]", p_model_layers_19_input_layernorm_weight: "bf16[1536]", p_model_layers_19_post_attention_layernorm_weight: "bf16[1536]", p_model_layers_20_input_layernorm_weight: "bf16[1536]", p_model_layers_20_post_attention_layernorm_weight: "bf16[1536]", p_model_layers_21_input_layernorm_weight: "bf16[1536]", p_model_layers_21_post_attention_layernorm_weight: "bf16[1536]", p_model_layers_22_input_layernorm_weight: "bf16[1536]", p_model_layers_22_post_attention_layernorm_weight: "bf16[1536]", p_model_layers_23_input_layernorm_weight: "bf16[1536]", p_model_layers_23_post_attention_layernorm_weight: "bf16[1536]", p_model_layers_24_input_layernorm_weight: "bf16[1536]", p_model_layers_24_post_attention_layernorm_weight: "bf16[1536]", p_model_layers_25_input_layernorm_weight: "bf16[1536]", p_model_layers_25_post_attention_layernorm_weight: "bf16[1536]", p_model_layers_26_input_layernorm_weight: "bf16[1536]", p_model_layers_26_post_attention_layernorm_weight: "bf16[1536]", p_model_layers_27_input_layernorm_weight: "bf16[1536]", p_model_layers_27_post_attention_layernorm_weight: "bf16[1536]", p_model_norm_weight: "bf16[1536]", p_model_embed_tokens_weight: "bf16[151936, 1536]", p_model_layers_0_self_attn_q_proj_weight: "bf16[1536, 1536]", p_model_layers_0_self_attn_q_proj_bias: "bf16[1536]", p_model_layers_0_self_attn_k_proj_weight: "bf16[256, 1536]", p_model_layers_0_self_attn_k_proj_bias: "bf16[256]", p_model_layers_0_self_attn_v_proj_weight: "bf16[256, 1536]", p_model_layers_0_self_attn_v_proj_bias: "bf16[256]", p_model_layers_0_self_attn_o_proj_weight: "bf16[1536, 1536]", p_model_layers_0_mlp_gate_proj_weight: "bf16[8960, 1536]", p_model_layers_0_mlp_up_proj_weight: "bf16[8960, 1536]", p_model_layers_0_mlp_down_proj_weight: "bf16[1536, 8960]", p_model_layers_1_self_attn_q_proj_weight: "bf16[1536, 1536]", p_model_layers_1_self_attn_q_proj_bias: "bf16[1536]", p_model_layers_1_self_attn_k_proj_weight: "bf16[256, 1536]", p_model_layers_1_self_attn_k_proj_bias: "bf16[256]", p_model_layers_1_self_attn_v_proj_weight: "bf16[256, 1536]", p_model_layers_1_self_attn_v_proj_bias: "bf16[256]", p_model_layers_1_self_attn_o_proj_weight: "bf16[1536, 1536]", p_model_layers_1_mlp_gate_proj_weight: "bf16[8960, 1536]", p_model_layers_1_mlp_up_proj_weight: "bf16[8960, 1536]", p_model_layers_1_mlp_down_proj_weight: "bf16[1536, 8960]", p_model_layers_2_self_attn_q_proj_weight: "bf16[1536, 1536]", p_model_layers_2_self_attn_q_proj_bias: "bf16[1536]", p_model_layers_2_self_attn_k_proj_weight: "bf16[256, 1536]", p_model_layers_2_self_attn_k_proj_bias: "bf16[256]", p_model_layers_2_self_attn_v_proj_weight: "bf16[256, 1536]", p_model_layers_2_self_attn_v_proj_bias: "bf16[256]", p_model_layers_2_self_attn_o_proj_weight: "bf16[1536, 1536]", p_model_layers_2_mlp_gate_proj_weight: "bf16[8960, 1536]", p_model_layers_2_mlp_up_proj_weight: "bf16[8960, 1536]", p_model_layers_2_mlp_down_proj_weight: "bf16[1536, 8960]", p_model_layers_3_self_attn_q_proj_weight: "bf16[1536, 1536]", p_model_layers_3_self_attn_q_proj_bias: "bf16[1536]", p_model_layers_3_self_attn_k_proj_weight: "bf16[256, 1536]", p_model_layers_3_self_attn_k_proj_bias: "bf16[256]", p_model_layers_3_self_attn_v_proj_weight: "bf16[256, 1536]", p_model_layers_3_self_attn_v_proj_bias: "bf16[256]", p_model_layers_3_self_attn_o_proj_weight: "bf16[1536, 1536]", p_model_layers_3_mlp_gate_proj_weight: "bf16[8960, 1536]", p_model_layers_3_mlp_up_proj_weight: "bf16[8960, 1536]", p_model_layers_3_mlp_down_proj_weight: "bf16[1536, 8960]", p_model_layers_4_self_attn_q_proj_weight: "bf16[1536, 1536]", p_model_layers_4_self_attn_q_proj_bias: "bf16[1536]", p_model_layers_4_self_attn_k_proj_weight: "bf16[256, 1536]", p_model_layers_4_self_attn_k_proj_bias: "bf16[256]", p_model_layers_4_self_attn_v_proj_weight: "bf16[256, 1536]", p_model_layers_4_self_attn_v_proj_bias: "bf16[256]", p_model_layers_4_self_attn_o_proj_weight: "bf16[1536, 1536]", p_model_layers_4_mlp_gate_proj_weight: "bf16[8960, 1536]", p_model_layers_4_mlp_up_proj_weight: "bf16[8960, 1536]", p_model_layers_4_mlp_down_proj_weight: "bf16[1536, 8960]", p_model_layers_5_self_attn_q_proj_weight: "bf16[1536, 1536]", p_model_layers_5_self_attn_q_proj_bias: "bf16[1536]", p_model_layers_5_self_attn_k_proj_weight: "bf16[256, 1536]", p_model_layers_5_self_attn_k_proj_bias: "bf16[256]", p_model_layers_5_self_attn_v_proj_weight: "bf16[256, 1536]", p_model_layers_5_self_attn_v_proj_bias: "bf16[256]", p_model_layers_5_self_attn_o_proj_weight: "bf16[1536, 1536]", p_model_layers_5_mlp_gate_proj_weight: "bf16[8960, 1536]", p_model_layers_5_mlp_up_proj_weight: "bf16[8960, 1536]", p_model_layers_5_mlp_down_proj_weight: "bf16[1536, 8960]", p_model_layers_6_self_attn_q_proj_weight: "bf16[1536, 1536]", p_model_layers_6_self_attn_q_proj_bias: "bf16[1536]", p_model_layers_6_self_attn_k_proj_weight: "bf16[256, 1536]", p_model_layers_6_self_attn_k_proj_bias: "bf16[256]", p_model_layers_6_self_attn_v_proj_weight: "bf16[256, 1536]", p_model_layers_6_self_attn_v_proj_bias: "bf16[256]", p_model_layers_6_self_attn_o_proj_weight: "bf16[1536, 1536]", p_model_layers_6_mlp_gate_proj_weight: "bf16[8960, 1536]", p_model_layers_6_mlp_up_proj_weight: "bf16[8960, 1536]", p_model_layers_6_mlp_down_proj_weight: "bf16[1536, 8960]", p_model_layers_7_self_attn_q_proj_weight: "bf16[1536, 1536]", p_model_layers_7_self_attn_q_proj_bias: "bf16[1536]", p_model_layers_7_self_attn_k_proj_weight: "bf16[256, 1536]", p_model_layers_7_self_attn_k_proj_bias: "bf16[256]", p_model_layers_7_self_attn_v_proj_weight: "bf16[256, 1536]", p_model_layers_7_self_attn_v_proj_bias: "bf16[256]", p_model_layers_7_self_attn_o_proj_weight: "bf16[1536, 1536]", p_model_layers_7_mlp_gate_proj_weight: "bf16[8960, 1536]", p_model_layers_7_mlp_up_proj_weight: "bf16[8960, 1536]", p_model_layers_7_mlp_down_proj_weight: "bf16[1536, 8960]", p_model_layers_8_self_attn_q_proj_weight: "bf16[1536, 1536]", p_model_layers_8_self_attn_q_proj_bias: "bf16[1536]", p_model_layers_8_self_attn_k_proj_weight: "bf16[256, 1536]", p_model_layers_8_self_attn_k_proj_bias: "bf16[256]", p_model_layers_8_self_attn_v_proj_weight: "bf16[256, 1536]", p_model_layers_8_self_attn_v_proj_bias: "bf16[256]", p_model_layers_8_self_attn_o_proj_weight: "bf16[1536, 1536]", p_model_layers_8_mlp_gate_proj_weight: "bf16[8960, 1536]", p_model_layers_8_mlp_up_proj_weight: "bf16[8960, 1536]", p_model_layers_8_mlp_down_proj_weight: "bf16[1536, 8960]", p_model_layers_9_self_attn_q_proj_weight: "bf16[1536, 1536]", p_model_layers_9_self_attn_q_proj_bias: "bf16[1536]", p_model_layers_9_self_attn_k_proj_weight: "bf16[256, 1536]", p_model_layers_9_self_attn_k_proj_bias: "bf16[256]", p_model_layers_9_self_attn_v_proj_weight: "bf16[256, 1536]", p_model_layers_9_self_attn_v_proj_bias: "bf16[256]", p_model_layers_9_self_attn_o_proj_weight: "bf16[1536, 1536]", p_model_layers_9_mlp_gate_proj_weight: "bf16[8960, 1536]", p_model_layers_9_mlp_up_proj_weight: "bf16[8960, 1536]", p_model_layers_9_mlp_down_proj_weight: "bf16[1536, 8960]", p_model_layers_10_self_attn_q_proj_weight: "bf16[1536, 1536]", p_model_layers_10_self_attn_q_proj_bias: "bf16[1536]", p_model_layers_10_self_attn_k_proj_weight: "bf16[256, 1536]", p_model_layers_10_self_attn_k_proj_bias: "bf16[256]", p_model_layers_10_self_attn_v_proj_weight: "bf16[256, 1536]", p_model_layers_10_self_attn_v_proj_bias: "bf16[256]", p_model_layers_10_self_attn_o_proj_weight: "bf16[1536, 1536]", p_model_layers_10_mlp_gate_proj_weight: "bf16[8960, 1536]", p_model_layers_10_mlp_up_proj_weight: "bf16[8960, 1536]", p_model_layers_10_mlp_down_proj_weight: "bf16[1536, 8960]", p_model_layers_11_self_attn_q_proj_weight: "bf16[1536, 1536]", p_model_layers_11_self_attn_q_proj_bias: "bf16[1536]", p_model_layers_11_self_attn_k_proj_weight: "bf16[256, 1536]", p_model_layers_11_self_attn_k_proj_bias: "bf16[256]", p_model_layers_11_self_attn_v_proj_weight: "bf16[256, 1536]", p_model_layers_11_self_attn_v_proj_bias: "bf16[256]", p_model_layers_11_self_attn_o_proj_weight: "bf16[1536, 1536]", p_model_layers_11_mlp_gate_proj_weight: "bf16[8960, 1536]", p_model_layers_11_mlp_up_proj_weight: "bf16[8960, 1536]", p_model_layers_11_mlp_down_proj_weight: "bf16[1536, 8960]", p_model_layers_12_self_attn_q_proj_weight: "bf16[1536, 1536]", p_model_layers_12_self_attn_q_proj_bias: "bf16[1536]", p_model_layers_12_self_attn_k_proj_weight: "bf16[256, 1536]", p_model_layers_12_self_attn_k_proj_bias: "bf16[256]", p_model_layers_12_self_attn_v_proj_weight: "bf16[256, 1536]", p_model_layers_12_self_attn_v_proj_bias: "bf16[256]", p_model_layers_12_self_attn_o_proj_weight: "bf16[1536, 1536]", p_model_layers_12_mlp_gate_proj_weight: "bf16[8960, 1536]", p_model_layers_12_mlp_up_proj_weight: "bf16[8960, 1536]", p_model_layers_12_mlp_down_proj_weight: "bf16[1536, 8960]", p_model_layers_13_self_attn_q_proj_weight: "bf16[1536, 1536]", p_model_layers_13_self_attn_q_proj_bias: "bf16[1536]", p_model_layers_13_self_attn_k_proj_weight: "bf16[256, 1536]", p_model_layers_13_self_attn_k_proj_bias: "bf16[256]", p_model_layers_13_self_attn_v_proj_weight: "bf16[256, 1536]", p_model_layers_13_self_attn_v_proj_bias: "bf16[256]", p_model_layers_13_self_attn_o_proj_weight: "bf16[1536, 1536]", p_model_layers_13_mlp_gate_proj_weight: "bf16[8960, 1536]", p_model_layers_13_mlp_up_proj_weight: "bf16[8960, 1536]", p_model_layers_13_mlp_down_proj_weight: "bf16[1536, 8960]", p_model_layers_14_self_attn_q_proj_weight: "bf16[1536, 1536]", p_model_layers_14_self_attn_q_proj_bias: "bf16[1536]", p_model_layers_14_self_attn_k_proj_weight: "bf16[256, 1536]", p_model_layers_14_self_attn_k_proj_bias: "bf16[256]", p_model_layers_14_self_attn_v_proj_weight: "bf16[256, 1536]", p_model_layers_14_self_attn_v_proj_bias: "bf16[256]", p_model_layers_14_self_attn_o_proj_weight: "bf16[1536, 1536]", p_model_layers_14_mlp_gate_proj_weight: "bf16[8960, 1536]", p_model_layers_14_mlp_up_proj_weight: "bf16[8960, 1536]", p_model_layers_14_mlp_down_proj_weight: "bf16[1536, 8960]", p_model_layers_15_self_attn_q_proj_weight: "bf16[1536, 1536]", p_model_layers_15_self_attn_q_proj_bias: "bf16[1536]", p_model_layers_15_self_attn_k_proj_weight: "bf16[256, 1536]", p_model_layers_15_self_attn_k_proj_bias: "bf16[256]", p_model_layers_15_self_attn_v_proj_weight: "bf16[256, 1536]", p_model_layers_15_self_attn_v_proj_bias: "bf16[256]", p_model_layers_15_self_attn_o_proj_weight: "bf16[1536, 1536]", p_model_layers_15_mlp_gate_proj_weight: "bf16[8960, 1536]", p_model_layers_15_mlp_up_proj_weight: "bf16[8960, 1536]", p_model_layers_15_mlp_down_proj_weight: "bf16[1536, 8960]", p_model_layers_16_self_attn_q_proj_weight: "bf16[1536, 1536]", p_model_layers_16_self_attn_q_proj_bias: "bf16[1536]", p_model_layers_16_self_attn_k_proj_weight: "bf16[256, 1536]", p_model_layers_16_self_attn_k_proj_bias: "bf16[256]", p_model_layers_16_self_attn_v_proj_weight: "bf16[256, 1536]", p_model_layers_16_self_attn_v_proj_bias: "bf16[256]", p_model_layers_16_self_attn_o_proj_weight: "bf16[1536, 1536]", p_model_layers_16_mlp_gate_proj_weight: "bf16[8960, 1536]", p_model_layers_16_mlp_up_proj_weight: "bf16[8960, 1536]", p_model_layers_16_mlp_down_proj_weight: "bf16[1536, 8960]", p_model_layers_17_self_attn_q_proj_weight: "bf16[1536, 1536]", p_model_layers_17_self_attn_q_proj_bias: "bf16[1536]", p_model_layers_17_self_attn_k_proj_weight: "bf16[256, 1536]", p_model_layers_17_self_attn_k_proj_bias: "bf16[256]", p_model_layers_17_self_attn_v_proj_weight: "bf16[256, 1536]", p_model_layers_17_self_attn_v_proj_bias: "bf16[256]", p_model_layers_17_self_attn_o_proj_weight: "bf16[1536, 1536]", p_model_layers_17_mlp_gate_proj_weight: "bf16[8960, 1536]", p_model_layers_17_mlp_up_proj_weight: "bf16[8960, 1536]", p_model_layers_17_mlp_down_proj_weight: "bf16[1536, 8960]", p_model_layers_18_self_attn_q_proj_weight: "bf16[1536, 1536]", p_model_layers_18_self_attn_q_proj_bias: "bf16[1536]", p_model_layers_18_self_attn_k_proj_weight: "bf16[256, 1536]", p_model_layers_18_self_attn_k_proj_bias: "bf16[256]", p_model_layers_18_self_attn_v_proj_weight: "bf16[256, 1536]", p_model_layers_18_self_attn_v_proj_bias: "bf16[256]", p_model_layers_18_self_attn_o_proj_weight: "bf16[1536, 1536]", p_model_layers_18_mlp_gate_proj_weight: "bf16[8960, 1536]", p_model_layers_18_mlp_up_proj_weight: "bf16[8960, 1536]", p_model_layers_18_mlp_down_proj_weight: "bf16[1536, 8960]", p_model_layers_19_self_attn_q_proj_weight: "bf16[1536, 1536]", p_model_layers_19_self_attn_q_proj_bias: "bf16[1536]", p_model_layers_19_self_attn_k_proj_weight: "bf16[256, 1536]", p_model_layers_19_self_attn_k_proj_bias: "bf16[256]", p_model_layers_19_self_attn_v_proj_weight: "bf16[256, 1536]", p_model_layers_19_self_attn_v_proj_bias: "bf16[256]", p_model_layers_19_self_attn_o_proj_weight: "bf16[1536, 1536]", p_model_layers_19_mlp_gate_proj_weight: "bf16[8960, 1536]", p_model_layers_19_mlp_up_proj_weight: "bf16[8960, 1536]", p_model_layers_19_mlp_down_proj_weight: "bf16[1536, 8960]", p_model_layers_20_self_attn_q_proj_weight: "bf16[1536, 1536]", p_model_layers_20_self_attn_q_proj_bias: "bf16[1536]", p_model_layers_20_self_attn_k_proj_weight: "bf16[256, 1536]", p_model_layers_20_self_attn_k_proj_bias: "bf16[256]", p_model_layers_20_self_attn_v_proj_weight: "bf16[256, 1536]", p_model_layers_20_self_attn_v_proj_bias: "bf16[256]", p_model_layers_20_self_attn_o_proj_weight: "bf16[1536, 1536]", p_model_layers_20_mlp_gate_proj_weight: "bf16[8960, 1536]", p_model_layers_20_mlp_up_proj_weight: "bf16[8960, 1536]", p_model_layers_20_mlp_down_proj_weight: "bf16[1536, 8960]", p_model_layers_21_self_attn_q_proj_weight: "bf16[1536, 1536]", p_model_layers_21_self_attn_q_proj_bias: "bf16[1536]", p_model_layers_21_self_attn_k_proj_weight: "bf16[256, 1536]", p_model_layers_21_self_attn_k_proj_bias: "bf16[256]", p_model_layers_21_self_attn_v_proj_weight: "bf16[256, 1536]", p_model_layers_21_self_attn_v_proj_bias: "bf16[256]", p_model_layers_21_self_attn_o_proj_weight: "bf16[1536, 1536]", p_model_layers_21_mlp_gate_proj_weight: "bf16[8960, 1536]", p_model_layers_21_mlp_up_proj_weight: "bf16[8960, 1536]", p_model_layers_21_mlp_down_proj_weight: "bf16[1536, 8960]", p_model_layers_22_self_attn_q_proj_weight: "bf16[1536, 1536]", p_model_layers_22_self_attn_q_proj_bias: "bf16[1536]", p_model_layers_22_self_attn_k_proj_weight: "bf16[256, 1536]", p_model_layers_22_self_attn_k_proj_bias: "bf16[256]", p_model_layers_22_self_attn_v_proj_weight: "bf16[256, 1536]", p_model_layers_22_self_attn_v_proj_bias: "bf16[256]", p_model_layers_22_self_attn_o_proj_weight: "bf16[1536, 1536]", p_model_layers_22_mlp_gate_proj_weight: "bf16[8960, 1536]", p_model_layers_22_mlp_up_proj_weight: "bf16[8960, 1536]", p_model_layers_22_mlp_down_proj_weight: "bf16[1536, 8960]", p_model_layers_23_self_attn_q_proj_weight: "bf16[1536, 1536]", p_model_layers_23_self_attn_q_proj_bias: "bf16[1536]", p_model_layers_23_self_attn_k_proj_weight: "bf16[256, 1536]", p_model_layers_23_self_attn_k_proj_bias: "bf16[256]", p_model_layers_23_self_attn_v_proj_weight: "bf16[256, 1536]", p_model_layers_23_self_attn_v_proj_bias: "bf16[256]", p_model_layers_23_self_attn_o_proj_weight: "bf16[1536, 1536]", p_model_layers_23_mlp_gate_proj_weight: "bf16[8960, 1536]", p_model_layers_23_mlp_up_proj_weight: "bf16[8960, 1536]", p_model_layers_23_mlp_down_proj_weight: "bf16[1536, 8960]", p_model_layers_24_self_attn_q_proj_weight: "bf16[1536, 1536]", p_model_layers_24_self_attn_q_proj_bias: "bf16[1536]", p_model_layers_24_self_attn_k_proj_weight: "bf16[256, 1536]", p_model_layers_24_self_attn_k_proj_bias: "bf16[256]", p_model_layers_24_self_attn_v_proj_weight: "bf16[256, 1536]", p_model_layers_24_self_attn_v_proj_bias: "bf16[256]", p_model_layers_24_self_attn_o_proj_weight: "bf16[1536, 1536]", p_model_layers_24_mlp_gate_proj_weight: "bf16[8960, 1536]", p_model_layers_24_mlp_up_proj_weight: "bf16[8960, 1536]", p_model_layers_24_mlp_down_proj_weight: "bf16[1536, 8960]", p_model_layers_25_self_attn_q_proj_weight: "bf16[1536, 1536]", p_model_layers_25_self_attn_q_proj_bias: "bf16[1536]", p_model_layers_25_self_attn_k_proj_weight: "bf16[256, 1536]", p_model_layers_25_self_attn_k_proj_bias: "bf16[256]", p_model_layers_25_self_attn_v_proj_weight: "bf16[256, 1536]", p_model_layers_25_self_attn_v_proj_bias: "bf16[256]", p_model_layers_25_self_attn_o_proj_weight: "bf16[1536, 1536]", p_model_layers_25_mlp_gate_proj_weight: "bf16[8960, 1536]", p_model_layers_25_mlp_up_proj_weight: "bf16[8960, 1536]", p_model_layers_25_mlp_down_proj_weight: "bf16[1536, 8960]", p_model_layers_26_self_attn_q_proj_weight: "bf16[1536, 1536]", p_model_layers_26_self_attn_q_proj_bias: "bf16[1536]", p_model_layers_26_self_attn_k_proj_weight: "bf16[256, 1536]", p_model_layers_26_self_attn_k_proj_bias: "bf16[256]", p_model_layers_26_self_attn_v_proj_weight: "bf16[256, 1536]", p_model_layers_26_self_attn_v_proj_bias: "bf16[256]", p_model_layers_26_self_attn_o_proj_weight: "bf16[1536, 1536]", p_model_layers_26_mlp_gate_proj_weight: "bf16[8960, 1536]", p_model_layers_26_mlp_up_proj_weight: "bf16[8960, 1536]", p_model_layers_26_mlp_down_proj_weight: "bf16[1536, 8960]", p_model_layers_27_self_attn_q_proj_weight: "bf16[1536, 1536]", p_model_layers_27_self_attn_q_proj_bias: "bf16[1536]", p_model_layers_27_self_attn_k_proj_weight: "bf16[256, 1536]", p_model_layers_27_self_attn_k_proj_bias: "bf16[256]", p_model_layers_27_self_attn_v_proj_weight: "bf16[256, 1536]", p_model_layers_27_self_attn_v_proj_bias: "bf16[256]", p_model_layers_27_self_attn_o_proj_weight: "bf16[1536, 1536]", p_model_layers_27_mlp_gate_proj_weight: "bf16[8960, 1536]", p_model_layers_27_mlp_up_proj_weight: "bf16[8960, 1536]", p_model_layers_27_mlp_down_proj_weight: "bf16[1536, 8960]", p_lm_head_weight: "bf16[151936, 1536]", b_model_layers_0_self_attn_rotary_emb_cos_cached: "bf16[131072, 128]", b_model_layers_0_self_attn_rotary_emb_sin_cached: "bf16[131072, 128]", b_model_layers_1_self_attn_rotary_emb_cos_cached: "bf16[131072, 128]", b_model_layers_1_self_attn_rotary_emb_sin_cached: "bf16[131072, 128]", b_model_layers_2_self_attn_rotary_emb_cos_cached: "bf16[131072, 128]", b_model_layers_2_self_attn_rotary_emb_sin_cached: "bf16[131072, 128]", b_model_layers_3_self_attn_rotary_emb_cos_cached: "bf16[131072, 128]", b_model_layers_3_self_attn_rotary_emb_sin_cached: "bf16[131072, 128]", b_model_layers_4_self_attn_rotary_emb_cos_cached: "bf16[131072, 128]", b_model_layers_4_self_attn_rotary_emb_sin_cached: "bf16[131072, 128]", b_model_layers_5_self_attn_rotary_emb_cos_cached: "bf16[131072, 128]", b_model_layers_5_self_attn_rotary_emb_sin_cached: "bf16[131072, 128]", b_model_layers_6_self_attn_rotary_emb_cos_cached: "bf16[131072, 128]", b_model_layers_6_self_attn_rotary_emb_sin_cached: "bf16[131072, 128]", b_model_layers_7_self_attn_rotary_emb_cos_cached: "bf16[131072, 128]", b_model_layers_7_self_attn_rotary_emb_sin_cached: "bf16[131072, 128]", b_model_layers_8_self_attn_rotary_emb_cos_cached: "bf16[131072, 128]", b_model_layers_8_self_attn_rotary_emb_sin_cached: "bf16[131072, 128]", b_model_layers_9_self_attn_rotary_emb_cos_cached: "bf16[131072, 128]", b_model_layers_9_self_attn_rotary_emb_sin_cached: "bf16[131072, 128]", b_model_layers_10_self_attn_rotary_emb_cos_cached: "bf16[131072, 128]", b_model_layers_10_self_attn_rotary_emb_sin_cached: "bf16[131072, 128]", b_model_layers_11_self_attn_rotary_emb_cos_cached: "bf16[131072, 128]", b_model_layers_11_self_attn_rotary_emb_sin_cached: "bf16[131072, 128]", b_model_layers_12_self_attn_rotary_emb_cos_cached: "bf16[131072, 128]", b_model_layers_12_self_attn_rotary_emb_sin_cached: "bf16[131072, 128]", b_model_layers_13_self_attn_rotary_emb_cos_cached: "bf16[131072, 128]", b_model_layers_13_self_attn_rotary_emb_sin_cached: "bf16[131072, 128]", b_model_layers_14_self_attn_rotary_emb_cos_cached: "bf16[131072, 128]", b_model_layers_14_self_attn_rotary_emb_sin_cached: "bf16[131072, 128]", b_model_layers_15_self_attn_rotary_emb_cos_cached: "bf16[131072, 128]", b_model_layers_15_self_attn_rotary_emb_sin_cached: "bf16[131072, 128]", b_model_layers_16_self_attn_rotary_emb_cos_cached: "bf16[131072, 128]", b_model_layers_16_self_attn_rotary_emb_sin_cached: "bf16[131072, 128]", b_model_layers_17_self_attn_rotary_emb_cos_cached: "bf16[131072, 128]", b_model_layers_17_self_attn_rotary_emb_sin_cached: "bf16[131072, 128]", b_model_layers_18_self_attn_rotary_emb_cos_cached: "bf16[131072, 128]", b_model_layers_18_self_attn_rotary_emb_sin_cached: "bf16[131072, 128]", b_model_layers_19_self_attn_rotary_emb_cos_cached: "bf16[131072, 128]", b_model_layers_19_self_attn_rotary_emb_sin_cached: "bf16[131072, 128]", b_model_layers_20_self_attn_rotary_emb_cos_cached: "bf16[131072, 128]", b_model_layers_20_self_attn_rotary_emb_sin_cached: "bf16[131072, 128]", b_model_layers_21_self_attn_rotary_emb_cos_cached: "bf16[131072, 128]", b_model_layers_21_self_attn_rotary_emb_sin_cached: "bf16[131072, 128]", b_model_layers_22_self_attn_rotary_emb_cos_cached: "bf16[131072, 128]", b_model_layers_22_self_attn_rotary_emb_sin_cached: "bf16[131072, 128]", b_model_layers_23_self_attn_rotary_emb_cos_cached: "bf16[131072, 128]", b_model_layers_23_self_attn_rotary_emb_sin_cached: "bf16[131072, 128]", b_model_layers_24_self_attn_rotary_emb_cos_cached: "bf16[131072, 128]", b_model_layers_24_self_attn_rotary_emb_sin_cached: "bf16[131072, 128]", b_model_layers_25_self_attn_rotary_emb_cos_cached: "bf16[131072, 128]", b_model_layers_25_self_attn_rotary_emb_sin_cached: "bf16[131072, 128]", b_model_layers_26_self_attn_rotary_emb_cos_cached: "bf16[131072, 128]", b_model_layers_26_self_attn_rotary_emb_sin_cached: "bf16[131072, 128]", b_model_layers_27_self_attn_rotary_emb_cos_cached: "bf16[131072, 128]", b_model_layers_27_self_attn_rotary_emb_sin_cached: "bf16[131072, 128]", input_ids: "i64[1, s0]", attention_mask: "i64[1, s0]"):
             # 
            sym_size_int_64: "Sym(s0)" = torch.ops.aten.sym_size.int(input_ids, 1)
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:878 in forward, code: inputs_embeds = self.embed_tokens(input_ids)
            embedding: "bf16[1, s0, 1536]" = torch.ops.aten.embedding.default(p_model_embed_tokens_weight, input_ids);  p_model_embed_tokens_weight = input_ids = None
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:882 in forward, code: cache_position = torch.arange(
            arange: "i64[s0]" = torch.ops.aten.arange.start(0, sym_size_int_64, device = device(type='cpu'), pin_memory = False)
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:886 in forward, code: position_ids = cache_position.unsqueeze(0)
            unsqueeze: "i64[1, s0]" = torch.ops.aten.unsqueeze.default(arange, 0)
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:99 in _prepare_4d_causal_attention_mask_with_cache_position, code: causal_mask = torch.full((sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device)
            full: "bf16[s0, s0]" = torch.ops.aten.full.default([sym_size_int_64, sym_size_int_64], -3.3895313892515355e+38, dtype = torch.bfloat16, device = device(type='cpu'), pin_memory = False)
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:101 in _prepare_4d_causal_attention_mask_with_cache_position, code: causal_mask = torch.triu(causal_mask, diagonal=1)
            triu: "bf16[s0, s0]" = torch.ops.aten.triu.default(full, 1);  full = None
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:102 in _prepare_4d_causal_attention_mask_with_cache_position, code: causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)
            arange_1: "i64[s0]" = torch.ops.aten.arange.default(sym_size_int_64, device = device(type='cpu'), pin_memory = False)
            reshape: "i64[s0, 1]" = torch.ops.aten.reshape.default(arange, [-1, 1]);  arange = None
            gt: "b8[s0, s0]" = torch.ops.aten.gt.Tensor(arange_1, reshape);  arange_1 = reshape = None
            mul_: "bf16[s0, s0]" = torch.ops.aten.mul_.Tensor(triu, gt);  triu = gt = None
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:103 in _prepare_4d_causal_attention_mask_with_cache_position, code: causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)
            unsqueeze_1: "bf16[1, s0, s0]" = torch.ops.aten.unsqueeze.default(mul_, 0);  mul_ = None
            unsqueeze_2: "bf16[1, 1, s0, s0]" = torch.ops.aten.unsqueeze.default(unsqueeze_1, 1);  unsqueeze_1 = None
            slice_1: "bf16[1, 1, s0, s0]" = torch.ops.aten.slice.Tensor(unsqueeze_2, 2, 0, 9223372036854775807);  unsqueeze_2 = None
            slice_2: "bf16[1, 1, s0, s0]" = torch.ops.aten.slice.Tensor(slice_1, 3, 0, 9223372036854775807);  slice_1 = None
            expand: "bf16[1, 1, s0, s0]" = torch.ops.aten.expand.default(slice_2, [1, 1, -1, -1]);  slice_2 = None
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:105 in _prepare_4d_causal_attention_mask_with_cache_position, code: causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit
            clone: "bf16[1, 1, s0, s0]" = torch.ops.aten.clone.default(expand);  expand = None
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:107 in _prepare_4d_causal_attention_mask_with_cache_position, code: padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]
            slice_3: "bf16[1, 1, s0, s0]" = torch.ops.aten.slice.Tensor(clone, 0, 0, 9223372036854775807)
            slice_4: "bf16[1, 1, s0, s0]" = torch.ops.aten.slice.Tensor(slice_3, 1, 0, 9223372036854775807);  slice_3 = None
            slice_5: "bf16[1, 1, s0, s0]" = torch.ops.aten.slice.Tensor(slice_4, 2, 0, 9223372036854775807);  slice_4 = None
            slice_6: "i64[1, s0]" = torch.ops.aten.slice.Tensor(attention_mask, 0, 0, 9223372036854775807);  attention_mask = None
            unsqueeze_3: "i64[1, 1, s0]" = torch.ops.aten.unsqueeze.default(slice_6, 1);  slice_6 = None
            unsqueeze_4: "i64[1, 1, 1, s0]" = torch.ops.aten.unsqueeze.default(unsqueeze_3, 2);  unsqueeze_3 = None
            slice_7: "i64[1, 1, 1, s0]" = torch.ops.aten.slice.Tensor(unsqueeze_4, 3, 0, 9223372036854775807);  unsqueeze_4 = None
            add: "bf16[1, 1, s0, s0]" = torch.ops.aten.add.Tensor(slice_5, slice_7);  slice_5 = slice_7 = None
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:108 in _prepare_4d_causal_attention_mask_with_cache_position, code: padding_mask = padding_mask == 0
            eq_5: "b8[1, 1, s0, s0]" = torch.ops.aten.eq.Scalar(add, 0);  add = None
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:109 in _prepare_4d_causal_attention_mask_with_cache_position, code: causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(
            slice_8: "bf16[1, 1, s0, s0]" = torch.ops.aten.slice.Tensor(clone, 0, 0, 9223372036854775807)
            slice_9: "bf16[1, 1, s0, s0]" = torch.ops.aten.slice.Tensor(slice_8, 1, 0, 9223372036854775807);  slice_8 = None
            slice_10: "bf16[1, 1, s0, s0]" = torch.ops.aten.slice.Tensor(slice_9, 2, 0, 9223372036854775807);  slice_9 = None
            masked_fill: "bf16[1, 1, s0, s0]" = torch.ops.aten.masked_fill.Scalar(slice_10, eq_5, -3.3895313892515355e+38);  slice_10 = eq_5 = None
            slice_11: "bf16[1, 1, s0, s0]" = torch.ops.aten.slice.Tensor(clone, 0, 0, 9223372036854775807)
            slice_12: "bf16[1, 1, s0, s0]" = torch.ops.aten.slice.Tensor(slice_11, 1, 0, 9223372036854775807);  slice_11 = None
            slice_13: "bf16[1, 1, s0, s0]" = torch.ops.aten.slice.Tensor(slice_12, 2, 0, 9223372036854775807);  slice_12 = None
            copy_: "bf16[1, 1, s0, s0]" = torch.ops.aten.copy_.default(slice_13, masked_fill);  slice_13 = masked_fill = copy_ = None

 -----------------------------------------------------------------------------------------------------------------------


             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:128 in forward, code: hidden_states = hidden_states.to(torch.float32)
            to: "f32[1, s0, 1536]" = torch.ops.aten.to.dtype(embedding, torch.float32)
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:129 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_1: "f32[1, s0, 1536]" = torch.ops.aten.pow.Tensor_Scalar(to, 2)
            mean: "f32[1, s0, 1]" = torch.ops.aten.mean.dim(pow_1, [-1], True);  pow_1 = None
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:130 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_1: "f32[1, s0, 1]" = torch.ops.aten.add.Tensor(mean, 1e-06);  mean = None
            rsqrt: "f32[1, s0, 1]" = torch.ops.aten.rsqrt.default(add_1);  add_1 = None
            mul: "f32[1, s0, 1536]" = torch.ops.aten.mul.Tensor(to, rsqrt);  to = rsqrt = None
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:131 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_1: "bf16[1, s0, 1536]" = torch.ops.aten.to.dtype(mul, torch.bfloat16);  mul = None
            mul_1: "bf16[1, s0, 1536]" = torch.ops.aten.mul.Tensor(p_model_layers_0_input_layernorm_weight, to_1);  p_model_layers_0_input_layernorm_weight = to_1 = None
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:294 in forward, code: query_states = self.q_proj(hidden_states)
            linear: "bf16[1, s0, 1536]" = torch.ops.aten.linear.default(mul_1, p_model_layers_0_self_attn_q_proj_weight, p_model_layers_0_self_attn_q_proj_bias);  p_model_layers_0_self_attn_q_proj_weight = p_model_layers_0_self_attn_q_proj_bias = None
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:295 in forward, code: key_states = self.k_proj(hidden_states)
            linear_1: "bf16[1, s0, 256]" = torch.ops.aten.linear.default(mul_1, p_model_layers_0_self_attn_k_proj_weight, p_model_layers_0_self_attn_k_proj_bias);  p_model_layers_0_self_attn_k_proj_weight = p_model_layers_0_self_attn_k_proj_bias = None
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:296 in forward, code: value_states = self.v_proj(hidden_states)
            linear_2: "bf16[1, s0, 256]" = torch.ops.aten.linear.default(mul_1, p_model_layers_0_self_attn_v_proj_weight, p_model_layers_0_self_attn_v_proj_bias);  mul_1 = p_model_layers_0_self_attn_v_proj_weight = p_model_layers_0_self_attn_v_proj_bias = None
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:298 in forward, code: query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)
            view: "bf16[1, s0, 12, 128]" = torch.ops.aten.view.default(linear, [1, sym_size_int_64, 12, 128]);  linear = None
            transpose: "bf16[1, 12, s0, 128]" = torch.ops.aten.transpose.int(view, 1, 2);  view = None
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:299 in forward, code: key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)
            view_1: "bf16[1, s0, 2, 128]" = torch.ops.aten.view.default(linear_1, [1, sym_size_int_64, 2, 128]);  linear_1 = None
            transpose_1: "bf16[1, 2, s0, 128]" = torch.ops.aten.transpose.int(view_1, 1, 2);  view_1 = None
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:300 in forward, code: value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)
            view_2: "bf16[1, s0, 2, 128]" = torch.ops.aten.view.default(linear_2, [1, sym_size_int_64, 2, 128]);  linear_2 = None
            transpose_2: "bf16[1, 2, s0, 128]" = torch.ops.aten.transpose.int(view_2, 1, 2);  view_2 = None
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:169 in forward, code: self.cos_cached[:seq_len].to(dtype=x.dtype),
            slice_14: "bf16[s0, 128]" = torch.ops.aten.slice.Tensor(b_model_layers_0_self_attn_rotary_emb_cos_cached, 0, 0, sym_size_int_64);  b_model_layers_0_self_attn_rotary_emb_cos_cached = None
            to_2: "bf16[s0, 128]" = torch.ops.aten.to.dtype(slice_14, torch.bfloat16);  slice_14 = None
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:170 in forward, code: self.sin_cached[:seq_len].to(dtype=x.dtype),
            slice_15: "bf16[s0, 128]" = torch.ops.aten.slice.Tensor(b_model_layers_0_self_attn_rotary_emb_sin_cached, 0, 0, sym_size_int_64);  b_model_layers_0_self_attn_rotary_emb_sin_cached = None
            to_3: "bf16[s0, 128]" = torch.ops.aten.to.dtype(slice_15, torch.bfloat16);  slice_15 = None
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:204 in apply_rotary_pos_emb, code: cos = cos[position_ids].unsqueeze(unsqueeze_dim)
            index: "bf16[1, s0, 128]" = torch.ops.aten.index.Tensor(to_2, [unsqueeze]);  to_2 = None
            unsqueeze_5: "bf16[1, 1, s0, 128]" = torch.ops.aten.unsqueeze.default(index, 1);  index = None
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:205 in apply_rotary_pos_emb, code: sin = sin[position_ids].unsqueeze(unsqueeze_dim)
            index_1: "bf16[1, s0, 128]" = torch.ops.aten.index.Tensor(to_3, [unsqueeze]);  to_3 = None
            unsqueeze_6: "bf16[1, 1, s0, 128]" = torch.ops.aten.unsqueeze.default(index_1, 1);  index_1 = None
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:206 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_2: "bf16[1, 12, s0, 128]" = torch.ops.aten.mul.Tensor(transpose, unsqueeze_5)
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:177 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            slice_16: "bf16[1, 12, s0, 64]" = torch.ops.aten.slice.Tensor(transpose, 3, 0, 64)
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:178 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            slice_17: "bf16[1, 12, s0, 64]" = torch.ops.aten.slice.Tensor(transpose, 3, 64, 9223372036854775807);  transpose = None
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:179 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg: "bf16[1, 12, s0, 64]" = torch.ops.aten.neg.default(slice_17);  slice_17 = None
            cat: "bf16[1, 12, s0, 128]" = torch.ops.aten.cat.default([neg, slice_16], -1);  neg = slice_16 = None
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:206 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
            mul_3: "bf16[1, 12, s0, 128]" = torch.ops.aten.mul.Tensor(cat, unsqueeze_6);  cat = None
            add_2: "bf16[1, 12, s0, 128]" = torch.ops.aten.add.Tensor(mul_2, mul_3);  mul_2 = mul_3 = None
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:207 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_4: "bf16[1, 2, s0, 128]" = torch.ops.aten.mul.Tensor(transpose_1, unsqueeze_5);  unsqueeze_5 = None
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:177 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
            slice_18: "bf16[1, 2, s0, 64]" = torch.ops.aten.slice.Tensor(transpose_1, 3, 0, 64)
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:178 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
            slice_19: "bf16[1, 2, s0, 64]" = torch.ops.aten.slice.Tensor(transpose_1, 3, 64, 9223372036854775807);  transpose_1 = None
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:179 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
            neg_1: "bf16[1, 2, s0, 64]" = torch.ops.aten.neg.default(slice_19);  slice_19 = None
            cat_1: "bf16[1, 2, s0, 128]" = torch.ops.aten.cat.default([neg_1, slice_18], -1);  neg_1 = slice_18 = None
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:207 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
            mul_5: "bf16[1, 2, s0, 128]" = torch.ops.aten.mul.Tensor(cat_1, unsqueeze_6);  cat_1 = unsqueeze_6 = None
            add_3: "bf16[1, 2, s0, 128]" = torch.ops.aten.add.Tensor(mul_4, mul_5);  mul_4 = mul_5 = None
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:235 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            slice_20: "bf16[1, 2, s0, 128]" = torch.ops.aten.slice.Tensor(add_3, 0, 0, 9223372036854775807)
            slice_21: "bf16[1, 2, s0, 128]" = torch.ops.aten.slice.Tensor(slice_20, 1, 0, 9223372036854775807);  slice_20 = None
            unsqueeze_7: "bf16[1, 2, 1, s0, 128]" = torch.ops.aten.unsqueeze.default(slice_21, 2);  slice_21 = None
            slice_22: "bf16[1, 2, 1, s0, 128]" = torch.ops.aten.slice.Tensor(unsqueeze_7, 3, 0, 9223372036854775807);  unsqueeze_7 = None
            slice_23: "bf16[1, 2, 1, s0, 128]" = torch.ops.aten.slice.Tensor(slice_22, 4, 0, 9223372036854775807);  slice_22 = None
            expand_1: "bf16[1, 2, 6, s0, 128]" = torch.ops.aten.expand.default(slice_23, [1, 2, 6, sym_size_int_64, 128]);  slice_23 = None
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:236 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            reshape_1: "bf16[1, 12, s0, 128]" = torch.ops.aten.reshape.default(expand_1, [1, 12, sym_size_int_64, 128]);  expand_1 = None
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:235 in repeat_kv, code: hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
            slice_24: "bf16[1, 2, s0, 128]" = torch.ops.aten.slice.Tensor(transpose_2, 0, 0, 9223372036854775807)
            slice_25: "bf16[1, 2, s0, 128]" = torch.ops.aten.slice.Tensor(slice_24, 1, 0, 9223372036854775807);  slice_24 = None
            unsqueeze_8: "bf16[1, 2, 1, s0, 128]" = torch.ops.aten.unsqueeze.default(slice_25, 2);  slice_25 = None
            slice_26: "bf16[1, 2, 1, s0, 128]" = torch.ops.aten.slice.Tensor(unsqueeze_8, 3, 0, 9223372036854775807);  unsqueeze_8 = None
            slice_27: "bf16[1, 2, 1, s0, 128]" = torch.ops.aten.slice.Tensor(slice_26, 4, 0, 9223372036854775807);  slice_26 = None
            expand_2: "bf16[1, 2, 6, s0, 128]" = torch.ops.aten.expand.default(slice_27, [1, 2, 6, sym_size_int_64, 128]);  slice_27 = None
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:236 in repeat_kv, code: return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
            reshape_2: "bf16[1, 12, s0, 128]" = torch.ops.aten.reshape.default(expand_2, [1, 12, sym_size_int_64, 128]);  expand_2 = None
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:322 in forward, code: attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)
            transpose_3: "bf16[1, 12, 128, s0]" = torch.ops.aten.transpose.int(reshape_1, 2, 3);  reshape_1 = None
            matmul: "bf16[1, 12, s0, s0]" = torch.ops.aten.matmul.default(add_2, transpose_3);  add_2 = transpose_3 = None
            div: "bf16[1, 12, s0, s0]" = torch.ops.aten.div.Tensor(matmul, 11.313708498984761);  matmul = None
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:331 in forward, code: causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]
            slice_28: "bf16[1, 1, s0, s0]" = torch.ops.aten.slice.Tensor(clone, 0, 0, 9223372036854775807)
            slice_29: "bf16[1, 1, s0, s0]" = torch.ops.aten.slice.Tensor(slice_28, 1, 0, 9223372036854775807);  slice_28 = None
            slice_30: "bf16[1, 1, s0, s0]" = torch.ops.aten.slice.Tensor(slice_29, 2, 0, 9223372036854775807);  slice_29 = None
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:332 in forward, code: attn_weights = attn_weights + causal_mask
            add_4: "bf16[1, 12, s0, s0]" = torch.ops.aten.add.Tensor(div, slice_30);  div = slice_30 = None
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:335 in forward, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)
            softmax: "f32[1, 12, s0, s0]" = torch.ops.aten.softmax.int(add_4, -1, torch.float32);  add_4 = None
            to_4: "bf16[1, 12, s0, s0]" = torch.ops.aten.to.dtype(softmax, torch.bfloat16);  softmax = None
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:336 in forward, code: attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)
            dropout: "bf16[1, 12, s0, s0]" = torch.ops.aten.dropout.default(to_4, 0.0, False);  to_4 = None
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:337 in forward, code: attn_output = torch.matmul(attn_weights, value_states)
            matmul_1: "bf16[1, 12, s0, 128]" = torch.ops.aten.matmul.default(dropout, reshape_2);  dropout = reshape_2 = None
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:345 in forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
            transpose_4: "bf16[1, s0, 12, 128]" = torch.ops.aten.transpose.int(matmul_1, 1, 2);  matmul_1 = None
            contiguous: "bf16[1, s0, 12, 128]" = torch.ops.aten.contiguous.default(transpose_4);  transpose_4 = None
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:346 in forward, code: attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)
            reshape_3: "bf16[1, s0, 1536]" = torch.ops.aten.reshape.default(contiguous, [1, sym_size_int_64, 1536]);  contiguous = None
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:348 in forward, code: attn_output = self.o_proj(attn_output)
            linear_3: "bf16[1, s0, 1536]" = torch.ops.aten.linear.default(reshape_3, p_model_layers_0_self_attn_o_proj_weight);  reshape_3 = p_model_layers_0_self_attn_o_proj_weight = None
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:664 in forward, code: hidden_states = residual + hidden_states
            add_5: "bf16[1, s0, 1536]" = torch.ops.aten.add.Tensor(embedding, linear_3);  embedding = linear_3 = None
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:128 in forward, code: hidden_states = hidden_states.to(torch.float32)
            to_5: "f32[1, s0, 1536]" = torch.ops.aten.to.dtype(add_5, torch.float32)
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:129 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_2: "f32[1, s0, 1536]" = torch.ops.aten.pow.Tensor_Scalar(to_5, 2)
            mean_1: "f32[1, s0, 1]" = torch.ops.aten.mean.dim(pow_2, [-1], True);  pow_2 = None
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:130 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_6: "f32[1, s0, 1]" = torch.ops.aten.add.Tensor(mean_1, 1e-06);  mean_1 = None
            rsqrt_1: "f32[1, s0, 1]" = torch.ops.aten.rsqrt.default(add_6);  add_6 = None
            mul_6: "f32[1, s0, 1536]" = torch.ops.aten.mul.Tensor(to_5, rsqrt_1);  to_5 = rsqrt_1 = None
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:131 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_6: "bf16[1, s0, 1536]" = torch.ops.aten.to.dtype(mul_6, torch.bfloat16);  mul_6 = None
            mul_7: "bf16[1, s0, 1536]" = torch.ops.aten.mul.Tensor(p_model_layers_0_post_attention_layernorm_weight, to_6);  p_model_layers_0_post_attention_layernorm_weight = to_6 = None
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:223 in forward, code: return self.down_proj(self.act_fn(self.gate_proj(hidden_state)) * self.up_proj(hidden_state))
            linear_4: "bf16[1, s0, 8960]" = torch.ops.aten.linear.default(mul_7, p_model_layers_0_mlp_gate_proj_weight);  p_model_layers_0_mlp_gate_proj_weight = None
            silu: "bf16[1, s0, 8960]" = torch.ops.aten.silu.default(linear_4);  linear_4 = None
            linear_5: "bf16[1, s0, 8960]" = torch.ops.aten.linear.default(mul_7, p_model_layers_0_mlp_up_proj_weight);  mul_7 = p_model_layers_0_mlp_up_proj_weight = None
            mul_8: "bf16[1, s0, 8960]" = torch.ops.aten.mul.Tensor(silu, linear_5);  silu = linear_5 = None
            linear_6: "bf16[1, s0, 1536]" = torch.ops.aten.linear.default(mul_8, p_model_layers_0_mlp_down_proj_weight);  mul_8 = p_model_layers_0_mlp_down_proj_weight = None
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:670 in forward, code: hidden_states = residual + hidden_states
            add_7: "bf16[1, s0, 1536]" = torch.ops.aten.add.Tensor(add_5, linear_6);  add_5 = linear_6 = None
            

            
 -----------------------------------------------------------------------------------------------------------------------
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:128 in forward, code: hidden_states = hidden_states.to(torch.float32)
            to_196: "f32[1, s0, 1536]" = torch.ops.aten.to.dtype(add_196, torch.float32);  add_196 = None
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:129 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_57: "f32[1, s0, 1536]" = torch.ops.aten.pow.Tensor_Scalar(to_196, 2)
            mean_56: "f32[1, s0, 1]" = torch.ops.aten.mean.dim(pow_57, [-1], True);  pow_57 = None
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:130 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_197: "f32[1, s0, 1]" = torch.ops.aten.add.Tensor(mean_56, 1e-06);  mean_56 = None
            rsqrt_56: "f32[1, s0, 1]" = torch.ops.aten.rsqrt.default(add_197);  add_197 = None
            mul_252: "f32[1, s0, 1536]" = torch.ops.aten.mul.Tensor(to_196, rsqrt_56);  to_196 = rsqrt_56 = None
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:131 in forward, code: return self.weight * hidden_states.to(input_dtype)
            to_197: "bf16[1, s0, 1536]" = torch.ops.aten.to.dtype(mul_252, torch.bfloat16);  mul_252 = None
            mul_253: "bf16[1, s0, 1536]" = torch.ops.aten.mul.Tensor(p_model_norm_weight, to_197);  p_model_norm_weight = to_197 = None
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:1118 in forward, code: logits = self.lm_head(hidden_states)
            linear_196: "bf16[1, s0, 151936]" = torch.ops.aten.linear.default(mul_253, p_lm_head_weight);  mul_253 = p_lm_head_weight = None
            
             # File: /home/gsh/anaconda3/envs/aipiler/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:1119 in forward, code: logits = logits.float()
            to_198: "f32[1, s0, 151936]" = torch.ops.aten.to.dtype(linear_196, torch.float32);  linear_196 = None
            return (to_198, add_3, transpose_2, add_10, transpose_7, add_17, transpose_12, add_24, transpose_17, add_31, transpose_22, add_38, transpose_27, add_45, transpose_32, add_52, transpose_37, add_59, transpose_42, add_66, transpose_47, add_73, transpose_52, add_80, transpose_57, add_87, transpose_62, add_94, transpose_67, add_101, transpose_72, add_108, transpose_77, add_115, transpose_82, add_122, transpose_87, add_129, transpose_92, add_136, transpose_97, add_143, transpose_102, add_150, transpose_107, add_157, transpose_112, add_164, transpose_117, add_171, transpose_122, add_178, transpose_127, add_185, transpose_132, add_192, transpose_137)
            

