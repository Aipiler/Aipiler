import torch
import pickle
from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig
import os
from torch.export import Dim, export
import json
import iree.runtime as rt
from iree.turbine import aot
import numpy as np
from Aipiler.benchmark import BenchmarkConfig, BenchmarkResult, BenchmarkRunner


def load_deepseek_model(local_model_path):
    """åŠ è½½DeepSeekæ¨¡å‹å’Œtokenizer"""
    print("ğŸš€ åŠ è½½DeepSeekæ¨¡å‹...")

    # åŠ è½½é…ç½®
    config = AutoConfig.from_pretrained(
        local_model_path, local_files_only=True)
    print(f"æ¨¡å‹é…ç½®: {config.model_type}")
    
    # åŠ è½½tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        local_model_path, local_files_only=True, trust_remote_code=True
    )

    # åŠ è½½æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(
        local_model_path,
        local_files_only=True,
        trust_remote_code=True,
        attn_implementation="eager",
        torch_dtype=torch.bfloat16   # åœ¨è¿™é‡ŒæŒ‡å®šæƒé‡æ•°æ®ç±»å‹ä¸ºbfloat16
    )

    # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    model.eval()

    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    print(f"   - æ¨¡å‹ç±»å‹: {type(model).__name__}")
    print(f"   - å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    print(f"   - å‚æ•°ç±»å‹: {model.parameters().__next__().dtype}")
    print(f"   - è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")

    return model, tokenizer, config


def create_example_inputs(tokenizer, config, batch_size=1, seq_length=32):
    """åˆ›å»ºæ¨¡å‹è¾“å…¥ç¤ºä¾‹ - ä¸ºtorch.exportä¼˜åŒ–"""
    print(f"ğŸ“ åˆ›å»ºç¤ºä¾‹è¾“å…¥ (batch_size={batch_size}, seq_length={seq_length})")

    # torch.exportéœ€è¦ç¡®å®šçš„è¾“å…¥å½¢çŠ¶ï¼Œä½¿ç”¨è¾ƒå°çš„åºåˆ—é•¿åº¦
    # ä½¿ç”¨çœŸå®çš„tokenç¼–ç 
    sample_text = "Hello world"
    encoded = tokenizer(
        sample_text,
        return_tensors="pt",
        padding="max_length",
        max_length=seq_length,
        truncation=True,
    )

    input_ids = encoded["input_ids"]
    attention_mask = encoded.get("attention_mask", None)

    print(f"   - input_idså½¢çŠ¶: {input_ids.shape}")
    print(f"   - input_ids dtype: {input_ids.dtype}")
    if attention_mask is not None:
        print(f"   - attention_maskå½¢çŠ¶: {attention_mask.shape}")
        print(f"   - attention_maskå½¢çŠ¶: {attention_mask.dtype}")
    print(f"   - ç¤ºä¾‹æ–‡æœ¬: {sample_text}")

    return input_ids, attention_mask


def export_model_with_torch_export(
    model, input_ids, attention_mask=None
) -> torch.export.ExportedProgram:
    """ä½¿ç”¨torch.exportå¯¼å‡ºæ¨¡å‹"""
    print("ğŸ”„ ä½¿ç”¨torch.exportå¯¼å‡ºPyTorchæ¨¡å‹...")

    try:
        # å…ˆè¿›è¡Œä¸€æ¬¡å‰å‘ä¼ æ’­æµ‹è¯•
        print("   - æµ‹è¯•å‰å‘ä¼ æ’­...")
        with torch.no_grad():
            if attention_mask is not None:
                output = model(input_ids=input_ids,
                               attention_mask=attention_mask)
                print(f"   - è¾“å‡ºlogitså½¢çŠ¶: {output.logits.shape}")
            else:
                output = model(input_ids=input_ids)
                print(f"   - è¾“å‡ºlogitså½¢çŠ¶: {output.logits.shape}")

        print("   - å°è¯•ç›´æ¥å¯¼å‡ºæ¨¡å‹...")
        try:
            if attention_mask is not None:
                example_args = (input_ids, attention_mask)
                print(
                    f"   - è¾“å…¥å‚æ•°: input_ids{input_ids.shape}, attention_mask{attention_mask.shape}"
                )
            else:
                example_args = (input_ids,)
                print(f"   - è¾“å…¥å‚æ•°: input_ids{input_ids.shape}")

            # Create a dynamic batch size
            batch = Dim("batch")
            seq_len = Dim("seq_len", max=4095)
            dynamic_shapes = {
                "input_ids": {0: batch},
                "input_ids": {1: seq_len},
                "attention_mask": {0: batch},
                "attention_mask": {1: seq_len},
            }
            # ç›´æ¥ä¼ é€’æ¨¡å‹å¯¹è±¡å’Œä½ç½®å‚æ•°
            exported_program = export(
                model, args=example_args, dynamic_shapes=dynamic_shapes
            )
            print("âœ… ç›´æ¥å¯¼å‡ºæˆåŠŸ")
            return exported_program

        except Exception as e1:
            print(f"   âŒ ç›´æ¥å¯¼å‡ºå¤±è´¥: {e1}")

    except Exception as e:
        print(f"âŒ torch.exportå¯¼å‡ºå¤±è´¥: {e}")
        print("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        import traceback

        traceback.print_exc()


def analyze_exported_graph(exported_program: torch.export.ExportedProgram):
    """åˆ†æå¯¼å‡ºçš„å›¾ç»“æ„"""
    if exported_program is None:
        print("âš ï¸ æ²¡æœ‰å¯¼å‡ºçš„æ¨¡å‹å¯ä¾›åˆ†æ")
        return

    print("\n" + "=" * 60)
    print("ğŸ“Š åˆ†ætorch.exportå¯¼å‡ºçš„å›¾ç»“æ„")
    print("=" * 60)

    # æ‰“å°å›¾çš„åŸºæœ¬ä¿¡æ¯
    print(f"å›¾èŠ‚ç‚¹æ•°é‡: {len(exported_program.graph.nodes)}")

    # ç»Ÿè®¡èŠ‚ç‚¹ç±»å‹
    node_types = {}
    for node in exported_program.graph.nodes:
        node_type = node.op
        node_types[node_type] = node_types.get(node_type, 0) + 1

    print("\nèŠ‚ç‚¹ç±»å‹ç»Ÿè®¡:")
    for node_type, count in node_types.items():
        print(f"  {node_type:15}: {count:4d} ä¸ª")

    # åˆ†æå…·ä½“èŠ‚ç‚¹
    print(f"\nğŸ” è¯¦ç»†èŠ‚ç‚¹åˆ†æ:")
    print("-" * 60)

    for i, node in enumerate(exported_program.graph.nodes):
        if i >= 20:  # åªæ˜¾ç¤ºå‰20ä¸ªèŠ‚ç‚¹
            print(f"... (è¿˜æœ‰ {len(exported_program.graph.nodes) - 20} ä¸ªèŠ‚ç‚¹)")
            break

        print(f"\nèŠ‚ç‚¹ {i+1}: {node.name}")
        print(f"  æ“ä½œç±»å‹: {node.op}")

        if node.op == "placeholder":
            print(f"  å‚æ•°: {node.args}")
            if hasattr(node, "meta") and "val" in node.meta:
                val = node.meta["val"]
                if hasattr(val, "shape"):
                    print(f"  è¾“å…¥å½¢çŠ¶: {val.shape}")
                    print(f"  æ•°æ®ç±»å‹: {val.dtype}")

        elif node.op == "call_function":
            print(f"  è°ƒç”¨å‡½æ•°: {node.target}")
            print(f"  å‚æ•°æ•°é‡: {len(node.args)}")

            # æ‰“å°å‰å‡ ä¸ªå‚æ•°çš„ä¿¡æ¯
            for j, arg in enumerate(node.args[:3]):
                if hasattr(arg, "name"):
                    print(f"    å‚æ•°{j+1}: {arg.name} (èŠ‚ç‚¹)")
                else:
                    print(f"    å‚æ•°{j+1}: {type(arg)} - {str(arg)[:50]}")

            if len(node.args) > 3:
                print(f"    ... (è¿˜æœ‰ {len(node.args)-3} ä¸ªå‚æ•°)")

            # è¾“å‡ºä¿¡æ¯
            if hasattr(node, "meta") and "val" in node.meta:
                val = node.meta["val"]
                if hasattr(val, "shape"):
                    print(f"  è¾“å‡ºå½¢çŠ¶: {val.shape}")
                    print(f"  è¾“å‡ºç±»å‹: {val.dtype}")
                elif isinstance(val, (list, tuple)):
                    print(f"  è¾“å‡ºç±»å‹: {type(val)} (é•¿åº¦: {len(val)})")
                else:
                    print(f"  è¾“å‡º: {type(val)}")

        elif node.op == "output":
            print(f"  è¾“å‡ºèŠ‚ç‚¹")
            print(f"  å‚æ•°: {node.args}")


def analyze_graph_operators(exported_program):
    """åˆ†æå›¾ä¸­çš„ç®—å­ç±»å‹"""
    if exported_program is None:
        return

    print("\n" + "=" * 60)
    print("ğŸ”§ ç®—å­ç±»å‹åˆ†æ")
    print("=" * 60)

    operators = {}

    for node in exported_program.graph.nodes:
        if node.op == "call_function":
            op_name = str(node.target)
            operators[op_name] = operators.get(op_name, 0) + 1
        elif node.op == "call_method":
            method_name = str(node.target)
            operators[f"method_{method_name}"] = (
                operators.get(f"method_{method_name}", 0) + 1
            )

    print("æ£€æµ‹åˆ°çš„ç®—å­:")
    for op_name, count in sorted(operators.items()):
        print(f"  {op_name:30}: {count:4d} æ¬¡")

    return operators


def save_export_analysis(exported_program, output_dir="./export_analysis"):
    """ä¿å­˜å¯¼å‡ºåˆ†æç»“æœ"""
    if exported_program is None:
        return

    print(f"\nğŸ’¾ ä¿å­˜å¯¼å‡ºåˆ†æç»“æœåˆ° {output_dir}")
    os.makedirs(output_dir, exist_ok=True)

    # ä¿å­˜å›¾ç»“æ„
    torch.export.save(exported_program, output_dir + "/exported_program.pt2")
    with open(
        os.path.join(output_dir, "exported_graph.txt"), "w", encoding="utf-8"
    ) as f:
        f.write(str(exported_program))

    # ä¿å­˜èŠ‚ç‚¹è¯¦æƒ…
    # nodes_info = []
    # for node in exported_program.graph.nodes:
    #     node_info = {
    #         "name": node.name,
    #         "op": node.op,
    #         "target": str(node.target) if node.target else None,
    #         "args_count": len(node.args),
    #     }

    #     if hasattr(node, "meta") and "val" in node.meta:
    #         val = node.meta["val"]
    #         if hasattr(val, "shape"):
    #             node_info["output_shape"] = list(val.shape)
    #             node_info["output_dtype"] = str(val.dtype)

    #     nodes_info.append(node_info)

    # with open(
    #     os.path.join(output_dir, "nodes_analysis.json"), "w", encoding="utf-8"
    # ) as f:
    #     json.dump(nodes_info, f, indent=2, ensure_ascii=False)

    print("âœ… å¯¼å‡ºåˆ†æç»“æœå·²ä¿å­˜")


def main():
    """ä¸»å‡½æ•°"""
    local_model_path = "/home/gsh/DeepSeek-R1-Distill-Qwen-1.5B"

    try:
        # 1. åŠ è½½æ¨¡å‹
        model, tokenizer, config = load_deepseek_model(local_model_path)

        # 2. åˆ›å»ºç¤ºä¾‹è¾“å…¥ (ä½¿ç”¨è¾ƒå°çš„å°ºå¯¸ä»¥é€‚é…torch.export)
        input_ids, attention_mask = create_example_inputs(
            tokenizer, config, batch_size=1, seq_length=32  # è¾ƒå°çš„åºåˆ—é•¿åº¦
        )

        # target_backend = "cuda"
        # device = "cuda"

        # example_args = (input_ids, attention_mask)
        # exported = aot.export(model, args=example_args)
        # # exported.print_readable()
        # compiled_binary = exported.compile(
        #     save_to=None, target_backends=target_backend)

        # config = rt.Config(device)
        # vm_module = rt.VmModule.copy_buffer(
        #     config.vm_instance, compiled_binary.map_memory()
        # )

        # inputs = [input_ids.numpy(), attention_mask.numpy()]
        # benchmark_config = BenchmarkConfig(num_runs=10)
        # benchmarker = BenchmarkRunner(benchmark_config)
        # result = benchmarker.run_benchmark(
        #     vm_module,
        #     "main",
        #     inputs,
        #     f"main",
        #     device=device,
        # )
        # benchmarker.print_result_simple(result)

        # 3. ä½¿ç”¨torch.exportå¯¼å‡ºæ¨¡å‹
        exported_program = export_model_with_torch_export(
            model, input_ids, attention_mask
        )

        if exported_program is not None:
            # 4. åˆ†æå¯¼å‡ºçš„å›¾
            analyze_exported_graph(exported_program)

            # 5. åˆ†æç®—å­ç±»å‹
            operators = analyze_graph_operators(exported_program)

            # 6. ä¿å­˜åˆ†æç»“æœ
            save_export_analysis(exported_program)

            print("\nâœ… torch.exportå¯¼å‡ºå’Œåˆ†æå®Œæˆ")

        # 7. é¢å¤–çš„æ¨¡å‹ç»“æ„åˆ†æ
        print("\n" + "=" * 60)
        print("ğŸ“‹ è¡¥å……æ¨¡å‹ç»“æ„ä¿¡æ¯")
        print("=" * 60)

        total_params = sum(p.numel() for p in model.parameters())
        print(f"æ¨¡å‹æ€»å‚æ•°é‡: {total_params:,}")
        print(f"æ¨¡å‹ç±»å‹: {type(model).__name__}")
        print(f"é…ç½®ä¿¡æ¯:")

        key_configs = [
            "hidden_size",
            "num_hidden_layers",
            "num_attention_heads",
            "vocab_size",
        ]
        for key in key_configs:
            if hasattr(config, key):
                print(f"  {key}: {getattr(config, key)}")

    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()