TelechatModel(
  (word_embeddings): Embedding(120000, 5120)
  (h): ModuleList(
    (0-37): 38 x TelechatBlock(
      (input_layernorm): MixedFusedRMSNorm()
      (self_attention): TelechatAttention(
        (query): Linear(in_features=5120, out_features=5120, bias=False)
        (key_value): Linear(in_features=5120, out_features=10240, bias=False)
        (dense): Linear(in_features=5120, out_features=5120, bias=True)
        (attention_dropout): Dropout(p=0.0, inplace=False)
        (rotary_emb): RotaryEmbedding()
      )
      (post_attention_layernorm): MixedFusedRMSNorm()
      (mlp): TelechatMLP(
        (gate_proj): Linear(in_features=5120, out_features=12288, bias=False)
        (up_proj): Linear(in_features=5120, out_features=12288, bias=False)
        (down_proj): Linear(in_features=12288, out_features=5120, bias=True)
      )
    )
  )
  (ln_f): MixedFusedRMSNorm()
)
<class 'telechat_src.modeling_telechat.TelechatModel'>
inputs: tensor([[ 23509, 113348, 112103,  77755,  32185, 112203]])
input_ids of TelechatModel forward tensor([[ 23509, 113348, 112103,  77755,  32185, 112203]])
inputs_embeds None
use_cache True
output_attentions False
output_hidden_states False
past_key_values None
attention_mask None
src_length 6
combined_attention_mask tensor([[[[False,  True,  True,  True,  True,  True],
          [False, False,  True,  True,  True,  True],
          [False, False, False,  True,  True,  True],
          [False, False, False, False,  True,  True],
          [False, False, False, False, False,  True],
          [False, False, False, False, False, False]]]])
return_dict True
responese:

tensor([], size=(0, 6, 5120), dtype=torch.float16, grad_fn=<SliceBackward0>)
ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, p_h_0_input_layernorm_weight: "f32[5120]", p_h_0_post_attention_layernorm_weight: "f32[5120]", p_h_1_input_layernorm_weight: "f32[5120]", p_h_1_post_attention_layernorm_weight: "f32[5120]", p_h_2_input_layernorm_weight: "f32[5120]", p_h_2_post_attention_layernorm_weight: "f32[5120]", p_h_3_input_layernorm_weight: "f32[5120]", p_h_3_post_attention_layernorm_weight: "f32[5120]", p_h_4_input_layernorm_weight: "f32[5120]", p_h_4_post_attention_layernorm_weight: "f32[5120]", p_h_5_input_layernorm_weight: "f32[5120]", p_h_5_post_attention_layernorm_weight: "f32[5120]", p_h_6_input_layernorm_weight: "f32[5120]", p_h_6_post_attention_layernorm_weight: "f32[5120]", p_h_7_input_layernorm_weight: "f32[5120]", p_h_7_post_attention_layernorm_weight: "f32[5120]", p_h_8_input_layernorm_weight: "f32[5120]", p_h_8_post_attention_layernorm_weight: "f32[5120]", p_h_9_input_layernorm_weight: "f32[5120]", p_h_9_post_attention_layernorm_weight: "f32[5120]", p_h_10_input_layernorm_weight: "f32[5120]", p_h_10_post_attention_layernorm_weight: "f32[5120]", p_h_11_input_layernorm_weight: "f32[5120]", p_h_11_post_attention_layernorm_weight: "f32[5120]", p_h_12_input_layernorm_weight: "f32[5120]", p_h_12_post_attention_layernorm_weight: "f32[5120]", p_h_13_input_layernorm_weight: "f32[5120]", p_h_13_post_attention_layernorm_weight: "f32[5120]", p_h_14_input_layernorm_weight: "f32[5120]", p_h_14_post_attention_layernorm_weight: "f32[5120]", p_h_15_input_layernorm_weight: "f32[5120]", p_h_15_post_attention_layernorm_weight: "f32[5120]", p_h_16_input_layernorm_weight: "f32[5120]", p_h_16_post_attention_layernorm_weight: "f32[5120]", p_h_17_input_layernorm_weight: "f32[5120]", p_h_17_post_attention_layernorm_weight: "f32[5120]", p_h_18_input_layernorm_weight: "f32[5120]", p_h_18_post_attention_layernorm_weight: "f32[5120]", p_h_19_input_layernorm_weight: "f32[5120]", p_h_19_post_attention_layernorm_weight: "f32[5120]", p_h_20_input_layernorm_weight: "f32[5120]", p_h_20_post_attention_layernorm_weight: "f32[5120]", p_h_21_input_layernorm_weight: "f32[5120]", p_h_21_post_attention_layernorm_weight: "f32[5120]", p_h_22_input_layernorm_weight: "f32[5120]", p_h_22_post_attention_layernorm_weight: "f32[5120]", p_h_23_input_layernorm_weight: "f32[5120]", p_h_23_post_attention_layernorm_weight: "f32[5120]", p_h_24_input_layernorm_weight: "f32[5120]", p_h_24_post_attention_layernorm_weight: "f32[5120]", p_h_25_input_layernorm_weight: "f32[5120]", p_h_25_post_attention_layernorm_weight: "f32[5120]", p_h_26_input_layernorm_weight: "f32[5120]", p_h_26_post_attention_layernorm_weight: "f32[5120]", p_h_27_input_layernorm_weight: "f32[5120]", p_h_27_post_attention_layernorm_weight: "f32[5120]", p_h_28_input_layernorm_weight: "f32[5120]", p_h_28_post_attention_layernorm_weight: "f32[5120]", p_h_29_input_layernorm_weight: "f32[5120]", p_h_29_post_attention_layernorm_weight: "f32[5120]", p_h_30_input_layernorm_weight: "f32[5120]", p_h_30_post_attention_layernorm_weight: "f32[5120]", p_h_31_input_layernorm_weight: "f32[5120]", p_h_31_post_attention_layernorm_weight: "f32[5120]", p_h_32_input_layernorm_weight: "f32[5120]", p_h_32_post_attention_layernorm_weight: "f32[5120]", p_h_33_input_layernorm_weight: "f32[5120]", p_h_33_post_attention_layernorm_weight: "f32[5120]", p_h_34_input_layernorm_weight: "f32[5120]", p_h_34_post_attention_layernorm_weight: "f32[5120]", p_h_35_input_layernorm_weight: "f32[5120]", p_h_35_post_attention_layernorm_weight: "f32[5120]", p_h_36_input_layernorm_weight: "f32[5120]", p_h_36_post_attention_layernorm_weight: "f32[5120]", p_h_37_input_layernorm_weight: "f32[5120]", p_h_37_post_attention_layernorm_weight: "f32[5120]", p_ln_f_weight: "f32[5120]", p_word_embeddings_weight: "f32[120000, 5120]", p_h_0_self_attention_query_weight: "f32[5120, 5120]", p_h_0_self_attention_key_value_weight: "f32[10240, 5120]", p_h_0_self_attention_dense_weight: "f32[5120, 5120]", p_h_0_self_attention_dense_bias: "f32[5120]", p_h_0_mlp_gate_proj_weight: "f32[12288, 5120]", p_h_0_mlp_up_proj_weight: "f32[12288, 5120]", p_h_0_mlp_down_proj_weight: "f32[5120, 12288]", p_h_0_mlp_down_proj_bias: "f32[5120]", p_h_1_self_attention_query_weight: "f32[5120, 5120]", p_h_1_self_attention_key_value_weight: "f32[10240, 5120]", p_h_1_self_attention_dense_weight: "f32[5120, 5120]", p_h_1_self_attention_dense_bias: "f32[5120]", p_h_1_mlp_gate_proj_weight: "f32[12288, 5120]", p_h_1_mlp_up_proj_weight: "f32[12288, 5120]", p_h_1_mlp_down_proj_weight: "f32[5120, 12288]", p_h_1_mlp_down_proj_bias: "f32[5120]", p_h_2_self_attention_query_weight: "f32[5120, 5120]", p_h_2_self_attention_key_value_weight: "f32[10240, 5120]", p_h_2_self_attention_dense_weight: "f32[5120, 5120]", p_h_2_self_attention_dense_bias: "f32[5120]", p_h_2_mlp_gate_proj_weight: "f32[12288, 5120]", p_h_2_mlp_up_proj_weight: "f32[12288, 5120]", p_h_2_mlp_down_proj_weight: "f32[5120, 12288]", p_h_2_mlp_down_proj_bias: "f32[5120]", p_h_3_self_attention_query_weight: "f32[5120, 5120]", p_h_3_self_attention_key_value_weight: "f32[10240, 5120]", p_h_3_self_attention_dense_weight: "f32[5120, 5120]", p_h_3_self_attention_dense_bias: "f32[5120]", p_h_3_mlp_gate_proj_weight: "f32[12288, 5120]", p_h_3_mlp_up_proj_weight: "f32[12288, 5120]", p_h_3_mlp_down_proj_weight: "f32[5120, 12288]", p_h_3_mlp_down_proj_bias: "f32[5120]", p_h_4_self_attention_query_weight: "f32[5120, 5120]", p_h_4_self_attention_key_value_weight: "f32[10240, 5120]", p_h_4_self_attention_dense_weight: "f32[5120, 5120]", p_h_4_self_attention_dense_bias: "f32[5120]", p_h_4_mlp_gate_proj_weight: "f32[12288, 5120]", p_h_4_mlp_up_proj_weight: "f32[12288, 5120]", p_h_4_mlp_down_proj_weight: "f32[5120, 12288]", p_h_4_mlp_down_proj_bias: "f32[5120]", p_h_5_self_attention_query_weight: "f32[5120, 5120]", p_h_5_self_attention_key_value_weight: "f32[10240, 5120]", p_h_5_self_attention_dense_weight: "f32[5120, 5120]", p_h_5_self_attention_dense_bias: "f32[5120]", p_h_5_mlp_gate_proj_weight: "f32[12288, 5120]", p_h_5_mlp_up_proj_weight: "f32[12288, 5120]", p_h_5_mlp_down_proj_weight: "f32[5120, 12288]", p_h_5_mlp_down_proj_bias: "f32[5120]", p_h_6_self_attention_query_weight: "f32[5120, 5120]", p_h_6_self_attention_key_value_weight: "f32[10240, 5120]", p_h_6_self_attention_dense_weight: "f32[5120, 5120]", p_h_6_self_attention_dense_bias: "f32[5120]", p_h_6_mlp_gate_proj_weight: "f32[12288, 5120]", p_h_6_mlp_up_proj_weight: "f32[12288, 5120]", p_h_6_mlp_down_proj_weight: "f32[5120, 12288]", p_h_6_mlp_down_proj_bias: "f32[5120]", p_h_7_self_attention_query_weight: "f32[5120, 5120]", p_h_7_self_attention_key_value_weight: "f32[10240, 5120]", p_h_7_self_attention_dense_weight: "f32[5120, 5120]", p_h_7_self_attention_dense_bias: "f32[5120]", p_h_7_mlp_gate_proj_weight: "f32[12288, 5120]", p_h_7_mlp_up_proj_weight: "f32[12288, 5120]", p_h_7_mlp_down_proj_weight: "f32[5120, 12288]", p_h_7_mlp_down_proj_bias: "f32[5120]", p_h_8_self_attention_query_weight: "f32[5120, 5120]", p_h_8_self_attention_key_value_weight: "f32[10240, 5120]", p_h_8_self_attention_dense_weight: "f32[5120, 5120]", p_h_8_self_attention_dense_bias: "f32[5120]", p_h_8_mlp_gate_proj_weight: "f32[12288, 5120]", p_h_8_mlp_up_proj_weight: "f32[12288, 5120]", p_h_8_mlp_down_proj_weight: "f32[5120, 12288]", p_h_8_mlp_down_proj_bias: "f32[5120]", p_h_9_self_attention_query_weight: "f32[5120, 5120]", p_h_9_self_attention_key_value_weight: "f32[10240, 5120]", p_h_9_self_attention_dense_weight: "f32[5120, 5120]", p_h_9_self_attention_dense_bias: "f32[5120]", p_h_9_mlp_gate_proj_weight: "f32[12288, 5120]", p_h_9_mlp_up_proj_weight: "f32[12288, 5120]", p_h_9_mlp_down_proj_weight: "f32[5120, 12288]", p_h_9_mlp_down_proj_bias: "f32[5120]", p_h_10_self_attention_query_weight: "f32[5120, 5120]", p_h_10_self_attention_key_value_weight: "f32[10240, 5120]", p_h_10_self_attention_dense_weight: "f32[5120, 5120]", p_h_10_self_attention_dense_bias: "f32[5120]", p_h_10_mlp_gate_proj_weight: "f32[12288, 5120]", p_h_10_mlp_up_proj_weight: "f32[12288, 5120]", p_h_10_mlp_down_proj_weight: "f32[5120, 12288]", p_h_10_mlp_down_proj_bias: "f32[5120]", p_h_11_self_attention_query_weight: "f32[5120, 5120]", p_h_11_self_attention_key_value_weight: "f32[10240, 5120]", p_h_11_self_attention_dense_weight: "f32[5120, 5120]", p_h_11_self_attention_dense_bias: "f32[5120]", p_h_11_mlp_gate_proj_weight: "f32[12288, 5120]", p_h_11_mlp_up_proj_weight: "f32[12288, 5120]", p_h_11_mlp_down_proj_weight: "f32[5120, 12288]", p_h_11_mlp_down_proj_bias: "f32[5120]", p_h_12_self_attention_query_weight: "f32[5120, 5120]", p_h_12_self_attention_key_value_weight: "f32[10240, 5120]", p_h_12_self_attention_dense_weight: "f32[5120, 5120]", p_h_12_self_attention_dense_bias: "f32[5120]", p_h_12_mlp_gate_proj_weight: "f32[12288, 5120]", p_h_12_mlp_up_proj_weight: "f32[12288, 5120]", p_h_12_mlp_down_proj_weight: "f32[5120, 12288]", p_h_12_mlp_down_proj_bias: "f32[5120]", p_h_13_self_attention_query_weight: "f32[5120, 5120]", p_h_13_self_attention_key_value_weight: "f32[10240, 5120]", p_h_13_self_attention_dense_weight: "f32[5120, 5120]", p_h_13_self_attention_dense_bias: "f32[5120]", p_h_13_mlp_gate_proj_weight: "f32[12288, 5120]", p_h_13_mlp_up_proj_weight: "f32[12288, 5120]", p_h_13_mlp_down_proj_weight: "f32[5120, 12288]", p_h_13_mlp_down_proj_bias: "f32[5120]", p_h_14_self_attention_query_weight: "f32[5120, 5120]", p_h_14_self_attention_key_value_weight: "f32[10240, 5120]", p_h_14_self_attention_dense_weight: "f32[5120, 5120]", p_h_14_self_attention_dense_bias: "f32[5120]", p_h_14_mlp_gate_proj_weight: "f32[12288, 5120]", p_h_14_mlp_up_proj_weight: "f32[12288, 5120]", p_h_14_mlp_down_proj_weight: "f32[5120, 12288]", p_h_14_mlp_down_proj_bias: "f32[5120]", p_h_15_self_attention_query_weight: "f32[5120, 5120]", p_h_15_self_attention_key_value_weight: "f32[10240, 5120]", p_h_15_self_attention_dense_weight: "f32[5120, 5120]", p_h_15_self_attention_dense_bias: "f32[5120]", p_h_15_mlp_gate_proj_weight: "f32[12288, 5120]", p_h_15_mlp_up_proj_weight: "f32[12288, 5120]", p_h_15_mlp_down_proj_weight: "f32[5120, 12288]", p_h_15_mlp_down_proj_bias: "f32[5120]", p_h_16_self_attention_query_weight: "f32[5120, 5120]", p_h_16_self_attention_key_value_weight: "f32[10240, 5120]", p_h_16_self_attention_dense_weight: "f32[5120, 5120]", p_h_16_self_attention_dense_bias: "f32[5120]", p_h_16_mlp_gate_proj_weight: "f32[12288, 5120]", p_h_16_mlp_up_proj_weight: "f32[12288, 5120]", p_h_16_mlp_down_proj_weight: "f32[5120, 12288]", p_h_16_mlp_down_proj_bias: "f32[5120]", p_h_17_self_attention_query_weight: "f32[5120, 5120]", p_h_17_self_attention_key_value_weight: "f32[10240, 5120]", p_h_17_self_attention_dense_weight: "f32[5120, 5120]", p_h_17_self_attention_dense_bias: "f32[5120]", p_h_17_mlp_gate_proj_weight: "f32[12288, 5120]", p_h_17_mlp_up_proj_weight: "f32[12288, 5120]", p_h_17_mlp_down_proj_weight: "f32[5120, 12288]", p_h_17_mlp_down_proj_bias: "f32[5120]", p_h_18_self_attention_query_weight: "f32[5120, 5120]", p_h_18_self_attention_key_value_weight: "f32[10240, 5120]", p_h_18_self_attention_dense_weight: "f32[5120, 5120]", p_h_18_self_attention_dense_bias: "f32[5120]", p_h_18_mlp_gate_proj_weight: "f32[12288, 5120]", p_h_18_mlp_up_proj_weight: "f32[12288, 5120]", p_h_18_mlp_down_proj_weight: "f32[5120, 12288]", p_h_18_mlp_down_proj_bias: "f32[5120]", p_h_19_self_attention_query_weight: "f32[5120, 5120]", p_h_19_self_attention_key_value_weight: "f32[10240, 5120]", p_h_19_self_attention_dense_weight: "f32[5120, 5120]", p_h_19_self_attention_dense_bias: "f32[5120]", p_h_19_mlp_gate_proj_weight: "f32[12288, 5120]", p_h_19_mlp_up_proj_weight: "f32[12288, 5120]", p_h_19_mlp_down_proj_weight: "f32[5120, 12288]", p_h_19_mlp_down_proj_bias: "f32[5120]", p_h_20_self_attention_query_weight: "f32[5120, 5120]", p_h_20_self_attention_key_value_weight: "f32[10240, 5120]", p_h_20_self_attention_dense_weight: "f32[5120, 5120]", p_h_20_self_attention_dense_bias: "f32[5120]", p_h_20_mlp_gate_proj_weight: "f32[12288, 5120]", p_h_20_mlp_up_proj_weight: "f32[12288, 5120]", p_h_20_mlp_down_proj_weight: "f32[5120, 12288]", p_h_20_mlp_down_proj_bias: "f32[5120]", p_h_21_self_attention_query_weight: "f32[5120, 5120]", p_h_21_self_attention_key_value_weight: "f32[10240, 5120]", p_h_21_self_attention_dense_weight: "f32[5120, 5120]", p_h_21_self_attention_dense_bias: "f32[5120]", p_h_21_mlp_gate_proj_weight: "f32[12288, 5120]", p_h_21_mlp_up_proj_weight: "f32[12288, 5120]", p_h_21_mlp_down_proj_weight: "f32[5120, 12288]", p_h_21_mlp_down_proj_bias: "f32[5120]", p_h_22_self_attention_query_weight: "f32[5120, 5120]", p_h_22_self_attention_key_value_weight: "f32[10240, 5120]", p_h_22_self_attention_dense_weight: "f32[5120, 5120]", p_h_22_self_attention_dense_bias: "f32[5120]", p_h_22_mlp_gate_proj_weight: "f32[12288, 5120]", p_h_22_mlp_up_proj_weight: "f32[12288, 5120]", p_h_22_mlp_down_proj_weight: "f32[5120, 12288]", p_h_22_mlp_down_proj_bias: "f32[5120]", p_h_23_self_attention_query_weight: "f32[5120, 5120]", p_h_23_self_attention_key_value_weight: "f32[10240, 5120]", p_h_23_self_attention_dense_weight: "f32[5120, 5120]", p_h_23_self_attention_dense_bias: "f32[5120]", p_h_23_mlp_gate_proj_weight: "f32[12288, 5120]", p_h_23_mlp_up_proj_weight: "f32[12288, 5120]", p_h_23_mlp_down_proj_weight: "f32[5120, 12288]", p_h_23_mlp_down_proj_bias: "f32[5120]", p_h_24_self_attention_query_weight: "f32[5120, 5120]", p_h_24_self_attention_key_value_weight: "f32[10240, 5120]", p_h_24_self_attention_dense_weight: "f32[5120, 5120]", p_h_24_self_attention_dense_bias: "f32[5120]", p_h_24_mlp_gate_proj_weight: "f32[12288, 5120]", p_h_24_mlp_up_proj_weight: "f32[12288, 5120]", p_h_24_mlp_down_proj_weight: "f32[5120, 12288]", p_h_24_mlp_down_proj_bias: "f32[5120]", p_h_25_self_attention_query_weight: "f32[5120, 5120]", p_h_25_self_attention_key_value_weight: "f32[10240, 5120]", p_h_25_self_attention_dense_weight: "f32[5120, 5120]", p_h_25_self_attention_dense_bias: "f32[5120]", p_h_25_mlp_gate_proj_weight: "f32[12288, 5120]", p_h_25_mlp_up_proj_weight: "f32[12288, 5120]", p_h_25_mlp_down_proj_weight: "f32[5120, 12288]", p_h_25_mlp_down_proj_bias: "f32[5120]", p_h_26_self_attention_query_weight: "f32[5120, 5120]", p_h_26_self_attention_key_value_weight: "f32[10240, 5120]", p_h_26_self_attention_dense_weight: "f32[5120, 5120]", p_h_26_self_attention_dense_bias: "f32[5120]", p_h_26_mlp_gate_proj_weight: "f32[12288, 5120]", p_h_26_mlp_up_proj_weight: "f32[12288, 5120]", p_h_26_mlp_down_proj_weight: "f32[5120, 12288]", p_h_26_mlp_down_proj_bias: "f32[5120]", p_h_27_self_attention_query_weight: "f32[5120, 5120]", p_h_27_self_attention_key_value_weight: "f32[10240, 5120]", p_h_27_self_attention_dense_weight: "f32[5120, 5120]", p_h_27_self_attention_dense_bias: "f32[5120]", p_h_27_mlp_gate_proj_weight: "f32[12288, 5120]", p_h_27_mlp_up_proj_weight: "f32[12288, 5120]", p_h_27_mlp_down_proj_weight: "f32[5120, 12288]", p_h_27_mlp_down_proj_bias: "f32[5120]", p_h_28_self_attention_query_weight: "f32[5120, 5120]", p_h_28_self_attention_key_value_weight: "f32[10240, 5120]", p_h_28_self_attention_dense_weight: "f32[5120, 5120]", p_h_28_self_attention_dense_bias: "f32[5120]", p_h_28_mlp_gate_proj_weight: "f32[12288, 5120]", p_h_28_mlp_up_proj_weight: "f32[12288, 5120]", p_h_28_mlp_down_proj_weight: "f32[5120, 12288]", p_h_28_mlp_down_proj_bias: "f32[5120]", p_h_29_self_attention_query_weight: "f32[5120, 5120]", p_h_29_self_attention_key_value_weight: "f32[10240, 5120]", p_h_29_self_attention_dense_weight: "f32[5120, 5120]", p_h_29_self_attention_dense_bias: "f32[5120]", p_h_29_mlp_gate_proj_weight: "f32[12288, 5120]", p_h_29_mlp_up_proj_weight: "f32[12288, 5120]", p_h_29_mlp_down_proj_weight: "f32[5120, 12288]", p_h_29_mlp_down_proj_bias: "f32[5120]", p_h_30_self_attention_query_weight: "f32[5120, 5120]", p_h_30_self_attention_key_value_weight: "f32[10240, 5120]", p_h_30_self_attention_dense_weight: "f32[5120, 5120]", p_h_30_self_attention_dense_bias: "f32[5120]", p_h_30_mlp_gate_proj_weight: "f32[12288, 5120]", p_h_30_mlp_up_proj_weight: "f32[12288, 5120]", p_h_30_mlp_down_proj_weight: "f32[5120, 12288]", p_h_30_mlp_down_proj_bias: "f32[5120]", p_h_31_self_attention_query_weight: "f32[5120, 5120]", p_h_31_self_attention_key_value_weight: "f32[10240, 5120]", p_h_31_self_attention_dense_weight: "f32[5120, 5120]", p_h_31_self_attention_dense_bias: "f32[5120]", p_h_31_mlp_gate_proj_weight: "f32[12288, 5120]", p_h_31_mlp_up_proj_weight: "f32[12288, 5120]", p_h_31_mlp_down_proj_weight: "f32[5120, 12288]", p_h_31_mlp_down_proj_bias: "f32[5120]", p_h_32_self_attention_query_weight: "f32[5120, 5120]", p_h_32_self_attention_key_value_weight: "f32[10240, 5120]", p_h_32_self_attention_dense_weight: "f32[5120, 5120]", p_h_32_self_attention_dense_bias: "f32[5120]", p_h_32_mlp_gate_proj_weight: "f32[12288, 5120]", p_h_32_mlp_up_proj_weight: "f32[12288, 5120]", p_h_32_mlp_down_proj_weight: "f32[5120, 12288]", p_h_32_mlp_down_proj_bias: "f32[5120]", p_h_33_self_attention_query_weight: "f32[5120, 5120]", p_h_33_self_attention_key_value_weight: "f32[10240, 5120]", p_h_33_self_attention_dense_weight: "f32[5120, 5120]", p_h_33_self_attention_dense_bias: "f32[5120]", p_h_33_mlp_gate_proj_weight: "f32[12288, 5120]", p_h_33_mlp_up_proj_weight: "f32[12288, 5120]", p_h_33_mlp_down_proj_weight: "f32[5120, 12288]", p_h_33_mlp_down_proj_bias: "f32[5120]", p_h_34_self_attention_query_weight: "f32[5120, 5120]", p_h_34_self_attention_key_value_weight: "f32[10240, 5120]", p_h_34_self_attention_dense_weight: "f32[5120, 5120]", p_h_34_self_attention_dense_bias: "f32[5120]", p_h_34_mlp_gate_proj_weight: "f32[12288, 5120]", p_h_34_mlp_up_proj_weight: "f32[12288, 5120]", p_h_34_mlp_down_proj_weight: "f32[5120, 12288]", p_h_34_mlp_down_proj_bias: "f32[5120]", p_h_35_self_attention_query_weight: "f32[5120, 5120]", p_h_35_self_attention_key_value_weight: "f32[10240, 5120]", p_h_35_self_attention_dense_weight: "f32[5120, 5120]", p_h_35_self_attention_dense_bias: "f32[5120]", p_h_35_mlp_gate_proj_weight: "f32[12288, 5120]", p_h_35_mlp_up_proj_weight: "f32[12288, 5120]", p_h_35_mlp_down_proj_weight: "f32[5120, 12288]", p_h_35_mlp_down_proj_bias: "f32[5120]", p_h_36_self_attention_query_weight: "f32[5120, 5120]", p_h_36_self_attention_key_value_weight: "f32[10240, 5120]", p_h_36_self_attention_dense_weight: "f32[5120, 5120]", p_h_36_self_attention_dense_bias: "f32[5120]", p_h_36_mlp_gate_proj_weight: "f32[12288, 5120]", p_h_36_mlp_up_proj_weight: "f32[12288, 5120]", p_h_36_mlp_down_proj_weight: "f32[5120, 12288]", p_h_36_mlp_down_proj_bias: "f32[5120]", p_h_37_self_attention_query_weight: "f32[5120, 5120]", p_h_37_self_attention_key_value_weight: "f32[10240, 5120]", p_h_37_self_attention_dense_weight: "f32[5120, 5120]", p_h_37_self_attention_dense_bias: "f32[5120]", p_h_37_mlp_gate_proj_weight: "f32[12288, 5120]", p_h_37_mlp_up_proj_weight: "f32[12288, 5120]", p_h_37_mlp_down_proj_weight: "f32[5120, 12288]", p_h_37_mlp_down_proj_bias: "f32[5120]", c_lifted_tensor_0: "i64[1, 6]", input_ids: "f32[10, 10]", past_key_values: "f32[10, 10]"):
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:664 in forward, code: input_ids = torch.tensor([[23509, 113348, 112103, 77755, 32185, 112203]])
            lift_fresh_copy: "i64[1, 6]" = torch.ops.aten.lift_fresh_copy.default(c_lifted_tensor_0);  c_lifted_tensor_0 = None
            detach: "i64[1, 6]" = torch.ops.aten.detach.default(lift_fresh_copy);  lift_fresh_copy = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:679 in forward, code: inputs_embeds = self.word_embeddings(input_ids)
            embedding: "f32[1, 6, 5120]" = torch.ops.aten.embedding.default(p_word_embeddings_weight, detach);  p_word_embeddings_weight = detach = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:717 in forward, code: attention_mask = torch.ones((batch_size, seq_length_with_past), device=hidden_states.device)
            ones: "f32[1, 6]" = torch.ops.aten.ones.default([1, 6], device = device(type='cpu'), pin_memory = False)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:221 in _make_causal_mask, code: mask = torch.empty((target_length, target_length + past_key_values_length), dtype=torch.bool, device=device)
            empty: "b8[6, 6]" = torch.ops.aten.empty.memory_format([6, 6], dtype = torch.bool, device = device(type='cpu'), pin_memory = False)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:223 in _make_causal_mask, code: seq_ids = torch.arange(target_length, device=device)
            arange: "i64[6]" = torch.ops.aten.arange.default(6, device = device(type='cpu'), pin_memory = False)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:224 in _make_causal_mask, code: mask[:, past_key_values_length:] = seq_ids[:, None] < seq_ids[None, :]
            slice_1: "i64[6]" = torch.ops.aten.slice.Tensor(arange, 0, 0, 9223372036854775807)
            unsqueeze: "i64[6, 1]" = torch.ops.aten.unsqueeze.default(slice_1, 1);  slice_1 = None
            unsqueeze_1: "i64[1, 6]" = torch.ops.aten.unsqueeze.default(arange, 0);  arange = None
            slice_2: "i64[1, 6]" = torch.ops.aten.slice.Tensor(unsqueeze_1, 1, 0, 9223372036854775807);  unsqueeze_1 = None
            lt: "b8[6, 6]" = torch.ops.aten.lt.Tensor(unsqueeze, slice_2);  unsqueeze = slice_2 = None
            slice_3: "b8[6, 6]" = torch.ops.aten.slice.Tensor(empty, 0, 0, 9223372036854775807)
            slice_4: "b8[6, 6]" = torch.ops.aten.slice.Tensor(slice_3, 1, 0, 9223372036854775807);  slice_3 = None
            copy: "b8[6, 6]" = torch.ops.aten.copy.default(slice_4, lt);  slice_4 = lt = None
            slice_5: "b8[6, 6]" = torch.ops.aten.slice.Tensor(empty, 0, 0, 9223372036854775807)
            slice_scatter: "b8[6, 6]" = torch.ops.aten.slice_scatter.default(slice_5, copy, 1, 0, 9223372036854775807);  slice_5 = copy = None
            slice_scatter_1: "b8[6, 6]" = torch.ops.aten.slice_scatter.default(empty, slice_scatter, 0, 0, 9223372036854775807);  empty = slice_scatter = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:240 in _expand_mask, code: expanded_mask = ~(mask[:, None, None, :].to(torch.bool))
            slice_10: "f32[1, 6]" = torch.ops.aten.slice.Tensor(ones, 0, 0, 9223372036854775807);  ones = None
            unsqueeze_4: "f32[1, 1, 6]" = torch.ops.aten.unsqueeze.default(slice_10, 1);  slice_10 = None
            unsqueeze_5: "f32[1, 1, 1, 6]" = torch.ops.aten.unsqueeze.default(unsqueeze_4, 2);  unsqueeze_4 = None
            slice_11: "f32[1, 1, 1, 6]" = torch.ops.aten.slice.Tensor(unsqueeze_5, 3, 0, 9223372036854775807);  unsqueeze_5 = None
            _to_copy: "b8[1, 1, 1, 6]" = torch.ops.aten._to_copy.default(slice_11, dtype = torch.bool);  slice_11 = None
            bitwise_not: "b8[1, 1, 1, 6]" = torch.ops.aten.bitwise_not.default(_to_copy);  _to_copy = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:241 in _expand_mask, code: return expanded_mask.expand(batch_size, 1, tgt_length, src_length)
            expand_1: "b8[1, 1, 6, 6]" = torch.ops.aten.expand.default(bitwise_not, [1, 1, 6, 6]);  bitwise_not = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:628 in _prepare_attn_mask, code: combined_attention_mask = ( expanded_attn_mask | combined_attention_mask
            unsqueeze_6: "b8[1, 6, 6]" = torch.ops.aten.unsqueeze.default(slice_scatter_1, 0);  slice_scatter_1 = None
            unsqueeze_7: "b8[1, 1, 6, 6]" = torch.ops.aten.unsqueeze.default(unsqueeze_6, 1);  unsqueeze_6 = None
            slice_12: "b8[1, 1, 6, 6]" = torch.ops.aten.slice.Tensor(unsqueeze_7, 2, 0, 9223372036854775807);  unsqueeze_7 = None
            slice_13: "b8[1, 1, 6, 6]" = torch.ops.aten.slice.Tensor(slice_12, 3, 0, 9223372036854775807);  slice_12 = None
            expand_2: "b8[1, 1, 6, 6]" = torch.ops.aten.expand.default(slice_13, [1, 1, 6, 6]);  slice_13 = None
            or_1: "b8[1, 1, 6, 6]" = torch.ops.aten.__or__.Tensor(expand_1, expand_2);  expand_1 = expand_2 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_1: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(embedding, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_1: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_1, 2)
            mean: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_1, [-1], True);  pow_1 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean, 1e-05);  mean = None
            rsqrt: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add);  add = None
            mul: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_1, rsqrt);  _to_copy_1 = rsqrt = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_2: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul, dtype = torch.float32);  mul = None
            mul_1: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_0_input_layernorm_weight, _to_copy_2);  p_h_0_input_layernorm_weight = _to_copy_2 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:404 in forward, code: hidden_states = hidden_states.transpose(1, 0)
            transpose: "f32[6, 1, 5120]" = torch.ops.aten.transpose.int(mul_1, 1, 0);  mul_1 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:405 in forward, code: query_layer = self.query(hidden_states)
            linear: "f32[6, 1, 5120]" = torch.ops.aten.linear.default(transpose, p_h_0_self_attention_query_weight);  p_h_0_self_attention_query_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:409 in forward, code: query_layer = query_layer.view(*new_tensor_shape)
            view: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(linear, [6, 1, 32, 160]);  linear = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:411 in forward, code: mixed_kv_layer = self.key_value(hidden_states)
            linear_1: "f32[6, 1, 10240]" = torch.ops.aten.linear.default(transpose, p_h_0_self_attention_key_value_weight);  transpose = p_h_0_self_attention_key_value_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:415 in forward, code: mixed_kv_layer = mixed_kv_layer.view(*new_tensor_shape)
            view_1: "f32[6, 1, 32, 320]" = torch.ops.aten.view.default(linear_1, [6, 1, 32, 320]);  linear_1 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:381 in split_tensor_along_last_dim, code: tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)
            split = torch.ops.aten.split.Tensor(view_1, 160, 3);  view_1 = None
            getitem: "f32[6, 1, 32, 160]" = split[0]
            getitem_1: "f32[6, 1, 32, 160]" = split[1];  split = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:423 in forward, code: query_layer = query_layer.view(output_size[2], output_size[0] * output_size[1], -1)
            view_2: "f32[6, 32, 160]" = torch.ops.aten.view.default(view, [6, 32, -1]);  view = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:424 in forward, code: key_layer = key_layer.view(output_size[3], output_size[0] * output_size[1], -1)
            view_3: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem, [6, 32, -1]);  getitem = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:112 in forward, code: self.inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, device=x.device).float() / self.dim))
            arange_1: "i64[80]" = torch.ops.aten.arange.start_step(0, 160, 2, device = device(type='cpu'), pin_memory = False)
            _to_copy_3: "f32[80]" = torch.ops.aten._to_copy.default(arange_1, dtype = torch.float32);  arange_1 = None
            div: "f32[80]" = torch.ops.aten.div.Tensor(_to_copy_3, 160);  _to_copy_3 = None
            pow_2: "f32[80]" = torch.ops.aten.pow.Scalar(30420.108888514722, div);  div = None
            reciprocal: "f32[80]" = torch.ops.aten.reciprocal.default(pow_2);  pow_2 = None
            mul_2: "f32[80]" = torch.ops.aten.mul.Tensor(reciprocal, 1.0);  reciprocal = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:114 in forward, code: t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)
            arange_2: "f32[8192]" = torch.ops.aten.arange.default(8192, dtype = torch.float32, device = device(type='cpu'), pin_memory = False)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:115 in forward, code: freqs = torch.einsum('i,j->ij', t, self.inv_freq)
            einsum: "f32[8192, 80]" = torch.ops.aten.einsum.default('i,j->ij', [arange_2, mul_2]);  arange_2 = mul_2 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:117 in forward, code: emb = torch.cat((freqs, freqs), dim=-1).to(x.device)
            cat: "f32[8192, 160]" = torch.ops.aten.cat.default([einsum, einsum], -1);  einsum = None
            _to_copy_4: "f32[8192, 160]" = torch.ops.aten._to_copy.default(cat, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  cat = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:121 in forward, code: self.cos_cached = self.mscale * emb.cos()[:, None, :].to(dtype)
            cos: "f32[8192, 160]" = torch.ops.aten.cos.default(_to_copy_4)
            slice_14: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(cos, 0, 0, 9223372036854775807);  cos = None
            unsqueeze_8: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_14, 1);  slice_14 = None
            slice_15: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_8, 2, 0, 9223372036854775807);  unsqueeze_8 = None
            _to_copy_5: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_15, dtype = torch.float32);  slice_15 = None
            mul_3: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_5, 1.0);  _to_copy_5 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:122 in forward, code: self.sin_cached = self.mscale * emb.sin()[:, None, :].to(dtype)
            sin: "f32[8192, 160]" = torch.ops.aten.sin.default(_to_copy_4);  _to_copy_4 = None
            slice_16: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(sin, 0, 0, 9223372036854775807);  sin = None
            unsqueeze_9: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_16, 1);  slice_16 = None
            slice_17: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_9, 2, 0, 9223372036854775807);  unsqueeze_9 = None
            _to_copy_6: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_17, dtype = torch.float32);  slice_17 = None
            mul_4: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_6, 1.0);  _to_copy_6 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:123 in forward, code: return self.cos_cached[:seq_len, ...], self.sin_cached[:seq_len, ...]
            alias: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_3);  mul_3 = None
            alias_1: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_4);  mul_4 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:133 in apply_rotary_pos_emb_torch, code: cos, sin = cos[offset:q.shape[0] + offset, ...], sin[offset:q.shape[0] + offset, ...]
            slice_18: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias, 0, 0, 6);  alias = None
            slice_19: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_1, 0, 0, 6);  alias_1 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_5: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_2, slice_18)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_20: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_2, 2, 0, 80)
            slice_21: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_2, 2, 80, 9223372036854775807);  view_2 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_21);  slice_21 = None
            cat_1: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg, slice_20], 2);  neg = slice_20 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_6: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_1, slice_19);  cat_1 = None
            add_1: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_5, mul_6);  mul_5 = mul_6 = None
            mul_7: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_3, slice_18);  slice_18 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_22: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_3, 2, 0, 80)
            slice_23: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_3, 2, 80, 9223372036854775807);  view_3 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_1: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_23);  slice_23 = None
            cat_2: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_1, slice_22], 2);  neg_1 = slice_22 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_8: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_2, slice_19);  cat_2 = slice_19 = None
            add_2: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_7, mul_8);  mul_7 = mul_8 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:448 in forward, code: query_layer = query_layer.reshape((s_query, bz, head, dim))
            view_4: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_1, [6, 1, 32, 160]);  add_1 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:449 in forward, code: key_layer = key_layer.reshape((s_key, bz, head, dim))
            view_5: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_2, [6, 1, 32, 160]);  add_2 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:458 in forward, code: query_layer = query_layer.reshape(s_query, bz * self.num_heads, dim)
            view_6: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_4, [6, 32, 160]);  view_4 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:460 in forward, code: key_layer = key_layer.reshape(s_key, bz * self.num_heads, dim)
            view_7: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_5, [6, 32, 160]);  view_5 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            transpose_1: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_6, 0, 1);  view_6 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:462 in forward, code: key_layer.transpose(0, 1).transpose(1, 2))
            transpose_2: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_7, 0, 1);  view_7 = None
            transpose_3: "f32[32, 160, 6]" = torch.ops.aten.transpose.int(transpose_2, 1, 2);  transpose_2 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            einsum_1: "f32[32, 6, 6]" = torch.ops.aten.einsum.default('bik,bkj->bij', [transpose_1, transpose_3]);  transpose_1 = transpose_3 = None
            mul_9: "f32[32, 6, 6]" = torch.ops.aten.mul.Tensor(einsum_1, 0.07905694150420949);  einsum_1 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:464 in forward, code: attention_scores = matmul_result.view(bz, self.num_heads, s_query, s_key)
            view_8: "f32[1, 32, 6, 6]" = torch.ops.aten.view.default(mul_9, [1, 32, 6, 6]);  mul_9 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:469 in forward, code: attn_weights = torch.masked_fill(attention_scores, attention_mask, torch.finfo(attention_scores.dtype).min)
            masked_fill: "f32[1, 32, 6, 6]" = torch.ops.aten.masked_fill.Scalar(view_8, or_1, -3.4028234663852886e+38);  view_8 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:470 in forward, code: attention_probs = F.softmax(attn_weights, dim=-1).to(input_dtype)  ##dtype = torch.float32
            softmax: "f32[1, 32, 6, 6]" = torch.ops.aten.softmax.int(masked_fill, -1);  masked_fill = None
            _to_copy_7: "f32[1, 32, 6, 6]" = torch.ops.aten._to_copy.default(softmax, dtype = torch.float32);  softmax = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:471 in forward, code: attention_probs = self.attention_dropout(attention_probs)
            dropout: "f32[1, 32, 6, 6]" = torch.ops.aten.dropout.default(_to_copy_7, 0.0, False);  _to_copy_7 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:472 in forward, code: attention_probs_reshaped = attention_probs.view(bz * self.num_heads, s_query, s_key)
            view_9: "f32[32, 6, 6]" = torch.ops.aten.view.default(dropout, [32, 6, 6]);  dropout = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:474 in forward, code: value_layer = value_layer.reshape(s_key, bz * self.num_heads, dim)
            view_10: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_1, [6, 32, 160]);  getitem_1 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:475 in forward, code: context_layer = torch.bmm(attention_probs_reshaped, value_layer.transpose(0, 1))
            transpose_4: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_10, 0, 1);  view_10 = None
            bmm: "f32[32, 6, 160]" = torch.ops.aten.bmm.default(view_9, transpose_4);  view_9 = transpose_4 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:391 in _merge_heads, code: x = x.view(batch_size, self.num_heads, seq_length, self.head_dim)
            view_11: "f32[1, 32, 6, 160]" = torch.ops.aten.view.default(bmm, [1, 32, 6, 160]);  bmm = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:392 in _merge_heads, code: x = x.permute(0, 2, 1, 3)
            permute: "f32[1, 6, 32, 160]" = torch.ops.aten.permute.default(view_11, [0, 2, 1, 3]);  view_11 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:393 in _merge_heads, code: return x.reshape(batch_size, seq_length, self.num_heads * self.head_dim)
            clone: "f32[1, 6, 32, 160]" = torch.ops.aten.clone.default(permute, memory_format = torch.contiguous_format);  permute = None
            _unsafe_view: "f32[1, 6, 5120]" = torch.ops.aten._unsafe_view.default(clone, [1, 6, 5120]);  clone = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:478 in forward, code: output_tensor = self.dense(context_layer)
            linear_2: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(_unsafe_view, p_h_0_self_attention_dense_weight, p_h_0_self_attention_dense_bias);  _unsafe_view = p_h_0_self_attention_dense_weight = p_h_0_self_attention_dense_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_1: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_2, 0.0, False);  linear_2 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_3: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(embedding, dropout_1);  embedding = dropout_1 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_8: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_3, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_3: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_8, 2)
            mean_1: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_3, [-1], True);  pow_3 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_4: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_1, 1e-05);  mean_1 = None
            rsqrt_1: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_4);  add_4 = None
            mul_10: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_8, rsqrt_1);  _to_copy_8 = rsqrt_1 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_9: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_10, dtype = torch.float32);  mul_10 = None
            mul_11: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_0_post_attention_layernorm_weight, _to_copy_9);  p_h_0_post_attention_layernorm_weight = _to_copy_9 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:499 in forward, code: intermediate_output = self.down_proj(F.silu(self.gate_proj(hidden_states)) * self.up_proj(hidden_states))
            linear_3: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_11, p_h_0_mlp_gate_proj_weight);  p_h_0_mlp_gate_proj_weight = None
            silu: "f32[1, 6, 12288]" = torch.ops.aten.silu.default(linear_3);  linear_3 = None
            linear_4: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_11, p_h_0_mlp_up_proj_weight);  mul_11 = p_h_0_mlp_up_proj_weight = None
            mul_12: "f32[1, 6, 12288]" = torch.ops.aten.mul.Tensor(silu, linear_4);  silu = linear_4 = None
            linear_5: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(mul_12, p_h_0_mlp_down_proj_weight, p_h_0_mlp_down_proj_bias);  mul_12 = p_h_0_mlp_down_proj_weight = p_h_0_mlp_down_proj_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_2: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_5, 0.0, False);  linear_5 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_5: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_3, dropout_2);  add_3 = dropout_2 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_10: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_5, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_4: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_10, 2)
            mean_2: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_4, [-1], True);  pow_4 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_6: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_2, 1e-05);  mean_2 = None
            rsqrt_2: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_6);  add_6 = None
            mul_13: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_10, rsqrt_2);  _to_copy_10 = rsqrt_2 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_11: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_13, dtype = torch.float32);  mul_13 = None
            mul_14: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_1_input_layernorm_weight, _to_copy_11);  p_h_1_input_layernorm_weight = _to_copy_11 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:404 in forward, code: hidden_states = hidden_states.transpose(1, 0)
            transpose_5: "f32[6, 1, 5120]" = torch.ops.aten.transpose.int(mul_14, 1, 0);  mul_14 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:405 in forward, code: query_layer = self.query(hidden_states)
            linear_6: "f32[6, 1, 5120]" = torch.ops.aten.linear.default(transpose_5, p_h_1_self_attention_query_weight);  p_h_1_self_attention_query_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:409 in forward, code: query_layer = query_layer.view(*new_tensor_shape)
            view_12: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(linear_6, [6, 1, 32, 160]);  linear_6 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:411 in forward, code: mixed_kv_layer = self.key_value(hidden_states)
            linear_7: "f32[6, 1, 10240]" = torch.ops.aten.linear.default(transpose_5, p_h_1_self_attention_key_value_weight);  transpose_5 = p_h_1_self_attention_key_value_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:415 in forward, code: mixed_kv_layer = mixed_kv_layer.view(*new_tensor_shape)
            view_13: "f32[6, 1, 32, 320]" = torch.ops.aten.view.default(linear_7, [6, 1, 32, 320]);  linear_7 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:381 in split_tensor_along_last_dim, code: tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)
            split_1 = torch.ops.aten.split.Tensor(view_13, 160, 3);  view_13 = None
            getitem_2: "f32[6, 1, 32, 160]" = split_1[0]
            getitem_3: "f32[6, 1, 32, 160]" = split_1[1];  split_1 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:423 in forward, code: query_layer = query_layer.view(output_size[2], output_size[0] * output_size[1], -1)
            view_14: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_12, [6, 32, -1]);  view_12 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:424 in forward, code: key_layer = key_layer.view(output_size[3], output_size[0] * output_size[1], -1)
            view_15: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_2, [6, 32, -1]);  getitem_2 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:112 in forward, code: self.inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, device=x.device).float() / self.dim))
            arange_3: "i64[80]" = torch.ops.aten.arange.start_step(0, 160, 2, device = device(type='cpu'), pin_memory = False)
            _to_copy_12: "f32[80]" = torch.ops.aten._to_copy.default(arange_3, dtype = torch.float32);  arange_3 = None
            div_1: "f32[80]" = torch.ops.aten.div.Tensor(_to_copy_12, 160);  _to_copy_12 = None
            pow_5: "f32[80]" = torch.ops.aten.pow.Scalar(30420.108888514722, div_1);  div_1 = None
            reciprocal_1: "f32[80]" = torch.ops.aten.reciprocal.default(pow_5);  pow_5 = None
            mul_15: "f32[80]" = torch.ops.aten.mul.Tensor(reciprocal_1, 1.0);  reciprocal_1 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:114 in forward, code: t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)
            arange_4: "f32[8192]" = torch.ops.aten.arange.default(8192, dtype = torch.float32, device = device(type='cpu'), pin_memory = False)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:115 in forward, code: freqs = torch.einsum('i,j->ij', t, self.inv_freq)
            einsum_2: "f32[8192, 80]" = torch.ops.aten.einsum.default('i,j->ij', [arange_4, mul_15]);  arange_4 = mul_15 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:117 in forward, code: emb = torch.cat((freqs, freqs), dim=-1).to(x.device)
            cat_3: "f32[8192, 160]" = torch.ops.aten.cat.default([einsum_2, einsum_2], -1);  einsum_2 = None
            _to_copy_13: "f32[8192, 160]" = torch.ops.aten._to_copy.default(cat_3, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  cat_3 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:121 in forward, code: self.cos_cached = self.mscale * emb.cos()[:, None, :].to(dtype)
            cos_1: "f32[8192, 160]" = torch.ops.aten.cos.default(_to_copy_13)
            slice_24: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(cos_1, 0, 0, 9223372036854775807);  cos_1 = None
            unsqueeze_10: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_24, 1);  slice_24 = None
            slice_25: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_10, 2, 0, 9223372036854775807);  unsqueeze_10 = None
            _to_copy_14: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_25, dtype = torch.float32);  slice_25 = None
            mul_16: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_14, 1.0);  _to_copy_14 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:122 in forward, code: self.sin_cached = self.mscale * emb.sin()[:, None, :].to(dtype)
            sin_1: "f32[8192, 160]" = torch.ops.aten.sin.default(_to_copy_13);  _to_copy_13 = None
            slice_26: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(sin_1, 0, 0, 9223372036854775807);  sin_1 = None
            unsqueeze_11: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_26, 1);  slice_26 = None
            slice_27: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_11, 2, 0, 9223372036854775807);  unsqueeze_11 = None
            _to_copy_15: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_27, dtype = torch.float32);  slice_27 = None
            mul_17: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_15, 1.0);  _to_copy_15 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:123 in forward, code: return self.cos_cached[:seq_len, ...], self.sin_cached[:seq_len, ...]
            alias_2: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_16);  mul_16 = None
            alias_3: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_17);  mul_17 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:133 in apply_rotary_pos_emb_torch, code: cos, sin = cos[offset:q.shape[0] + offset, ...], sin[offset:q.shape[0] + offset, ...]
            slice_28: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_2, 0, 0, 6);  alias_2 = None
            slice_29: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_3, 0, 0, 6);  alias_3 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_18: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_14, slice_28)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_30: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_14, 2, 0, 80)
            slice_31: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_14, 2, 80, 9223372036854775807);  view_14 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_2: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_31);  slice_31 = None
            cat_4: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_2, slice_30], 2);  neg_2 = slice_30 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_19: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_4, slice_29);  cat_4 = None
            add_7: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_18, mul_19);  mul_18 = mul_19 = None
            mul_20: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_15, slice_28);  slice_28 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_32: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_15, 2, 0, 80)
            slice_33: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_15, 2, 80, 9223372036854775807);  view_15 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_3: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_33);  slice_33 = None
            cat_5: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_3, slice_32], 2);  neg_3 = slice_32 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_21: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_5, slice_29);  cat_5 = slice_29 = None
            add_8: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_20, mul_21);  mul_20 = mul_21 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:448 in forward, code: query_layer = query_layer.reshape((s_query, bz, head, dim))
            view_16: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_7, [6, 1, 32, 160]);  add_7 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:449 in forward, code: key_layer = key_layer.reshape((s_key, bz, head, dim))
            view_17: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_8, [6, 1, 32, 160]);  add_8 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:458 in forward, code: query_layer = query_layer.reshape(s_query, bz * self.num_heads, dim)
            view_18: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_16, [6, 32, 160]);  view_16 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:460 in forward, code: key_layer = key_layer.reshape(s_key, bz * self.num_heads, dim)
            view_19: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_17, [6, 32, 160]);  view_17 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            transpose_6: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_18, 0, 1);  view_18 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:462 in forward, code: key_layer.transpose(0, 1).transpose(1, 2))
            transpose_7: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_19, 0, 1);  view_19 = None
            transpose_8: "f32[32, 160, 6]" = torch.ops.aten.transpose.int(transpose_7, 1, 2);  transpose_7 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            einsum_3: "f32[32, 6, 6]" = torch.ops.aten.einsum.default('bik,bkj->bij', [transpose_6, transpose_8]);  transpose_6 = transpose_8 = None
            mul_22: "f32[32, 6, 6]" = torch.ops.aten.mul.Tensor(einsum_3, 0.07905694150420949);  einsum_3 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:464 in forward, code: attention_scores = matmul_result.view(bz, self.num_heads, s_query, s_key)
            view_20: "f32[1, 32, 6, 6]" = torch.ops.aten.view.default(mul_22, [1, 32, 6, 6]);  mul_22 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:469 in forward, code: attn_weights = torch.masked_fill(attention_scores, attention_mask, torch.finfo(attention_scores.dtype).min)
            masked_fill_1: "f32[1, 32, 6, 6]" = torch.ops.aten.masked_fill.Scalar(view_20, or_1, -3.4028234663852886e+38);  view_20 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:470 in forward, code: attention_probs = F.softmax(attn_weights, dim=-1).to(input_dtype)  ##dtype = torch.float32
            softmax_1: "f32[1, 32, 6, 6]" = torch.ops.aten.softmax.int(masked_fill_1, -1);  masked_fill_1 = None
            _to_copy_16: "f32[1, 32, 6, 6]" = torch.ops.aten._to_copy.default(softmax_1, dtype = torch.float32);  softmax_1 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:471 in forward, code: attention_probs = self.attention_dropout(attention_probs)
            dropout_3: "f32[1, 32, 6, 6]" = torch.ops.aten.dropout.default(_to_copy_16, 0.0, False);  _to_copy_16 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:472 in forward, code: attention_probs_reshaped = attention_probs.view(bz * self.num_heads, s_query, s_key)
            view_21: "f32[32, 6, 6]" = torch.ops.aten.view.default(dropout_3, [32, 6, 6]);  dropout_3 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:474 in forward, code: value_layer = value_layer.reshape(s_key, bz * self.num_heads, dim)
            view_22: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_3, [6, 32, 160]);  getitem_3 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:475 in forward, code: context_layer = torch.bmm(attention_probs_reshaped, value_layer.transpose(0, 1))
            transpose_9: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_22, 0, 1);  view_22 = None
            bmm_1: "f32[32, 6, 160]" = torch.ops.aten.bmm.default(view_21, transpose_9);  view_21 = transpose_9 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:391 in _merge_heads, code: x = x.view(batch_size, self.num_heads, seq_length, self.head_dim)
            view_23: "f32[1, 32, 6, 160]" = torch.ops.aten.view.default(bmm_1, [1, 32, 6, 160]);  bmm_1 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:392 in _merge_heads, code: x = x.permute(0, 2, 1, 3)
            permute_1: "f32[1, 6, 32, 160]" = torch.ops.aten.permute.default(view_23, [0, 2, 1, 3]);  view_23 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:393 in _merge_heads, code: return x.reshape(batch_size, seq_length, self.num_heads * self.head_dim)
            clone_1: "f32[1, 6, 32, 160]" = torch.ops.aten.clone.default(permute_1, memory_format = torch.contiguous_format);  permute_1 = None
            _unsafe_view_1: "f32[1, 6, 5120]" = torch.ops.aten._unsafe_view.default(clone_1, [1, 6, 5120]);  clone_1 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:478 in forward, code: output_tensor = self.dense(context_layer)
            linear_8: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(_unsafe_view_1, p_h_1_self_attention_dense_weight, p_h_1_self_attention_dense_bias);  _unsafe_view_1 = p_h_1_self_attention_dense_weight = p_h_1_self_attention_dense_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_4: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_8, 0.0, False);  linear_8 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_9: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_5, dropout_4);  add_5 = dropout_4 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_17: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_9, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_6: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_17, 2)
            mean_3: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_6, [-1], True);  pow_6 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_10: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_3, 1e-05);  mean_3 = None
            rsqrt_3: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_10);  add_10 = None
            mul_23: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_17, rsqrt_3);  _to_copy_17 = rsqrt_3 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_18: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_23, dtype = torch.float32);  mul_23 = None
            mul_24: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_1_post_attention_layernorm_weight, _to_copy_18);  p_h_1_post_attention_layernorm_weight = _to_copy_18 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:499 in forward, code: intermediate_output = self.down_proj(F.silu(self.gate_proj(hidden_states)) * self.up_proj(hidden_states))
            linear_9: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_24, p_h_1_mlp_gate_proj_weight);  p_h_1_mlp_gate_proj_weight = None
            silu_1: "f32[1, 6, 12288]" = torch.ops.aten.silu.default(linear_9);  linear_9 = None
            linear_10: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_24, p_h_1_mlp_up_proj_weight);  mul_24 = p_h_1_mlp_up_proj_weight = None
            mul_25: "f32[1, 6, 12288]" = torch.ops.aten.mul.Tensor(silu_1, linear_10);  silu_1 = linear_10 = None
            linear_11: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(mul_25, p_h_1_mlp_down_proj_weight, p_h_1_mlp_down_proj_bias);  mul_25 = p_h_1_mlp_down_proj_weight = p_h_1_mlp_down_proj_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_5: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_11, 0.0, False);  linear_11 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_11: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_9, dropout_5);  add_9 = dropout_5 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_19: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_11, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_7: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_19, 2)
            mean_4: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_7, [-1], True);  pow_7 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_12: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_4, 1e-05);  mean_4 = None
            rsqrt_4: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_12);  add_12 = None
            mul_26: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_19, rsqrt_4);  _to_copy_19 = rsqrt_4 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_20: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_26, dtype = torch.float32);  mul_26 = None
            mul_27: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_2_input_layernorm_weight, _to_copy_20);  p_h_2_input_layernorm_weight = _to_copy_20 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:404 in forward, code: hidden_states = hidden_states.transpose(1, 0)
            transpose_10: "f32[6, 1, 5120]" = torch.ops.aten.transpose.int(mul_27, 1, 0);  mul_27 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:405 in forward, code: query_layer = self.query(hidden_states)
            linear_12: "f32[6, 1, 5120]" = torch.ops.aten.linear.default(transpose_10, p_h_2_self_attention_query_weight);  p_h_2_self_attention_query_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:409 in forward, code: query_layer = query_layer.view(*new_tensor_shape)
            view_24: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(linear_12, [6, 1, 32, 160]);  linear_12 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:411 in forward, code: mixed_kv_layer = self.key_value(hidden_states)
            linear_13: "f32[6, 1, 10240]" = torch.ops.aten.linear.default(transpose_10, p_h_2_self_attention_key_value_weight);  transpose_10 = p_h_2_self_attention_key_value_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:415 in forward, code: mixed_kv_layer = mixed_kv_layer.view(*new_tensor_shape)
            view_25: "f32[6, 1, 32, 320]" = torch.ops.aten.view.default(linear_13, [6, 1, 32, 320]);  linear_13 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:381 in split_tensor_along_last_dim, code: tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)
            split_2 = torch.ops.aten.split.Tensor(view_25, 160, 3);  view_25 = None
            getitem_4: "f32[6, 1, 32, 160]" = split_2[0]
            getitem_5: "f32[6, 1, 32, 160]" = split_2[1];  split_2 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:423 in forward, code: query_layer = query_layer.view(output_size[2], output_size[0] * output_size[1], -1)
            view_26: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_24, [6, 32, -1]);  view_24 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:424 in forward, code: key_layer = key_layer.view(output_size[3], output_size[0] * output_size[1], -1)
            view_27: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_4, [6, 32, -1]);  getitem_4 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:112 in forward, code: self.inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, device=x.device).float() / self.dim))
            arange_5: "i64[80]" = torch.ops.aten.arange.start_step(0, 160, 2, device = device(type='cpu'), pin_memory = False)
            _to_copy_21: "f32[80]" = torch.ops.aten._to_copy.default(arange_5, dtype = torch.float32);  arange_5 = None
            div_2: "f32[80]" = torch.ops.aten.div.Tensor(_to_copy_21, 160);  _to_copy_21 = None
            pow_8: "f32[80]" = torch.ops.aten.pow.Scalar(30420.108888514722, div_2);  div_2 = None
            reciprocal_2: "f32[80]" = torch.ops.aten.reciprocal.default(pow_8);  pow_8 = None
            mul_28: "f32[80]" = torch.ops.aten.mul.Tensor(reciprocal_2, 1.0);  reciprocal_2 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:114 in forward, code: t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)
            arange_6: "f32[8192]" = torch.ops.aten.arange.default(8192, dtype = torch.float32, device = device(type='cpu'), pin_memory = False)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:115 in forward, code: freqs = torch.einsum('i,j->ij', t, self.inv_freq)
            einsum_4: "f32[8192, 80]" = torch.ops.aten.einsum.default('i,j->ij', [arange_6, mul_28]);  arange_6 = mul_28 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:117 in forward, code: emb = torch.cat((freqs, freqs), dim=-1).to(x.device)
            cat_6: "f32[8192, 160]" = torch.ops.aten.cat.default([einsum_4, einsum_4], -1);  einsum_4 = None
            _to_copy_22: "f32[8192, 160]" = torch.ops.aten._to_copy.default(cat_6, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  cat_6 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:121 in forward, code: self.cos_cached = self.mscale * emb.cos()[:, None, :].to(dtype)
            cos_2: "f32[8192, 160]" = torch.ops.aten.cos.default(_to_copy_22)
            slice_34: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(cos_2, 0, 0, 9223372036854775807);  cos_2 = None
            unsqueeze_12: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_34, 1);  slice_34 = None
            slice_35: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_12, 2, 0, 9223372036854775807);  unsqueeze_12 = None
            _to_copy_23: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_35, dtype = torch.float32);  slice_35 = None
            mul_29: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_23, 1.0);  _to_copy_23 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:122 in forward, code: self.sin_cached = self.mscale * emb.sin()[:, None, :].to(dtype)
            sin_2: "f32[8192, 160]" = torch.ops.aten.sin.default(_to_copy_22);  _to_copy_22 = None
            slice_36: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(sin_2, 0, 0, 9223372036854775807);  sin_2 = None
            unsqueeze_13: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_36, 1);  slice_36 = None
            slice_37: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_13, 2, 0, 9223372036854775807);  unsqueeze_13 = None
            _to_copy_24: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_37, dtype = torch.float32);  slice_37 = None
            mul_30: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_24, 1.0);  _to_copy_24 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:123 in forward, code: return self.cos_cached[:seq_len, ...], self.sin_cached[:seq_len, ...]
            alias_4: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_29);  mul_29 = None
            alias_5: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_30);  mul_30 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:133 in apply_rotary_pos_emb_torch, code: cos, sin = cos[offset:q.shape[0] + offset, ...], sin[offset:q.shape[0] + offset, ...]
            slice_38: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_4, 0, 0, 6);  alias_4 = None
            slice_39: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_5, 0, 0, 6);  alias_5 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_31: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_26, slice_38)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_40: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_26, 2, 0, 80)
            slice_41: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_26, 2, 80, 9223372036854775807);  view_26 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_4: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_41);  slice_41 = None
            cat_7: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_4, slice_40], 2);  neg_4 = slice_40 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_32: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_7, slice_39);  cat_7 = None
            add_13: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_31, mul_32);  mul_31 = mul_32 = None
            mul_33: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_27, slice_38);  slice_38 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_42: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_27, 2, 0, 80)
            slice_43: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_27, 2, 80, 9223372036854775807);  view_27 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_5: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_43);  slice_43 = None
            cat_8: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_5, slice_42], 2);  neg_5 = slice_42 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_34: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_8, slice_39);  cat_8 = slice_39 = None
            add_14: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_33, mul_34);  mul_33 = mul_34 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:448 in forward, code: query_layer = query_layer.reshape((s_query, bz, head, dim))
            view_28: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_13, [6, 1, 32, 160]);  add_13 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:449 in forward, code: key_layer = key_layer.reshape((s_key, bz, head, dim))
            view_29: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_14, [6, 1, 32, 160]);  add_14 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:458 in forward, code: query_layer = query_layer.reshape(s_query, bz * self.num_heads, dim)
            view_30: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_28, [6, 32, 160]);  view_28 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:460 in forward, code: key_layer = key_layer.reshape(s_key, bz * self.num_heads, dim)
            view_31: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_29, [6, 32, 160]);  view_29 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            transpose_11: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_30, 0, 1);  view_30 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:462 in forward, code: key_layer.transpose(0, 1).transpose(1, 2))
            transpose_12: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_31, 0, 1);  view_31 = None
            transpose_13: "f32[32, 160, 6]" = torch.ops.aten.transpose.int(transpose_12, 1, 2);  transpose_12 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            einsum_5: "f32[32, 6, 6]" = torch.ops.aten.einsum.default('bik,bkj->bij', [transpose_11, transpose_13]);  transpose_11 = transpose_13 = None
            mul_35: "f32[32, 6, 6]" = torch.ops.aten.mul.Tensor(einsum_5, 0.07905694150420949);  einsum_5 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:464 in forward, code: attention_scores = matmul_result.view(bz, self.num_heads, s_query, s_key)
            view_32: "f32[1, 32, 6, 6]" = torch.ops.aten.view.default(mul_35, [1, 32, 6, 6]);  mul_35 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:469 in forward, code: attn_weights = torch.masked_fill(attention_scores, attention_mask, torch.finfo(attention_scores.dtype).min)
            masked_fill_2: "f32[1, 32, 6, 6]" = torch.ops.aten.masked_fill.Scalar(view_32, or_1, -3.4028234663852886e+38);  view_32 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:470 in forward, code: attention_probs = F.softmax(attn_weights, dim=-1).to(input_dtype)  ##dtype = torch.float32
            softmax_2: "f32[1, 32, 6, 6]" = torch.ops.aten.softmax.int(masked_fill_2, -1);  masked_fill_2 = None
            _to_copy_25: "f32[1, 32, 6, 6]" = torch.ops.aten._to_copy.default(softmax_2, dtype = torch.float32);  softmax_2 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:471 in forward, code: attention_probs = self.attention_dropout(attention_probs)
            dropout_6: "f32[1, 32, 6, 6]" = torch.ops.aten.dropout.default(_to_copy_25, 0.0, False);  _to_copy_25 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:472 in forward, code: attention_probs_reshaped = attention_probs.view(bz * self.num_heads, s_query, s_key)
            view_33: "f32[32, 6, 6]" = torch.ops.aten.view.default(dropout_6, [32, 6, 6]);  dropout_6 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:474 in forward, code: value_layer = value_layer.reshape(s_key, bz * self.num_heads, dim)
            view_34: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_5, [6, 32, 160]);  getitem_5 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:475 in forward, code: context_layer = torch.bmm(attention_probs_reshaped, value_layer.transpose(0, 1))
            transpose_14: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_34, 0, 1);  view_34 = None
            bmm_2: "f32[32, 6, 160]" = torch.ops.aten.bmm.default(view_33, transpose_14);  view_33 = transpose_14 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:391 in _merge_heads, code: x = x.view(batch_size, self.num_heads, seq_length, self.head_dim)
            view_35: "f32[1, 32, 6, 160]" = torch.ops.aten.view.default(bmm_2, [1, 32, 6, 160]);  bmm_2 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:392 in _merge_heads, code: x = x.permute(0, 2, 1, 3)
            permute_2: "f32[1, 6, 32, 160]" = torch.ops.aten.permute.default(view_35, [0, 2, 1, 3]);  view_35 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:393 in _merge_heads, code: return x.reshape(batch_size, seq_length, self.num_heads * self.head_dim)
            clone_2: "f32[1, 6, 32, 160]" = torch.ops.aten.clone.default(permute_2, memory_format = torch.contiguous_format);  permute_2 = None
            _unsafe_view_2: "f32[1, 6, 5120]" = torch.ops.aten._unsafe_view.default(clone_2, [1, 6, 5120]);  clone_2 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:478 in forward, code: output_tensor = self.dense(context_layer)
            linear_14: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(_unsafe_view_2, p_h_2_self_attention_dense_weight, p_h_2_self_attention_dense_bias);  _unsafe_view_2 = p_h_2_self_attention_dense_weight = p_h_2_self_attention_dense_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_7: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_14, 0.0, False);  linear_14 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_15: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_11, dropout_7);  add_11 = dropout_7 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_26: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_15, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_9: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_26, 2)
            mean_5: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_9, [-1], True);  pow_9 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_16: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_5, 1e-05);  mean_5 = None
            rsqrt_5: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_16);  add_16 = None
            mul_36: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_26, rsqrt_5);  _to_copy_26 = rsqrt_5 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_27: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_36, dtype = torch.float32);  mul_36 = None
            mul_37: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_2_post_attention_layernorm_weight, _to_copy_27);  p_h_2_post_attention_layernorm_weight = _to_copy_27 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:499 in forward, code: intermediate_output = self.down_proj(F.silu(self.gate_proj(hidden_states)) * self.up_proj(hidden_states))
            linear_15: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_37, p_h_2_mlp_gate_proj_weight);  p_h_2_mlp_gate_proj_weight = None
            silu_2: "f32[1, 6, 12288]" = torch.ops.aten.silu.default(linear_15);  linear_15 = None
            linear_16: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_37, p_h_2_mlp_up_proj_weight);  mul_37 = p_h_2_mlp_up_proj_weight = None
            mul_38: "f32[1, 6, 12288]" = torch.ops.aten.mul.Tensor(silu_2, linear_16);  silu_2 = linear_16 = None
            linear_17: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(mul_38, p_h_2_mlp_down_proj_weight, p_h_2_mlp_down_proj_bias);  mul_38 = p_h_2_mlp_down_proj_weight = p_h_2_mlp_down_proj_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_8: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_17, 0.0, False);  linear_17 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_17: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_15, dropout_8);  add_15 = dropout_8 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_28: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_17, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_10: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_28, 2)
            mean_6: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_10, [-1], True);  pow_10 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_18: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_6, 1e-05);  mean_6 = None
            rsqrt_6: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_18);  add_18 = None
            mul_39: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_28, rsqrt_6);  _to_copy_28 = rsqrt_6 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_29: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_39, dtype = torch.float32);  mul_39 = None
            mul_40: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_3_input_layernorm_weight, _to_copy_29);  p_h_3_input_layernorm_weight = _to_copy_29 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:404 in forward, code: hidden_states = hidden_states.transpose(1, 0)
            transpose_15: "f32[6, 1, 5120]" = torch.ops.aten.transpose.int(mul_40, 1, 0);  mul_40 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:405 in forward, code: query_layer = self.query(hidden_states)
            linear_18: "f32[6, 1, 5120]" = torch.ops.aten.linear.default(transpose_15, p_h_3_self_attention_query_weight);  p_h_3_self_attention_query_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:409 in forward, code: query_layer = query_layer.view(*new_tensor_shape)
            view_36: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(linear_18, [6, 1, 32, 160]);  linear_18 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:411 in forward, code: mixed_kv_layer = self.key_value(hidden_states)
            linear_19: "f32[6, 1, 10240]" = torch.ops.aten.linear.default(transpose_15, p_h_3_self_attention_key_value_weight);  transpose_15 = p_h_3_self_attention_key_value_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:415 in forward, code: mixed_kv_layer = mixed_kv_layer.view(*new_tensor_shape)
            view_37: "f32[6, 1, 32, 320]" = torch.ops.aten.view.default(linear_19, [6, 1, 32, 320]);  linear_19 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:381 in split_tensor_along_last_dim, code: tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)
            split_3 = torch.ops.aten.split.Tensor(view_37, 160, 3);  view_37 = None
            getitem_6: "f32[6, 1, 32, 160]" = split_3[0]
            getitem_7: "f32[6, 1, 32, 160]" = split_3[1];  split_3 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:423 in forward, code: query_layer = query_layer.view(output_size[2], output_size[0] * output_size[1], -1)
            view_38: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_36, [6, 32, -1]);  view_36 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:424 in forward, code: key_layer = key_layer.view(output_size[3], output_size[0] * output_size[1], -1)
            view_39: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_6, [6, 32, -1]);  getitem_6 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:112 in forward, code: self.inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, device=x.device).float() / self.dim))
            arange_7: "i64[80]" = torch.ops.aten.arange.start_step(0, 160, 2, device = device(type='cpu'), pin_memory = False)
            _to_copy_30: "f32[80]" = torch.ops.aten._to_copy.default(arange_7, dtype = torch.float32);  arange_7 = None
            div_3: "f32[80]" = torch.ops.aten.div.Tensor(_to_copy_30, 160);  _to_copy_30 = None
            pow_11: "f32[80]" = torch.ops.aten.pow.Scalar(30420.108888514722, div_3);  div_3 = None
            reciprocal_3: "f32[80]" = torch.ops.aten.reciprocal.default(pow_11);  pow_11 = None
            mul_41: "f32[80]" = torch.ops.aten.mul.Tensor(reciprocal_3, 1.0);  reciprocal_3 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:114 in forward, code: t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)
            arange_8: "f32[8192]" = torch.ops.aten.arange.default(8192, dtype = torch.float32, device = device(type='cpu'), pin_memory = False)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:115 in forward, code: freqs = torch.einsum('i,j->ij', t, self.inv_freq)
            einsum_6: "f32[8192, 80]" = torch.ops.aten.einsum.default('i,j->ij', [arange_8, mul_41]);  arange_8 = mul_41 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:117 in forward, code: emb = torch.cat((freqs, freqs), dim=-1).to(x.device)
            cat_9: "f32[8192, 160]" = torch.ops.aten.cat.default([einsum_6, einsum_6], -1);  einsum_6 = None
            _to_copy_31: "f32[8192, 160]" = torch.ops.aten._to_copy.default(cat_9, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  cat_9 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:121 in forward, code: self.cos_cached = self.mscale * emb.cos()[:, None, :].to(dtype)
            cos_3: "f32[8192, 160]" = torch.ops.aten.cos.default(_to_copy_31)
            slice_44: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(cos_3, 0, 0, 9223372036854775807);  cos_3 = None
            unsqueeze_14: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_44, 1);  slice_44 = None
            slice_45: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_14, 2, 0, 9223372036854775807);  unsqueeze_14 = None
            _to_copy_32: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_45, dtype = torch.float32);  slice_45 = None
            mul_42: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_32, 1.0);  _to_copy_32 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:122 in forward, code: self.sin_cached = self.mscale * emb.sin()[:, None, :].to(dtype)
            sin_3: "f32[8192, 160]" = torch.ops.aten.sin.default(_to_copy_31);  _to_copy_31 = None
            slice_46: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(sin_3, 0, 0, 9223372036854775807);  sin_3 = None
            unsqueeze_15: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_46, 1);  slice_46 = None
            slice_47: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_15, 2, 0, 9223372036854775807);  unsqueeze_15 = None
            _to_copy_33: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_47, dtype = torch.float32);  slice_47 = None
            mul_43: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_33, 1.0);  _to_copy_33 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:123 in forward, code: return self.cos_cached[:seq_len, ...], self.sin_cached[:seq_len, ...]
            alias_6: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_42);  mul_42 = None
            alias_7: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_43);  mul_43 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:133 in apply_rotary_pos_emb_torch, code: cos, sin = cos[offset:q.shape[0] + offset, ...], sin[offset:q.shape[0] + offset, ...]
            slice_48: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_6, 0, 0, 6);  alias_6 = None
            slice_49: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_7, 0, 0, 6);  alias_7 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_44: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_38, slice_48)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_50: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_38, 2, 0, 80)
            slice_51: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_38, 2, 80, 9223372036854775807);  view_38 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_6: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_51);  slice_51 = None
            cat_10: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_6, slice_50], 2);  neg_6 = slice_50 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_45: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_10, slice_49);  cat_10 = None
            add_19: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_44, mul_45);  mul_44 = mul_45 = None
            mul_46: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_39, slice_48);  slice_48 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_52: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_39, 2, 0, 80)
            slice_53: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_39, 2, 80, 9223372036854775807);  view_39 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_7: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_53);  slice_53 = None
            cat_11: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_7, slice_52], 2);  neg_7 = slice_52 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_47: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_11, slice_49);  cat_11 = slice_49 = None
            add_20: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_46, mul_47);  mul_46 = mul_47 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:448 in forward, code: query_layer = query_layer.reshape((s_query, bz, head, dim))
            view_40: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_19, [6, 1, 32, 160]);  add_19 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:449 in forward, code: key_layer = key_layer.reshape((s_key, bz, head, dim))
            view_41: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_20, [6, 1, 32, 160]);  add_20 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:458 in forward, code: query_layer = query_layer.reshape(s_query, bz * self.num_heads, dim)
            view_42: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_40, [6, 32, 160]);  view_40 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:460 in forward, code: key_layer = key_layer.reshape(s_key, bz * self.num_heads, dim)
            view_43: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_41, [6, 32, 160]);  view_41 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            transpose_16: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_42, 0, 1);  view_42 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:462 in forward, code: key_layer.transpose(0, 1).transpose(1, 2))
            transpose_17: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_43, 0, 1);  view_43 = None
            transpose_18: "f32[32, 160, 6]" = torch.ops.aten.transpose.int(transpose_17, 1, 2);  transpose_17 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            einsum_7: "f32[32, 6, 6]" = torch.ops.aten.einsum.default('bik,bkj->bij', [transpose_16, transpose_18]);  transpose_16 = transpose_18 = None
            mul_48: "f32[32, 6, 6]" = torch.ops.aten.mul.Tensor(einsum_7, 0.07905694150420949);  einsum_7 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:464 in forward, code: attention_scores = matmul_result.view(bz, self.num_heads, s_query, s_key)
            view_44: "f32[1, 32, 6, 6]" = torch.ops.aten.view.default(mul_48, [1, 32, 6, 6]);  mul_48 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:469 in forward, code: attn_weights = torch.masked_fill(attention_scores, attention_mask, torch.finfo(attention_scores.dtype).min)
            masked_fill_3: "f32[1, 32, 6, 6]" = torch.ops.aten.masked_fill.Scalar(view_44, or_1, -3.4028234663852886e+38);  view_44 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:470 in forward, code: attention_probs = F.softmax(attn_weights, dim=-1).to(input_dtype)  ##dtype = torch.float32
            softmax_3: "f32[1, 32, 6, 6]" = torch.ops.aten.softmax.int(masked_fill_3, -1);  masked_fill_3 = None
            _to_copy_34: "f32[1, 32, 6, 6]" = torch.ops.aten._to_copy.default(softmax_3, dtype = torch.float32);  softmax_3 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:471 in forward, code: attention_probs = self.attention_dropout(attention_probs)
            dropout_9: "f32[1, 32, 6, 6]" = torch.ops.aten.dropout.default(_to_copy_34, 0.0, False);  _to_copy_34 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:472 in forward, code: attention_probs_reshaped = attention_probs.view(bz * self.num_heads, s_query, s_key)
            view_45: "f32[32, 6, 6]" = torch.ops.aten.view.default(dropout_9, [32, 6, 6]);  dropout_9 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:474 in forward, code: value_layer = value_layer.reshape(s_key, bz * self.num_heads, dim)
            view_46: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_7, [6, 32, 160]);  getitem_7 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:475 in forward, code: context_layer = torch.bmm(attention_probs_reshaped, value_layer.transpose(0, 1))
            transpose_19: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_46, 0, 1);  view_46 = None
            bmm_3: "f32[32, 6, 160]" = torch.ops.aten.bmm.default(view_45, transpose_19);  view_45 = transpose_19 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:391 in _merge_heads, code: x = x.view(batch_size, self.num_heads, seq_length, self.head_dim)
            view_47: "f32[1, 32, 6, 160]" = torch.ops.aten.view.default(bmm_3, [1, 32, 6, 160]);  bmm_3 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:392 in _merge_heads, code: x = x.permute(0, 2, 1, 3)
            permute_3: "f32[1, 6, 32, 160]" = torch.ops.aten.permute.default(view_47, [0, 2, 1, 3]);  view_47 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:393 in _merge_heads, code: return x.reshape(batch_size, seq_length, self.num_heads * self.head_dim)
            clone_3: "f32[1, 6, 32, 160]" = torch.ops.aten.clone.default(permute_3, memory_format = torch.contiguous_format);  permute_3 = None
            _unsafe_view_3: "f32[1, 6, 5120]" = torch.ops.aten._unsafe_view.default(clone_3, [1, 6, 5120]);  clone_3 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:478 in forward, code: output_tensor = self.dense(context_layer)
            linear_20: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(_unsafe_view_3, p_h_3_self_attention_dense_weight, p_h_3_self_attention_dense_bias);  _unsafe_view_3 = p_h_3_self_attention_dense_weight = p_h_3_self_attention_dense_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_10: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_20, 0.0, False);  linear_20 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_21: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_17, dropout_10);  add_17 = dropout_10 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_35: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_21, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_12: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_35, 2)
            mean_7: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_12, [-1], True);  pow_12 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_22: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_7, 1e-05);  mean_7 = None
            rsqrt_7: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_22);  add_22 = None
            mul_49: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_35, rsqrt_7);  _to_copy_35 = rsqrt_7 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_36: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_49, dtype = torch.float32);  mul_49 = None
            mul_50: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_3_post_attention_layernorm_weight, _to_copy_36);  p_h_3_post_attention_layernorm_weight = _to_copy_36 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:499 in forward, code: intermediate_output = self.down_proj(F.silu(self.gate_proj(hidden_states)) * self.up_proj(hidden_states))
            linear_21: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_50, p_h_3_mlp_gate_proj_weight);  p_h_3_mlp_gate_proj_weight = None
            silu_3: "f32[1, 6, 12288]" = torch.ops.aten.silu.default(linear_21);  linear_21 = None
            linear_22: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_50, p_h_3_mlp_up_proj_weight);  mul_50 = p_h_3_mlp_up_proj_weight = None
            mul_51: "f32[1, 6, 12288]" = torch.ops.aten.mul.Tensor(silu_3, linear_22);  silu_3 = linear_22 = None
            linear_23: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(mul_51, p_h_3_mlp_down_proj_weight, p_h_3_mlp_down_proj_bias);  mul_51 = p_h_3_mlp_down_proj_weight = p_h_3_mlp_down_proj_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_11: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_23, 0.0, False);  linear_23 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_23: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_21, dropout_11);  add_21 = dropout_11 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_37: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_23, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_13: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_37, 2)
            mean_8: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_13, [-1], True);  pow_13 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_24: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_8, 1e-05);  mean_8 = None
            rsqrt_8: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_24);  add_24 = None
            mul_52: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_37, rsqrt_8);  _to_copy_37 = rsqrt_8 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_38: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_52, dtype = torch.float32);  mul_52 = None
            mul_53: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_4_input_layernorm_weight, _to_copy_38);  p_h_4_input_layernorm_weight = _to_copy_38 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:404 in forward, code: hidden_states = hidden_states.transpose(1, 0)
            transpose_20: "f32[6, 1, 5120]" = torch.ops.aten.transpose.int(mul_53, 1, 0);  mul_53 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:405 in forward, code: query_layer = self.query(hidden_states)
            linear_24: "f32[6, 1, 5120]" = torch.ops.aten.linear.default(transpose_20, p_h_4_self_attention_query_weight);  p_h_4_self_attention_query_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:409 in forward, code: query_layer = query_layer.view(*new_tensor_shape)
            view_48: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(linear_24, [6, 1, 32, 160]);  linear_24 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:411 in forward, code: mixed_kv_layer = self.key_value(hidden_states)
            linear_25: "f32[6, 1, 10240]" = torch.ops.aten.linear.default(transpose_20, p_h_4_self_attention_key_value_weight);  transpose_20 = p_h_4_self_attention_key_value_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:415 in forward, code: mixed_kv_layer = mixed_kv_layer.view(*new_tensor_shape)
            view_49: "f32[6, 1, 32, 320]" = torch.ops.aten.view.default(linear_25, [6, 1, 32, 320]);  linear_25 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:381 in split_tensor_along_last_dim, code: tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)
            split_4 = torch.ops.aten.split.Tensor(view_49, 160, 3);  view_49 = None
            getitem_8: "f32[6, 1, 32, 160]" = split_4[0]
            getitem_9: "f32[6, 1, 32, 160]" = split_4[1];  split_4 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:423 in forward, code: query_layer = query_layer.view(output_size[2], output_size[0] * output_size[1], -1)
            view_50: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_48, [6, 32, -1]);  view_48 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:424 in forward, code: key_layer = key_layer.view(output_size[3], output_size[0] * output_size[1], -1)
            view_51: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_8, [6, 32, -1]);  getitem_8 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:112 in forward, code: self.inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, device=x.device).float() / self.dim))
            arange_9: "i64[80]" = torch.ops.aten.arange.start_step(0, 160, 2, device = device(type='cpu'), pin_memory = False)
            _to_copy_39: "f32[80]" = torch.ops.aten._to_copy.default(arange_9, dtype = torch.float32);  arange_9 = None
            div_4: "f32[80]" = torch.ops.aten.div.Tensor(_to_copy_39, 160);  _to_copy_39 = None
            pow_14: "f32[80]" = torch.ops.aten.pow.Scalar(30420.108888514722, div_4);  div_4 = None
            reciprocal_4: "f32[80]" = torch.ops.aten.reciprocal.default(pow_14);  pow_14 = None
            mul_54: "f32[80]" = torch.ops.aten.mul.Tensor(reciprocal_4, 1.0);  reciprocal_4 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:114 in forward, code: t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)
            arange_10: "f32[8192]" = torch.ops.aten.arange.default(8192, dtype = torch.float32, device = device(type='cpu'), pin_memory = False)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:115 in forward, code: freqs = torch.einsum('i,j->ij', t, self.inv_freq)
            einsum_8: "f32[8192, 80]" = torch.ops.aten.einsum.default('i,j->ij', [arange_10, mul_54]);  arange_10 = mul_54 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:117 in forward, code: emb = torch.cat((freqs, freqs), dim=-1).to(x.device)
            cat_12: "f32[8192, 160]" = torch.ops.aten.cat.default([einsum_8, einsum_8], -1);  einsum_8 = None
            _to_copy_40: "f32[8192, 160]" = torch.ops.aten._to_copy.default(cat_12, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  cat_12 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:121 in forward, code: self.cos_cached = self.mscale * emb.cos()[:, None, :].to(dtype)
            cos_4: "f32[8192, 160]" = torch.ops.aten.cos.default(_to_copy_40)
            slice_54: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(cos_4, 0, 0, 9223372036854775807);  cos_4 = None
            unsqueeze_16: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_54, 1);  slice_54 = None
            slice_55: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_16, 2, 0, 9223372036854775807);  unsqueeze_16 = None
            _to_copy_41: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_55, dtype = torch.float32);  slice_55 = None
            mul_55: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_41, 1.0);  _to_copy_41 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:122 in forward, code: self.sin_cached = self.mscale * emb.sin()[:, None, :].to(dtype)
            sin_4: "f32[8192, 160]" = torch.ops.aten.sin.default(_to_copy_40);  _to_copy_40 = None
            slice_56: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(sin_4, 0, 0, 9223372036854775807);  sin_4 = None
            unsqueeze_17: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_56, 1);  slice_56 = None
            slice_57: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_17, 2, 0, 9223372036854775807);  unsqueeze_17 = None
            _to_copy_42: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_57, dtype = torch.float32);  slice_57 = None
            mul_56: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_42, 1.0);  _to_copy_42 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:123 in forward, code: return self.cos_cached[:seq_len, ...], self.sin_cached[:seq_len, ...]
            alias_8: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_55);  mul_55 = None
            alias_9: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_56);  mul_56 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:133 in apply_rotary_pos_emb_torch, code: cos, sin = cos[offset:q.shape[0] + offset, ...], sin[offset:q.shape[0] + offset, ...]
            slice_58: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_8, 0, 0, 6);  alias_8 = None
            slice_59: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_9, 0, 0, 6);  alias_9 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_57: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_50, slice_58)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_60: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_50, 2, 0, 80)
            slice_61: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_50, 2, 80, 9223372036854775807);  view_50 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_8: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_61);  slice_61 = None
            cat_13: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_8, slice_60], 2);  neg_8 = slice_60 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_58: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_13, slice_59);  cat_13 = None
            add_25: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_57, mul_58);  mul_57 = mul_58 = None
            mul_59: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_51, slice_58);  slice_58 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_62: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_51, 2, 0, 80)
            slice_63: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_51, 2, 80, 9223372036854775807);  view_51 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_9: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_63);  slice_63 = None
            cat_14: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_9, slice_62], 2);  neg_9 = slice_62 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_60: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_14, slice_59);  cat_14 = slice_59 = None
            add_26: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_59, mul_60);  mul_59 = mul_60 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:448 in forward, code: query_layer = query_layer.reshape((s_query, bz, head, dim))
            view_52: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_25, [6, 1, 32, 160]);  add_25 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:449 in forward, code: key_layer = key_layer.reshape((s_key, bz, head, dim))
            view_53: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_26, [6, 1, 32, 160]);  add_26 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:458 in forward, code: query_layer = query_layer.reshape(s_query, bz * self.num_heads, dim)
            view_54: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_52, [6, 32, 160]);  view_52 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:460 in forward, code: key_layer = key_layer.reshape(s_key, bz * self.num_heads, dim)
            view_55: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_53, [6, 32, 160]);  view_53 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            transpose_21: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_54, 0, 1);  view_54 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:462 in forward, code: key_layer.transpose(0, 1).transpose(1, 2))
            transpose_22: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_55, 0, 1);  view_55 = None
            transpose_23: "f32[32, 160, 6]" = torch.ops.aten.transpose.int(transpose_22, 1, 2);  transpose_22 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            einsum_9: "f32[32, 6, 6]" = torch.ops.aten.einsum.default('bik,bkj->bij', [transpose_21, transpose_23]);  transpose_21 = transpose_23 = None
            mul_61: "f32[32, 6, 6]" = torch.ops.aten.mul.Tensor(einsum_9, 0.07905694150420949);  einsum_9 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:464 in forward, code: attention_scores = matmul_result.view(bz, self.num_heads, s_query, s_key)
            view_56: "f32[1, 32, 6, 6]" = torch.ops.aten.view.default(mul_61, [1, 32, 6, 6]);  mul_61 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:469 in forward, code: attn_weights = torch.masked_fill(attention_scores, attention_mask, torch.finfo(attention_scores.dtype).min)
            masked_fill_4: "f32[1, 32, 6, 6]" = torch.ops.aten.masked_fill.Scalar(view_56, or_1, -3.4028234663852886e+38);  view_56 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:470 in forward, code: attention_probs = F.softmax(attn_weights, dim=-1).to(input_dtype)  ##dtype = torch.float32
            softmax_4: "f32[1, 32, 6, 6]" = torch.ops.aten.softmax.int(masked_fill_4, -1);  masked_fill_4 = None
            _to_copy_43: "f32[1, 32, 6, 6]" = torch.ops.aten._to_copy.default(softmax_4, dtype = torch.float32);  softmax_4 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:471 in forward, code: attention_probs = self.attention_dropout(attention_probs)
            dropout_12: "f32[1, 32, 6, 6]" = torch.ops.aten.dropout.default(_to_copy_43, 0.0, False);  _to_copy_43 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:472 in forward, code: attention_probs_reshaped = attention_probs.view(bz * self.num_heads, s_query, s_key)
            view_57: "f32[32, 6, 6]" = torch.ops.aten.view.default(dropout_12, [32, 6, 6]);  dropout_12 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:474 in forward, code: value_layer = value_layer.reshape(s_key, bz * self.num_heads, dim)
            view_58: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_9, [6, 32, 160]);  getitem_9 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:475 in forward, code: context_layer = torch.bmm(attention_probs_reshaped, value_layer.transpose(0, 1))
            transpose_24: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_58, 0, 1);  view_58 = None
            bmm_4: "f32[32, 6, 160]" = torch.ops.aten.bmm.default(view_57, transpose_24);  view_57 = transpose_24 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:391 in _merge_heads, code: x = x.view(batch_size, self.num_heads, seq_length, self.head_dim)
            view_59: "f32[1, 32, 6, 160]" = torch.ops.aten.view.default(bmm_4, [1, 32, 6, 160]);  bmm_4 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:392 in _merge_heads, code: x = x.permute(0, 2, 1, 3)
            permute_4: "f32[1, 6, 32, 160]" = torch.ops.aten.permute.default(view_59, [0, 2, 1, 3]);  view_59 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:393 in _merge_heads, code: return x.reshape(batch_size, seq_length, self.num_heads * self.head_dim)
            clone_4: "f32[1, 6, 32, 160]" = torch.ops.aten.clone.default(permute_4, memory_format = torch.contiguous_format);  permute_4 = None
            _unsafe_view_4: "f32[1, 6, 5120]" = torch.ops.aten._unsafe_view.default(clone_4, [1, 6, 5120]);  clone_4 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:478 in forward, code: output_tensor = self.dense(context_layer)
            linear_26: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(_unsafe_view_4, p_h_4_self_attention_dense_weight, p_h_4_self_attention_dense_bias);  _unsafe_view_4 = p_h_4_self_attention_dense_weight = p_h_4_self_attention_dense_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_13: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_26, 0.0, False);  linear_26 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_27: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_23, dropout_13);  add_23 = dropout_13 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_44: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_27, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_15: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_44, 2)
            mean_9: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_15, [-1], True);  pow_15 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_28: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_9, 1e-05);  mean_9 = None
            rsqrt_9: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_28);  add_28 = None
            mul_62: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_44, rsqrt_9);  _to_copy_44 = rsqrt_9 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_45: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_62, dtype = torch.float32);  mul_62 = None
            mul_63: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_4_post_attention_layernorm_weight, _to_copy_45);  p_h_4_post_attention_layernorm_weight = _to_copy_45 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:499 in forward, code: intermediate_output = self.down_proj(F.silu(self.gate_proj(hidden_states)) * self.up_proj(hidden_states))
            linear_27: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_63, p_h_4_mlp_gate_proj_weight);  p_h_4_mlp_gate_proj_weight = None
            silu_4: "f32[1, 6, 12288]" = torch.ops.aten.silu.default(linear_27);  linear_27 = None
            linear_28: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_63, p_h_4_mlp_up_proj_weight);  mul_63 = p_h_4_mlp_up_proj_weight = None
            mul_64: "f32[1, 6, 12288]" = torch.ops.aten.mul.Tensor(silu_4, linear_28);  silu_4 = linear_28 = None
            linear_29: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(mul_64, p_h_4_mlp_down_proj_weight, p_h_4_mlp_down_proj_bias);  mul_64 = p_h_4_mlp_down_proj_weight = p_h_4_mlp_down_proj_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_14: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_29, 0.0, False);  linear_29 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_29: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_27, dropout_14);  add_27 = dropout_14 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_46: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_29, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_16: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_46, 2)
            mean_10: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_16, [-1], True);  pow_16 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_30: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_10, 1e-05);  mean_10 = None
            rsqrt_10: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_30);  add_30 = None
            mul_65: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_46, rsqrt_10);  _to_copy_46 = rsqrt_10 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_47: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_65, dtype = torch.float32);  mul_65 = None
            mul_66: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_5_input_layernorm_weight, _to_copy_47);  p_h_5_input_layernorm_weight = _to_copy_47 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:404 in forward, code: hidden_states = hidden_states.transpose(1, 0)
            transpose_25: "f32[6, 1, 5120]" = torch.ops.aten.transpose.int(mul_66, 1, 0);  mul_66 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:405 in forward, code: query_layer = self.query(hidden_states)
            linear_30: "f32[6, 1, 5120]" = torch.ops.aten.linear.default(transpose_25, p_h_5_self_attention_query_weight);  p_h_5_self_attention_query_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:409 in forward, code: query_layer = query_layer.view(*new_tensor_shape)
            view_60: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(linear_30, [6, 1, 32, 160]);  linear_30 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:411 in forward, code: mixed_kv_layer = self.key_value(hidden_states)
            linear_31: "f32[6, 1, 10240]" = torch.ops.aten.linear.default(transpose_25, p_h_5_self_attention_key_value_weight);  transpose_25 = p_h_5_self_attention_key_value_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:415 in forward, code: mixed_kv_layer = mixed_kv_layer.view(*new_tensor_shape)
            view_61: "f32[6, 1, 32, 320]" = torch.ops.aten.view.default(linear_31, [6, 1, 32, 320]);  linear_31 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:381 in split_tensor_along_last_dim, code: tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)
            split_5 = torch.ops.aten.split.Tensor(view_61, 160, 3);  view_61 = None
            getitem_10: "f32[6, 1, 32, 160]" = split_5[0]
            getitem_11: "f32[6, 1, 32, 160]" = split_5[1];  split_5 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:423 in forward, code: query_layer = query_layer.view(output_size[2], output_size[0] * output_size[1], -1)
            view_62: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_60, [6, 32, -1]);  view_60 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:424 in forward, code: key_layer = key_layer.view(output_size[3], output_size[0] * output_size[1], -1)
            view_63: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_10, [6, 32, -1]);  getitem_10 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:112 in forward, code: self.inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, device=x.device).float() / self.dim))
            arange_11: "i64[80]" = torch.ops.aten.arange.start_step(0, 160, 2, device = device(type='cpu'), pin_memory = False)
            _to_copy_48: "f32[80]" = torch.ops.aten._to_copy.default(arange_11, dtype = torch.float32);  arange_11 = None
            div_5: "f32[80]" = torch.ops.aten.div.Tensor(_to_copy_48, 160);  _to_copy_48 = None
            pow_17: "f32[80]" = torch.ops.aten.pow.Scalar(30420.108888514722, div_5);  div_5 = None
            reciprocal_5: "f32[80]" = torch.ops.aten.reciprocal.default(pow_17);  pow_17 = None
            mul_67: "f32[80]" = torch.ops.aten.mul.Tensor(reciprocal_5, 1.0);  reciprocal_5 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:114 in forward, code: t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)
            arange_12: "f32[8192]" = torch.ops.aten.arange.default(8192, dtype = torch.float32, device = device(type='cpu'), pin_memory = False)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:115 in forward, code: freqs = torch.einsum('i,j->ij', t, self.inv_freq)
            einsum_10: "f32[8192, 80]" = torch.ops.aten.einsum.default('i,j->ij', [arange_12, mul_67]);  arange_12 = mul_67 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:117 in forward, code: emb = torch.cat((freqs, freqs), dim=-1).to(x.device)
            cat_15: "f32[8192, 160]" = torch.ops.aten.cat.default([einsum_10, einsum_10], -1);  einsum_10 = None
            _to_copy_49: "f32[8192, 160]" = torch.ops.aten._to_copy.default(cat_15, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  cat_15 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:121 in forward, code: self.cos_cached = self.mscale * emb.cos()[:, None, :].to(dtype)
            cos_5: "f32[8192, 160]" = torch.ops.aten.cos.default(_to_copy_49)
            slice_64: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(cos_5, 0, 0, 9223372036854775807);  cos_5 = None
            unsqueeze_18: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_64, 1);  slice_64 = None
            slice_65: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_18, 2, 0, 9223372036854775807);  unsqueeze_18 = None
            _to_copy_50: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_65, dtype = torch.float32);  slice_65 = None
            mul_68: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_50, 1.0);  _to_copy_50 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:122 in forward, code: self.sin_cached = self.mscale * emb.sin()[:, None, :].to(dtype)
            sin_5: "f32[8192, 160]" = torch.ops.aten.sin.default(_to_copy_49);  _to_copy_49 = None
            slice_66: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(sin_5, 0, 0, 9223372036854775807);  sin_5 = None
            unsqueeze_19: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_66, 1);  slice_66 = None
            slice_67: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_19, 2, 0, 9223372036854775807);  unsqueeze_19 = None
            _to_copy_51: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_67, dtype = torch.float32);  slice_67 = None
            mul_69: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_51, 1.0);  _to_copy_51 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:123 in forward, code: return self.cos_cached[:seq_len, ...], self.sin_cached[:seq_len, ...]
            alias_10: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_68);  mul_68 = None
            alias_11: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_69);  mul_69 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:133 in apply_rotary_pos_emb_torch, code: cos, sin = cos[offset:q.shape[0] + offset, ...], sin[offset:q.shape[0] + offset, ...]
            slice_68: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_10, 0, 0, 6);  alias_10 = None
            slice_69: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_11, 0, 0, 6);  alias_11 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_70: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_62, slice_68)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_70: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_62, 2, 0, 80)
            slice_71: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_62, 2, 80, 9223372036854775807);  view_62 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_10: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_71);  slice_71 = None
            cat_16: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_10, slice_70], 2);  neg_10 = slice_70 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_71: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_16, slice_69);  cat_16 = None
            add_31: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_70, mul_71);  mul_70 = mul_71 = None
            mul_72: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_63, slice_68);  slice_68 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_72: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_63, 2, 0, 80)
            slice_73: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_63, 2, 80, 9223372036854775807);  view_63 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_11: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_73);  slice_73 = None
            cat_17: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_11, slice_72], 2);  neg_11 = slice_72 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_73: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_17, slice_69);  cat_17 = slice_69 = None
            add_32: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_72, mul_73);  mul_72 = mul_73 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:448 in forward, code: query_layer = query_layer.reshape((s_query, bz, head, dim))
            view_64: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_31, [6, 1, 32, 160]);  add_31 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:449 in forward, code: key_layer = key_layer.reshape((s_key, bz, head, dim))
            view_65: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_32, [6, 1, 32, 160]);  add_32 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:458 in forward, code: query_layer = query_layer.reshape(s_query, bz * self.num_heads, dim)
            view_66: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_64, [6, 32, 160]);  view_64 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:460 in forward, code: key_layer = key_layer.reshape(s_key, bz * self.num_heads, dim)
            view_67: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_65, [6, 32, 160]);  view_65 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            transpose_26: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_66, 0, 1);  view_66 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:462 in forward, code: key_layer.transpose(0, 1).transpose(1, 2))
            transpose_27: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_67, 0, 1);  view_67 = None
            transpose_28: "f32[32, 160, 6]" = torch.ops.aten.transpose.int(transpose_27, 1, 2);  transpose_27 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            einsum_11: "f32[32, 6, 6]" = torch.ops.aten.einsum.default('bik,bkj->bij', [transpose_26, transpose_28]);  transpose_26 = transpose_28 = None
            mul_74: "f32[32, 6, 6]" = torch.ops.aten.mul.Tensor(einsum_11, 0.07905694150420949);  einsum_11 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:464 in forward, code: attention_scores = matmul_result.view(bz, self.num_heads, s_query, s_key)
            view_68: "f32[1, 32, 6, 6]" = torch.ops.aten.view.default(mul_74, [1, 32, 6, 6]);  mul_74 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:469 in forward, code: attn_weights = torch.masked_fill(attention_scores, attention_mask, torch.finfo(attention_scores.dtype).min)
            masked_fill_5: "f32[1, 32, 6, 6]" = torch.ops.aten.masked_fill.Scalar(view_68, or_1, -3.4028234663852886e+38);  view_68 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:470 in forward, code: attention_probs = F.softmax(attn_weights, dim=-1).to(input_dtype)  ##dtype = torch.float32
            softmax_5: "f32[1, 32, 6, 6]" = torch.ops.aten.softmax.int(masked_fill_5, -1);  masked_fill_5 = None
            _to_copy_52: "f32[1, 32, 6, 6]" = torch.ops.aten._to_copy.default(softmax_5, dtype = torch.float32);  softmax_5 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:471 in forward, code: attention_probs = self.attention_dropout(attention_probs)
            dropout_15: "f32[1, 32, 6, 6]" = torch.ops.aten.dropout.default(_to_copy_52, 0.0, False);  _to_copy_52 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:472 in forward, code: attention_probs_reshaped = attention_probs.view(bz * self.num_heads, s_query, s_key)
            view_69: "f32[32, 6, 6]" = torch.ops.aten.view.default(dropout_15, [32, 6, 6]);  dropout_15 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:474 in forward, code: value_layer = value_layer.reshape(s_key, bz * self.num_heads, dim)
            view_70: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_11, [6, 32, 160]);  getitem_11 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:475 in forward, code: context_layer = torch.bmm(attention_probs_reshaped, value_layer.transpose(0, 1))
            transpose_29: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_70, 0, 1);  view_70 = None
            bmm_5: "f32[32, 6, 160]" = torch.ops.aten.bmm.default(view_69, transpose_29);  view_69 = transpose_29 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:391 in _merge_heads, code: x = x.view(batch_size, self.num_heads, seq_length, self.head_dim)
            view_71: "f32[1, 32, 6, 160]" = torch.ops.aten.view.default(bmm_5, [1, 32, 6, 160]);  bmm_5 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:392 in _merge_heads, code: x = x.permute(0, 2, 1, 3)
            permute_5: "f32[1, 6, 32, 160]" = torch.ops.aten.permute.default(view_71, [0, 2, 1, 3]);  view_71 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:393 in _merge_heads, code: return x.reshape(batch_size, seq_length, self.num_heads * self.head_dim)
            clone_5: "f32[1, 6, 32, 160]" = torch.ops.aten.clone.default(permute_5, memory_format = torch.contiguous_format);  permute_5 = None
            _unsafe_view_5: "f32[1, 6, 5120]" = torch.ops.aten._unsafe_view.default(clone_5, [1, 6, 5120]);  clone_5 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:478 in forward, code: output_tensor = self.dense(context_layer)
            linear_32: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(_unsafe_view_5, p_h_5_self_attention_dense_weight, p_h_5_self_attention_dense_bias);  _unsafe_view_5 = p_h_5_self_attention_dense_weight = p_h_5_self_attention_dense_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_16: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_32, 0.0, False);  linear_32 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_33: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_29, dropout_16);  add_29 = dropout_16 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_53: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_33, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_18: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_53, 2)
            mean_11: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_18, [-1], True);  pow_18 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_34: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_11, 1e-05);  mean_11 = None
            rsqrt_11: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_34);  add_34 = None
            mul_75: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_53, rsqrt_11);  _to_copy_53 = rsqrt_11 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_54: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_75, dtype = torch.float32);  mul_75 = None
            mul_76: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_5_post_attention_layernorm_weight, _to_copy_54);  p_h_5_post_attention_layernorm_weight = _to_copy_54 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:499 in forward, code: intermediate_output = self.down_proj(F.silu(self.gate_proj(hidden_states)) * self.up_proj(hidden_states))
            linear_33: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_76, p_h_5_mlp_gate_proj_weight);  p_h_5_mlp_gate_proj_weight = None
            silu_5: "f32[1, 6, 12288]" = torch.ops.aten.silu.default(linear_33);  linear_33 = None
            linear_34: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_76, p_h_5_mlp_up_proj_weight);  mul_76 = p_h_5_mlp_up_proj_weight = None
            mul_77: "f32[1, 6, 12288]" = torch.ops.aten.mul.Tensor(silu_5, linear_34);  silu_5 = linear_34 = None
            linear_35: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(mul_77, p_h_5_mlp_down_proj_weight, p_h_5_mlp_down_proj_bias);  mul_77 = p_h_5_mlp_down_proj_weight = p_h_5_mlp_down_proj_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_17: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_35, 0.0, False);  linear_35 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_35: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_33, dropout_17);  add_33 = dropout_17 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_55: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_35, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_19: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_55, 2)
            mean_12: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_19, [-1], True);  pow_19 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_36: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_12, 1e-05);  mean_12 = None
            rsqrt_12: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_36);  add_36 = None
            mul_78: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_55, rsqrt_12);  _to_copy_55 = rsqrt_12 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_56: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_78, dtype = torch.float32);  mul_78 = None
            mul_79: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_6_input_layernorm_weight, _to_copy_56);  p_h_6_input_layernorm_weight = _to_copy_56 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:404 in forward, code: hidden_states = hidden_states.transpose(1, 0)
            transpose_30: "f32[6, 1, 5120]" = torch.ops.aten.transpose.int(mul_79, 1, 0);  mul_79 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:405 in forward, code: query_layer = self.query(hidden_states)
            linear_36: "f32[6, 1, 5120]" = torch.ops.aten.linear.default(transpose_30, p_h_6_self_attention_query_weight);  p_h_6_self_attention_query_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:409 in forward, code: query_layer = query_layer.view(*new_tensor_shape)
            view_72: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(linear_36, [6, 1, 32, 160]);  linear_36 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:411 in forward, code: mixed_kv_layer = self.key_value(hidden_states)
            linear_37: "f32[6, 1, 10240]" = torch.ops.aten.linear.default(transpose_30, p_h_6_self_attention_key_value_weight);  transpose_30 = p_h_6_self_attention_key_value_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:415 in forward, code: mixed_kv_layer = mixed_kv_layer.view(*new_tensor_shape)
            view_73: "f32[6, 1, 32, 320]" = torch.ops.aten.view.default(linear_37, [6, 1, 32, 320]);  linear_37 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:381 in split_tensor_along_last_dim, code: tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)
            split_6 = torch.ops.aten.split.Tensor(view_73, 160, 3);  view_73 = None
            getitem_12: "f32[6, 1, 32, 160]" = split_6[0]
            getitem_13: "f32[6, 1, 32, 160]" = split_6[1];  split_6 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:423 in forward, code: query_layer = query_layer.view(output_size[2], output_size[0] * output_size[1], -1)
            view_74: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_72, [6, 32, -1]);  view_72 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:424 in forward, code: key_layer = key_layer.view(output_size[3], output_size[0] * output_size[1], -1)
            view_75: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_12, [6, 32, -1]);  getitem_12 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:112 in forward, code: self.inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, device=x.device).float() / self.dim))
            arange_13: "i64[80]" = torch.ops.aten.arange.start_step(0, 160, 2, device = device(type='cpu'), pin_memory = False)
            _to_copy_57: "f32[80]" = torch.ops.aten._to_copy.default(arange_13, dtype = torch.float32);  arange_13 = None
            div_6: "f32[80]" = torch.ops.aten.div.Tensor(_to_copy_57, 160);  _to_copy_57 = None
            pow_20: "f32[80]" = torch.ops.aten.pow.Scalar(30420.108888514722, div_6);  div_6 = None
            reciprocal_6: "f32[80]" = torch.ops.aten.reciprocal.default(pow_20);  pow_20 = None
            mul_80: "f32[80]" = torch.ops.aten.mul.Tensor(reciprocal_6, 1.0);  reciprocal_6 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:114 in forward, code: t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)
            arange_14: "f32[8192]" = torch.ops.aten.arange.default(8192, dtype = torch.float32, device = device(type='cpu'), pin_memory = False)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:115 in forward, code: freqs = torch.einsum('i,j->ij', t, self.inv_freq)
            einsum_12: "f32[8192, 80]" = torch.ops.aten.einsum.default('i,j->ij', [arange_14, mul_80]);  arange_14 = mul_80 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:117 in forward, code: emb = torch.cat((freqs, freqs), dim=-1).to(x.device)
            cat_18: "f32[8192, 160]" = torch.ops.aten.cat.default([einsum_12, einsum_12], -1);  einsum_12 = None
            _to_copy_58: "f32[8192, 160]" = torch.ops.aten._to_copy.default(cat_18, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  cat_18 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:121 in forward, code: self.cos_cached = self.mscale * emb.cos()[:, None, :].to(dtype)
            cos_6: "f32[8192, 160]" = torch.ops.aten.cos.default(_to_copy_58)
            slice_74: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(cos_6, 0, 0, 9223372036854775807);  cos_6 = None
            unsqueeze_20: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_74, 1);  slice_74 = None
            slice_75: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_20, 2, 0, 9223372036854775807);  unsqueeze_20 = None
            _to_copy_59: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_75, dtype = torch.float32);  slice_75 = None
            mul_81: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_59, 1.0);  _to_copy_59 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:122 in forward, code: self.sin_cached = self.mscale * emb.sin()[:, None, :].to(dtype)
            sin_6: "f32[8192, 160]" = torch.ops.aten.sin.default(_to_copy_58);  _to_copy_58 = None
            slice_76: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(sin_6, 0, 0, 9223372036854775807);  sin_6 = None
            unsqueeze_21: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_76, 1);  slice_76 = None
            slice_77: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_21, 2, 0, 9223372036854775807);  unsqueeze_21 = None
            _to_copy_60: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_77, dtype = torch.float32);  slice_77 = None
            mul_82: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_60, 1.0);  _to_copy_60 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:123 in forward, code: return self.cos_cached[:seq_len, ...], self.sin_cached[:seq_len, ...]
            alias_12: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_81);  mul_81 = None
            alias_13: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_82);  mul_82 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:133 in apply_rotary_pos_emb_torch, code: cos, sin = cos[offset:q.shape[0] + offset, ...], sin[offset:q.shape[0] + offset, ...]
            slice_78: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_12, 0, 0, 6);  alias_12 = None
            slice_79: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_13, 0, 0, 6);  alias_13 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_83: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_74, slice_78)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_80: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_74, 2, 0, 80)
            slice_81: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_74, 2, 80, 9223372036854775807);  view_74 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_12: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_81);  slice_81 = None
            cat_19: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_12, slice_80], 2);  neg_12 = slice_80 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_84: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_19, slice_79);  cat_19 = None
            add_37: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_83, mul_84);  mul_83 = mul_84 = None
            mul_85: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_75, slice_78);  slice_78 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_82: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_75, 2, 0, 80)
            slice_83: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_75, 2, 80, 9223372036854775807);  view_75 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_13: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_83);  slice_83 = None
            cat_20: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_13, slice_82], 2);  neg_13 = slice_82 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_86: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_20, slice_79);  cat_20 = slice_79 = None
            add_38: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_85, mul_86);  mul_85 = mul_86 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:448 in forward, code: query_layer = query_layer.reshape((s_query, bz, head, dim))
            view_76: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_37, [6, 1, 32, 160]);  add_37 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:449 in forward, code: key_layer = key_layer.reshape((s_key, bz, head, dim))
            view_77: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_38, [6, 1, 32, 160]);  add_38 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:458 in forward, code: query_layer = query_layer.reshape(s_query, bz * self.num_heads, dim)
            view_78: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_76, [6, 32, 160]);  view_76 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:460 in forward, code: key_layer = key_layer.reshape(s_key, bz * self.num_heads, dim)
            view_79: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_77, [6, 32, 160]);  view_77 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            transpose_31: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_78, 0, 1);  view_78 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:462 in forward, code: key_layer.transpose(0, 1).transpose(1, 2))
            transpose_32: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_79, 0, 1);  view_79 = None
            transpose_33: "f32[32, 160, 6]" = torch.ops.aten.transpose.int(transpose_32, 1, 2);  transpose_32 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            einsum_13: "f32[32, 6, 6]" = torch.ops.aten.einsum.default('bik,bkj->bij', [transpose_31, transpose_33]);  transpose_31 = transpose_33 = None
            mul_87: "f32[32, 6, 6]" = torch.ops.aten.mul.Tensor(einsum_13, 0.07905694150420949);  einsum_13 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:464 in forward, code: attention_scores = matmul_result.view(bz, self.num_heads, s_query, s_key)
            view_80: "f32[1, 32, 6, 6]" = torch.ops.aten.view.default(mul_87, [1, 32, 6, 6]);  mul_87 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:469 in forward, code: attn_weights = torch.masked_fill(attention_scores, attention_mask, torch.finfo(attention_scores.dtype).min)
            masked_fill_6: "f32[1, 32, 6, 6]" = torch.ops.aten.masked_fill.Scalar(view_80, or_1, -3.4028234663852886e+38);  view_80 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:470 in forward, code: attention_probs = F.softmax(attn_weights, dim=-1).to(input_dtype)  ##dtype = torch.float32
            softmax_6: "f32[1, 32, 6, 6]" = torch.ops.aten.softmax.int(masked_fill_6, -1);  masked_fill_6 = None
            _to_copy_61: "f32[1, 32, 6, 6]" = torch.ops.aten._to_copy.default(softmax_6, dtype = torch.float32);  softmax_6 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:471 in forward, code: attention_probs = self.attention_dropout(attention_probs)
            dropout_18: "f32[1, 32, 6, 6]" = torch.ops.aten.dropout.default(_to_copy_61, 0.0, False);  _to_copy_61 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:472 in forward, code: attention_probs_reshaped = attention_probs.view(bz * self.num_heads, s_query, s_key)
            view_81: "f32[32, 6, 6]" = torch.ops.aten.view.default(dropout_18, [32, 6, 6]);  dropout_18 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:474 in forward, code: value_layer = value_layer.reshape(s_key, bz * self.num_heads, dim)
            view_82: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_13, [6, 32, 160]);  getitem_13 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:475 in forward, code: context_layer = torch.bmm(attention_probs_reshaped, value_layer.transpose(0, 1))
            transpose_34: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_82, 0, 1);  view_82 = None
            bmm_6: "f32[32, 6, 160]" = torch.ops.aten.bmm.default(view_81, transpose_34);  view_81 = transpose_34 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:391 in _merge_heads, code: x = x.view(batch_size, self.num_heads, seq_length, self.head_dim)
            view_83: "f32[1, 32, 6, 160]" = torch.ops.aten.view.default(bmm_6, [1, 32, 6, 160]);  bmm_6 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:392 in _merge_heads, code: x = x.permute(0, 2, 1, 3)
            permute_6: "f32[1, 6, 32, 160]" = torch.ops.aten.permute.default(view_83, [0, 2, 1, 3]);  view_83 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:393 in _merge_heads, code: return x.reshape(batch_size, seq_length, self.num_heads * self.head_dim)
            clone_6: "f32[1, 6, 32, 160]" = torch.ops.aten.clone.default(permute_6, memory_format = torch.contiguous_format);  permute_6 = None
            _unsafe_view_6: "f32[1, 6, 5120]" = torch.ops.aten._unsafe_view.default(clone_6, [1, 6, 5120]);  clone_6 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:478 in forward, code: output_tensor = self.dense(context_layer)
            linear_38: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(_unsafe_view_6, p_h_6_self_attention_dense_weight, p_h_6_self_attention_dense_bias);  _unsafe_view_6 = p_h_6_self_attention_dense_weight = p_h_6_self_attention_dense_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_19: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_38, 0.0, False);  linear_38 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_39: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_35, dropout_19);  add_35 = dropout_19 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_62: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_39, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_21: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_62, 2)
            mean_13: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_21, [-1], True);  pow_21 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_40: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_13, 1e-05);  mean_13 = None
            rsqrt_13: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_40);  add_40 = None
            mul_88: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_62, rsqrt_13);  _to_copy_62 = rsqrt_13 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_63: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_88, dtype = torch.float32);  mul_88 = None
            mul_89: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_6_post_attention_layernorm_weight, _to_copy_63);  p_h_6_post_attention_layernorm_weight = _to_copy_63 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:499 in forward, code: intermediate_output = self.down_proj(F.silu(self.gate_proj(hidden_states)) * self.up_proj(hidden_states))
            linear_39: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_89, p_h_6_mlp_gate_proj_weight);  p_h_6_mlp_gate_proj_weight = None
            silu_6: "f32[1, 6, 12288]" = torch.ops.aten.silu.default(linear_39);  linear_39 = None
            linear_40: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_89, p_h_6_mlp_up_proj_weight);  mul_89 = p_h_6_mlp_up_proj_weight = None
            mul_90: "f32[1, 6, 12288]" = torch.ops.aten.mul.Tensor(silu_6, linear_40);  silu_6 = linear_40 = None
            linear_41: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(mul_90, p_h_6_mlp_down_proj_weight, p_h_6_mlp_down_proj_bias);  mul_90 = p_h_6_mlp_down_proj_weight = p_h_6_mlp_down_proj_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_20: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_41, 0.0, False);  linear_41 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_41: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_39, dropout_20);  add_39 = dropout_20 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_64: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_41, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_22: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_64, 2)
            mean_14: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_22, [-1], True);  pow_22 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_42: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_14, 1e-05);  mean_14 = None
            rsqrt_14: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_42);  add_42 = None
            mul_91: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_64, rsqrt_14);  _to_copy_64 = rsqrt_14 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_65: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_91, dtype = torch.float32);  mul_91 = None
            mul_92: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_7_input_layernorm_weight, _to_copy_65);  p_h_7_input_layernorm_weight = _to_copy_65 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:404 in forward, code: hidden_states = hidden_states.transpose(1, 0)
            transpose_35: "f32[6, 1, 5120]" = torch.ops.aten.transpose.int(mul_92, 1, 0);  mul_92 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:405 in forward, code: query_layer = self.query(hidden_states)
            linear_42: "f32[6, 1, 5120]" = torch.ops.aten.linear.default(transpose_35, p_h_7_self_attention_query_weight);  p_h_7_self_attention_query_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:409 in forward, code: query_layer = query_layer.view(*new_tensor_shape)
            view_84: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(linear_42, [6, 1, 32, 160]);  linear_42 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:411 in forward, code: mixed_kv_layer = self.key_value(hidden_states)
            linear_43: "f32[6, 1, 10240]" = torch.ops.aten.linear.default(transpose_35, p_h_7_self_attention_key_value_weight);  transpose_35 = p_h_7_self_attention_key_value_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:415 in forward, code: mixed_kv_layer = mixed_kv_layer.view(*new_tensor_shape)
            view_85: "f32[6, 1, 32, 320]" = torch.ops.aten.view.default(linear_43, [6, 1, 32, 320]);  linear_43 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:381 in split_tensor_along_last_dim, code: tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)
            split_7 = torch.ops.aten.split.Tensor(view_85, 160, 3);  view_85 = None
            getitem_14: "f32[6, 1, 32, 160]" = split_7[0]
            getitem_15: "f32[6, 1, 32, 160]" = split_7[1];  split_7 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:423 in forward, code: query_layer = query_layer.view(output_size[2], output_size[0] * output_size[1], -1)
            view_86: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_84, [6, 32, -1]);  view_84 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:424 in forward, code: key_layer = key_layer.view(output_size[3], output_size[0] * output_size[1], -1)
            view_87: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_14, [6, 32, -1]);  getitem_14 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:112 in forward, code: self.inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, device=x.device).float() / self.dim))
            arange_15: "i64[80]" = torch.ops.aten.arange.start_step(0, 160, 2, device = device(type='cpu'), pin_memory = False)
            _to_copy_66: "f32[80]" = torch.ops.aten._to_copy.default(arange_15, dtype = torch.float32);  arange_15 = None
            div_7: "f32[80]" = torch.ops.aten.div.Tensor(_to_copy_66, 160);  _to_copy_66 = None
            pow_23: "f32[80]" = torch.ops.aten.pow.Scalar(30420.108888514722, div_7);  div_7 = None
            reciprocal_7: "f32[80]" = torch.ops.aten.reciprocal.default(pow_23);  pow_23 = None
            mul_93: "f32[80]" = torch.ops.aten.mul.Tensor(reciprocal_7, 1.0);  reciprocal_7 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:114 in forward, code: t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)
            arange_16: "f32[8192]" = torch.ops.aten.arange.default(8192, dtype = torch.float32, device = device(type='cpu'), pin_memory = False)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:115 in forward, code: freqs = torch.einsum('i,j->ij', t, self.inv_freq)
            einsum_14: "f32[8192, 80]" = torch.ops.aten.einsum.default('i,j->ij', [arange_16, mul_93]);  arange_16 = mul_93 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:117 in forward, code: emb = torch.cat((freqs, freqs), dim=-1).to(x.device)
            cat_21: "f32[8192, 160]" = torch.ops.aten.cat.default([einsum_14, einsum_14], -1);  einsum_14 = None
            _to_copy_67: "f32[8192, 160]" = torch.ops.aten._to_copy.default(cat_21, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  cat_21 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:121 in forward, code: self.cos_cached = self.mscale * emb.cos()[:, None, :].to(dtype)
            cos_7: "f32[8192, 160]" = torch.ops.aten.cos.default(_to_copy_67)
            slice_84: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(cos_7, 0, 0, 9223372036854775807);  cos_7 = None
            unsqueeze_22: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_84, 1);  slice_84 = None
            slice_85: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_22, 2, 0, 9223372036854775807);  unsqueeze_22 = None
            _to_copy_68: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_85, dtype = torch.float32);  slice_85 = None
            mul_94: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_68, 1.0);  _to_copy_68 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:122 in forward, code: self.sin_cached = self.mscale * emb.sin()[:, None, :].to(dtype)
            sin_7: "f32[8192, 160]" = torch.ops.aten.sin.default(_to_copy_67);  _to_copy_67 = None
            slice_86: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(sin_7, 0, 0, 9223372036854775807);  sin_7 = None
            unsqueeze_23: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_86, 1);  slice_86 = None
            slice_87: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_23, 2, 0, 9223372036854775807);  unsqueeze_23 = None
            _to_copy_69: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_87, dtype = torch.float32);  slice_87 = None
            mul_95: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_69, 1.0);  _to_copy_69 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:123 in forward, code: return self.cos_cached[:seq_len, ...], self.sin_cached[:seq_len, ...]
            alias_14: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_94);  mul_94 = None
            alias_15: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_95);  mul_95 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:133 in apply_rotary_pos_emb_torch, code: cos, sin = cos[offset:q.shape[0] + offset, ...], sin[offset:q.shape[0] + offset, ...]
            slice_88: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_14, 0, 0, 6);  alias_14 = None
            slice_89: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_15, 0, 0, 6);  alias_15 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_96: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_86, slice_88)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_90: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_86, 2, 0, 80)
            slice_91: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_86, 2, 80, 9223372036854775807);  view_86 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_14: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_91);  slice_91 = None
            cat_22: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_14, slice_90], 2);  neg_14 = slice_90 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_97: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_22, slice_89);  cat_22 = None
            add_43: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_96, mul_97);  mul_96 = mul_97 = None
            mul_98: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_87, slice_88);  slice_88 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_92: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_87, 2, 0, 80)
            slice_93: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_87, 2, 80, 9223372036854775807);  view_87 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_15: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_93);  slice_93 = None
            cat_23: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_15, slice_92], 2);  neg_15 = slice_92 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_99: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_23, slice_89);  cat_23 = slice_89 = None
            add_44: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_98, mul_99);  mul_98 = mul_99 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:448 in forward, code: query_layer = query_layer.reshape((s_query, bz, head, dim))
            view_88: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_43, [6, 1, 32, 160]);  add_43 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:449 in forward, code: key_layer = key_layer.reshape((s_key, bz, head, dim))
            view_89: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_44, [6, 1, 32, 160]);  add_44 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:458 in forward, code: query_layer = query_layer.reshape(s_query, bz * self.num_heads, dim)
            view_90: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_88, [6, 32, 160]);  view_88 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:460 in forward, code: key_layer = key_layer.reshape(s_key, bz * self.num_heads, dim)
            view_91: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_89, [6, 32, 160]);  view_89 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            transpose_36: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_90, 0, 1);  view_90 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:462 in forward, code: key_layer.transpose(0, 1).transpose(1, 2))
            transpose_37: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_91, 0, 1);  view_91 = None
            transpose_38: "f32[32, 160, 6]" = torch.ops.aten.transpose.int(transpose_37, 1, 2);  transpose_37 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            einsum_15: "f32[32, 6, 6]" = torch.ops.aten.einsum.default('bik,bkj->bij', [transpose_36, transpose_38]);  transpose_36 = transpose_38 = None
            mul_100: "f32[32, 6, 6]" = torch.ops.aten.mul.Tensor(einsum_15, 0.07905694150420949);  einsum_15 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:464 in forward, code: attention_scores = matmul_result.view(bz, self.num_heads, s_query, s_key)
            view_92: "f32[1, 32, 6, 6]" = torch.ops.aten.view.default(mul_100, [1, 32, 6, 6]);  mul_100 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:469 in forward, code: attn_weights = torch.masked_fill(attention_scores, attention_mask, torch.finfo(attention_scores.dtype).min)
            masked_fill_7: "f32[1, 32, 6, 6]" = torch.ops.aten.masked_fill.Scalar(view_92, or_1, -3.4028234663852886e+38);  view_92 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:470 in forward, code: attention_probs = F.softmax(attn_weights, dim=-1).to(input_dtype)  ##dtype = torch.float32
            softmax_7: "f32[1, 32, 6, 6]" = torch.ops.aten.softmax.int(masked_fill_7, -1);  masked_fill_7 = None
            _to_copy_70: "f32[1, 32, 6, 6]" = torch.ops.aten._to_copy.default(softmax_7, dtype = torch.float32);  softmax_7 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:471 in forward, code: attention_probs = self.attention_dropout(attention_probs)
            dropout_21: "f32[1, 32, 6, 6]" = torch.ops.aten.dropout.default(_to_copy_70, 0.0, False);  _to_copy_70 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:472 in forward, code: attention_probs_reshaped = attention_probs.view(bz * self.num_heads, s_query, s_key)
            view_93: "f32[32, 6, 6]" = torch.ops.aten.view.default(dropout_21, [32, 6, 6]);  dropout_21 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:474 in forward, code: value_layer = value_layer.reshape(s_key, bz * self.num_heads, dim)
            view_94: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_15, [6, 32, 160]);  getitem_15 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:475 in forward, code: context_layer = torch.bmm(attention_probs_reshaped, value_layer.transpose(0, 1))
            transpose_39: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_94, 0, 1);  view_94 = None
            bmm_7: "f32[32, 6, 160]" = torch.ops.aten.bmm.default(view_93, transpose_39);  view_93 = transpose_39 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:391 in _merge_heads, code: x = x.view(batch_size, self.num_heads, seq_length, self.head_dim)
            view_95: "f32[1, 32, 6, 160]" = torch.ops.aten.view.default(bmm_7, [1, 32, 6, 160]);  bmm_7 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:392 in _merge_heads, code: x = x.permute(0, 2, 1, 3)
            permute_7: "f32[1, 6, 32, 160]" = torch.ops.aten.permute.default(view_95, [0, 2, 1, 3]);  view_95 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:393 in _merge_heads, code: return x.reshape(batch_size, seq_length, self.num_heads * self.head_dim)
            clone_7: "f32[1, 6, 32, 160]" = torch.ops.aten.clone.default(permute_7, memory_format = torch.contiguous_format);  permute_7 = None
            _unsafe_view_7: "f32[1, 6, 5120]" = torch.ops.aten._unsafe_view.default(clone_7, [1, 6, 5120]);  clone_7 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:478 in forward, code: output_tensor = self.dense(context_layer)
            linear_44: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(_unsafe_view_7, p_h_7_self_attention_dense_weight, p_h_7_self_attention_dense_bias);  _unsafe_view_7 = p_h_7_self_attention_dense_weight = p_h_7_self_attention_dense_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_22: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_44, 0.0, False);  linear_44 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_45: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_41, dropout_22);  add_41 = dropout_22 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_71: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_45, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_24: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_71, 2)
            mean_15: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_24, [-1], True);  pow_24 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_46: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_15, 1e-05);  mean_15 = None
            rsqrt_15: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_46);  add_46 = None
            mul_101: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_71, rsqrt_15);  _to_copy_71 = rsqrt_15 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_72: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_101, dtype = torch.float32);  mul_101 = None
            mul_102: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_7_post_attention_layernorm_weight, _to_copy_72);  p_h_7_post_attention_layernorm_weight = _to_copy_72 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:499 in forward, code: intermediate_output = self.down_proj(F.silu(self.gate_proj(hidden_states)) * self.up_proj(hidden_states))
            linear_45: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_102, p_h_7_mlp_gate_proj_weight);  p_h_7_mlp_gate_proj_weight = None
            silu_7: "f32[1, 6, 12288]" = torch.ops.aten.silu.default(linear_45);  linear_45 = None
            linear_46: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_102, p_h_7_mlp_up_proj_weight);  mul_102 = p_h_7_mlp_up_proj_weight = None
            mul_103: "f32[1, 6, 12288]" = torch.ops.aten.mul.Tensor(silu_7, linear_46);  silu_7 = linear_46 = None
            linear_47: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(mul_103, p_h_7_mlp_down_proj_weight, p_h_7_mlp_down_proj_bias);  mul_103 = p_h_7_mlp_down_proj_weight = p_h_7_mlp_down_proj_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_23: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_47, 0.0, False);  linear_47 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_47: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_45, dropout_23);  add_45 = dropout_23 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_73: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_47, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_25: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_73, 2)
            mean_16: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_25, [-1], True);  pow_25 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_48: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_16, 1e-05);  mean_16 = None
            rsqrt_16: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_48);  add_48 = None
            mul_104: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_73, rsqrt_16);  _to_copy_73 = rsqrt_16 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_74: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_104, dtype = torch.float32);  mul_104 = None
            mul_105: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_8_input_layernorm_weight, _to_copy_74);  p_h_8_input_layernorm_weight = _to_copy_74 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:404 in forward, code: hidden_states = hidden_states.transpose(1, 0)
            transpose_40: "f32[6, 1, 5120]" = torch.ops.aten.transpose.int(mul_105, 1, 0);  mul_105 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:405 in forward, code: query_layer = self.query(hidden_states)
            linear_48: "f32[6, 1, 5120]" = torch.ops.aten.linear.default(transpose_40, p_h_8_self_attention_query_weight);  p_h_8_self_attention_query_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:409 in forward, code: query_layer = query_layer.view(*new_tensor_shape)
            view_96: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(linear_48, [6, 1, 32, 160]);  linear_48 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:411 in forward, code: mixed_kv_layer = self.key_value(hidden_states)
            linear_49: "f32[6, 1, 10240]" = torch.ops.aten.linear.default(transpose_40, p_h_8_self_attention_key_value_weight);  transpose_40 = p_h_8_self_attention_key_value_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:415 in forward, code: mixed_kv_layer = mixed_kv_layer.view(*new_tensor_shape)
            view_97: "f32[6, 1, 32, 320]" = torch.ops.aten.view.default(linear_49, [6, 1, 32, 320]);  linear_49 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:381 in split_tensor_along_last_dim, code: tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)
            split_8 = torch.ops.aten.split.Tensor(view_97, 160, 3);  view_97 = None
            getitem_16: "f32[6, 1, 32, 160]" = split_8[0]
            getitem_17: "f32[6, 1, 32, 160]" = split_8[1];  split_8 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:423 in forward, code: query_layer = query_layer.view(output_size[2], output_size[0] * output_size[1], -1)
            view_98: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_96, [6, 32, -1]);  view_96 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:424 in forward, code: key_layer = key_layer.view(output_size[3], output_size[0] * output_size[1], -1)
            view_99: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_16, [6, 32, -1]);  getitem_16 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:112 in forward, code: self.inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, device=x.device).float() / self.dim))
            arange_17: "i64[80]" = torch.ops.aten.arange.start_step(0, 160, 2, device = device(type='cpu'), pin_memory = False)
            _to_copy_75: "f32[80]" = torch.ops.aten._to_copy.default(arange_17, dtype = torch.float32);  arange_17 = None
            div_8: "f32[80]" = torch.ops.aten.div.Tensor(_to_copy_75, 160);  _to_copy_75 = None
            pow_26: "f32[80]" = torch.ops.aten.pow.Scalar(30420.108888514722, div_8);  div_8 = None
            reciprocal_8: "f32[80]" = torch.ops.aten.reciprocal.default(pow_26);  pow_26 = None
            mul_106: "f32[80]" = torch.ops.aten.mul.Tensor(reciprocal_8, 1.0);  reciprocal_8 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:114 in forward, code: t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)
            arange_18: "f32[8192]" = torch.ops.aten.arange.default(8192, dtype = torch.float32, device = device(type='cpu'), pin_memory = False)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:115 in forward, code: freqs = torch.einsum('i,j->ij', t, self.inv_freq)
            einsum_16: "f32[8192, 80]" = torch.ops.aten.einsum.default('i,j->ij', [arange_18, mul_106]);  arange_18 = mul_106 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:117 in forward, code: emb = torch.cat((freqs, freqs), dim=-1).to(x.device)
            cat_24: "f32[8192, 160]" = torch.ops.aten.cat.default([einsum_16, einsum_16], -1);  einsum_16 = None
            _to_copy_76: "f32[8192, 160]" = torch.ops.aten._to_copy.default(cat_24, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  cat_24 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:121 in forward, code: self.cos_cached = self.mscale * emb.cos()[:, None, :].to(dtype)
            cos_8: "f32[8192, 160]" = torch.ops.aten.cos.default(_to_copy_76)
            slice_94: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(cos_8, 0, 0, 9223372036854775807);  cos_8 = None
            unsqueeze_24: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_94, 1);  slice_94 = None
            slice_95: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_24, 2, 0, 9223372036854775807);  unsqueeze_24 = None
            _to_copy_77: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_95, dtype = torch.float32);  slice_95 = None
            mul_107: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_77, 1.0);  _to_copy_77 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:122 in forward, code: self.sin_cached = self.mscale * emb.sin()[:, None, :].to(dtype)
            sin_8: "f32[8192, 160]" = torch.ops.aten.sin.default(_to_copy_76);  _to_copy_76 = None
            slice_96: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(sin_8, 0, 0, 9223372036854775807);  sin_8 = None
            unsqueeze_25: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_96, 1);  slice_96 = None
            slice_97: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_25, 2, 0, 9223372036854775807);  unsqueeze_25 = None
            _to_copy_78: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_97, dtype = torch.float32);  slice_97 = None
            mul_108: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_78, 1.0);  _to_copy_78 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:123 in forward, code: return self.cos_cached[:seq_len, ...], self.sin_cached[:seq_len, ...]
            alias_16: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_107);  mul_107 = None
            alias_17: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_108);  mul_108 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:133 in apply_rotary_pos_emb_torch, code: cos, sin = cos[offset:q.shape[0] + offset, ...], sin[offset:q.shape[0] + offset, ...]
            slice_98: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_16, 0, 0, 6);  alias_16 = None
            slice_99: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_17, 0, 0, 6);  alias_17 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_109: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_98, slice_98)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_100: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_98, 2, 0, 80)
            slice_101: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_98, 2, 80, 9223372036854775807);  view_98 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_16: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_101);  slice_101 = None
            cat_25: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_16, slice_100], 2);  neg_16 = slice_100 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_110: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_25, slice_99);  cat_25 = None
            add_49: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_109, mul_110);  mul_109 = mul_110 = None
            mul_111: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_99, slice_98);  slice_98 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_102: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_99, 2, 0, 80)
            slice_103: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_99, 2, 80, 9223372036854775807);  view_99 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_17: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_103);  slice_103 = None
            cat_26: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_17, slice_102], 2);  neg_17 = slice_102 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_112: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_26, slice_99);  cat_26 = slice_99 = None
            add_50: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_111, mul_112);  mul_111 = mul_112 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:448 in forward, code: query_layer = query_layer.reshape((s_query, bz, head, dim))
            view_100: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_49, [6, 1, 32, 160]);  add_49 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:449 in forward, code: key_layer = key_layer.reshape((s_key, bz, head, dim))
            view_101: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_50, [6, 1, 32, 160]);  add_50 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:458 in forward, code: query_layer = query_layer.reshape(s_query, bz * self.num_heads, dim)
            view_102: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_100, [6, 32, 160]);  view_100 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:460 in forward, code: key_layer = key_layer.reshape(s_key, bz * self.num_heads, dim)
            view_103: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_101, [6, 32, 160]);  view_101 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            transpose_41: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_102, 0, 1);  view_102 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:462 in forward, code: key_layer.transpose(0, 1).transpose(1, 2))
            transpose_42: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_103, 0, 1);  view_103 = None
            transpose_43: "f32[32, 160, 6]" = torch.ops.aten.transpose.int(transpose_42, 1, 2);  transpose_42 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            einsum_17: "f32[32, 6, 6]" = torch.ops.aten.einsum.default('bik,bkj->bij', [transpose_41, transpose_43]);  transpose_41 = transpose_43 = None
            mul_113: "f32[32, 6, 6]" = torch.ops.aten.mul.Tensor(einsum_17, 0.07905694150420949);  einsum_17 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:464 in forward, code: attention_scores = matmul_result.view(bz, self.num_heads, s_query, s_key)
            view_104: "f32[1, 32, 6, 6]" = torch.ops.aten.view.default(mul_113, [1, 32, 6, 6]);  mul_113 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:469 in forward, code: attn_weights = torch.masked_fill(attention_scores, attention_mask, torch.finfo(attention_scores.dtype).min)
            masked_fill_8: "f32[1, 32, 6, 6]" = torch.ops.aten.masked_fill.Scalar(view_104, or_1, -3.4028234663852886e+38);  view_104 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:470 in forward, code: attention_probs = F.softmax(attn_weights, dim=-1).to(input_dtype)  ##dtype = torch.float32
            softmax_8: "f32[1, 32, 6, 6]" = torch.ops.aten.softmax.int(masked_fill_8, -1);  masked_fill_8 = None
            _to_copy_79: "f32[1, 32, 6, 6]" = torch.ops.aten._to_copy.default(softmax_8, dtype = torch.float32);  softmax_8 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:471 in forward, code: attention_probs = self.attention_dropout(attention_probs)
            dropout_24: "f32[1, 32, 6, 6]" = torch.ops.aten.dropout.default(_to_copy_79, 0.0, False);  _to_copy_79 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:472 in forward, code: attention_probs_reshaped = attention_probs.view(bz * self.num_heads, s_query, s_key)
            view_105: "f32[32, 6, 6]" = torch.ops.aten.view.default(dropout_24, [32, 6, 6]);  dropout_24 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:474 in forward, code: value_layer = value_layer.reshape(s_key, bz * self.num_heads, dim)
            view_106: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_17, [6, 32, 160]);  getitem_17 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:475 in forward, code: context_layer = torch.bmm(attention_probs_reshaped, value_layer.transpose(0, 1))
            transpose_44: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_106, 0, 1);  view_106 = None
            bmm_8: "f32[32, 6, 160]" = torch.ops.aten.bmm.default(view_105, transpose_44);  view_105 = transpose_44 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:391 in _merge_heads, code: x = x.view(batch_size, self.num_heads, seq_length, self.head_dim)
            view_107: "f32[1, 32, 6, 160]" = torch.ops.aten.view.default(bmm_8, [1, 32, 6, 160]);  bmm_8 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:392 in _merge_heads, code: x = x.permute(0, 2, 1, 3)
            permute_8: "f32[1, 6, 32, 160]" = torch.ops.aten.permute.default(view_107, [0, 2, 1, 3]);  view_107 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:393 in _merge_heads, code: return x.reshape(batch_size, seq_length, self.num_heads * self.head_dim)
            clone_8: "f32[1, 6, 32, 160]" = torch.ops.aten.clone.default(permute_8, memory_format = torch.contiguous_format);  permute_8 = None
            _unsafe_view_8: "f32[1, 6, 5120]" = torch.ops.aten._unsafe_view.default(clone_8, [1, 6, 5120]);  clone_8 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:478 in forward, code: output_tensor = self.dense(context_layer)
            linear_50: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(_unsafe_view_8, p_h_8_self_attention_dense_weight, p_h_8_self_attention_dense_bias);  _unsafe_view_8 = p_h_8_self_attention_dense_weight = p_h_8_self_attention_dense_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_25: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_50, 0.0, False);  linear_50 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_51: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_47, dropout_25);  add_47 = dropout_25 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_80: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_51, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_27: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_80, 2)
            mean_17: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_27, [-1], True);  pow_27 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_52: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_17, 1e-05);  mean_17 = None
            rsqrt_17: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_52);  add_52 = None
            mul_114: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_80, rsqrt_17);  _to_copy_80 = rsqrt_17 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_81: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_114, dtype = torch.float32);  mul_114 = None
            mul_115: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_8_post_attention_layernorm_weight, _to_copy_81);  p_h_8_post_attention_layernorm_weight = _to_copy_81 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:499 in forward, code: intermediate_output = self.down_proj(F.silu(self.gate_proj(hidden_states)) * self.up_proj(hidden_states))
            linear_51: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_115, p_h_8_mlp_gate_proj_weight);  p_h_8_mlp_gate_proj_weight = None
            silu_8: "f32[1, 6, 12288]" = torch.ops.aten.silu.default(linear_51);  linear_51 = None
            linear_52: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_115, p_h_8_mlp_up_proj_weight);  mul_115 = p_h_8_mlp_up_proj_weight = None
            mul_116: "f32[1, 6, 12288]" = torch.ops.aten.mul.Tensor(silu_8, linear_52);  silu_8 = linear_52 = None
            linear_53: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(mul_116, p_h_8_mlp_down_proj_weight, p_h_8_mlp_down_proj_bias);  mul_116 = p_h_8_mlp_down_proj_weight = p_h_8_mlp_down_proj_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_26: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_53, 0.0, False);  linear_53 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_53: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_51, dropout_26);  add_51 = dropout_26 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_82: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_53, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_28: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_82, 2)
            mean_18: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_28, [-1], True);  pow_28 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_54: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_18, 1e-05);  mean_18 = None
            rsqrt_18: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_54);  add_54 = None
            mul_117: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_82, rsqrt_18);  _to_copy_82 = rsqrt_18 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_83: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_117, dtype = torch.float32);  mul_117 = None
            mul_118: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_9_input_layernorm_weight, _to_copy_83);  p_h_9_input_layernorm_weight = _to_copy_83 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:404 in forward, code: hidden_states = hidden_states.transpose(1, 0)
            transpose_45: "f32[6, 1, 5120]" = torch.ops.aten.transpose.int(mul_118, 1, 0);  mul_118 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:405 in forward, code: query_layer = self.query(hidden_states)
            linear_54: "f32[6, 1, 5120]" = torch.ops.aten.linear.default(transpose_45, p_h_9_self_attention_query_weight);  p_h_9_self_attention_query_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:409 in forward, code: query_layer = query_layer.view(*new_tensor_shape)
            view_108: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(linear_54, [6, 1, 32, 160]);  linear_54 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:411 in forward, code: mixed_kv_layer = self.key_value(hidden_states)
            linear_55: "f32[6, 1, 10240]" = torch.ops.aten.linear.default(transpose_45, p_h_9_self_attention_key_value_weight);  transpose_45 = p_h_9_self_attention_key_value_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:415 in forward, code: mixed_kv_layer = mixed_kv_layer.view(*new_tensor_shape)
            view_109: "f32[6, 1, 32, 320]" = torch.ops.aten.view.default(linear_55, [6, 1, 32, 320]);  linear_55 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:381 in split_tensor_along_last_dim, code: tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)
            split_9 = torch.ops.aten.split.Tensor(view_109, 160, 3);  view_109 = None
            getitem_18: "f32[6, 1, 32, 160]" = split_9[0]
            getitem_19: "f32[6, 1, 32, 160]" = split_9[1];  split_9 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:423 in forward, code: query_layer = query_layer.view(output_size[2], output_size[0] * output_size[1], -1)
            view_110: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_108, [6, 32, -1]);  view_108 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:424 in forward, code: key_layer = key_layer.view(output_size[3], output_size[0] * output_size[1], -1)
            view_111: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_18, [6, 32, -1]);  getitem_18 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:112 in forward, code: self.inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, device=x.device).float() / self.dim))
            arange_19: "i64[80]" = torch.ops.aten.arange.start_step(0, 160, 2, device = device(type='cpu'), pin_memory = False)
            _to_copy_84: "f32[80]" = torch.ops.aten._to_copy.default(arange_19, dtype = torch.float32);  arange_19 = None
            div_9: "f32[80]" = torch.ops.aten.div.Tensor(_to_copy_84, 160);  _to_copy_84 = None
            pow_29: "f32[80]" = torch.ops.aten.pow.Scalar(30420.108888514722, div_9);  div_9 = None
            reciprocal_9: "f32[80]" = torch.ops.aten.reciprocal.default(pow_29);  pow_29 = None
            mul_119: "f32[80]" = torch.ops.aten.mul.Tensor(reciprocal_9, 1.0);  reciprocal_9 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:114 in forward, code: t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)
            arange_20: "f32[8192]" = torch.ops.aten.arange.default(8192, dtype = torch.float32, device = device(type='cpu'), pin_memory = False)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:115 in forward, code: freqs = torch.einsum('i,j->ij', t, self.inv_freq)
            einsum_18: "f32[8192, 80]" = torch.ops.aten.einsum.default('i,j->ij', [arange_20, mul_119]);  arange_20 = mul_119 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:117 in forward, code: emb = torch.cat((freqs, freqs), dim=-1).to(x.device)
            cat_27: "f32[8192, 160]" = torch.ops.aten.cat.default([einsum_18, einsum_18], -1);  einsum_18 = None
            _to_copy_85: "f32[8192, 160]" = torch.ops.aten._to_copy.default(cat_27, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  cat_27 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:121 in forward, code: self.cos_cached = self.mscale * emb.cos()[:, None, :].to(dtype)
            cos_9: "f32[8192, 160]" = torch.ops.aten.cos.default(_to_copy_85)
            slice_104: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(cos_9, 0, 0, 9223372036854775807);  cos_9 = None
            unsqueeze_26: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_104, 1);  slice_104 = None
            slice_105: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_26, 2, 0, 9223372036854775807);  unsqueeze_26 = None
            _to_copy_86: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_105, dtype = torch.float32);  slice_105 = None
            mul_120: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_86, 1.0);  _to_copy_86 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:122 in forward, code: self.sin_cached = self.mscale * emb.sin()[:, None, :].to(dtype)
            sin_9: "f32[8192, 160]" = torch.ops.aten.sin.default(_to_copy_85);  _to_copy_85 = None
            slice_106: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(sin_9, 0, 0, 9223372036854775807);  sin_9 = None
            unsqueeze_27: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_106, 1);  slice_106 = None
            slice_107: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_27, 2, 0, 9223372036854775807);  unsqueeze_27 = None
            _to_copy_87: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_107, dtype = torch.float32);  slice_107 = None
            mul_121: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_87, 1.0);  _to_copy_87 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:123 in forward, code: return self.cos_cached[:seq_len, ...], self.sin_cached[:seq_len, ...]
            alias_18: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_120);  mul_120 = None
            alias_19: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_121);  mul_121 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:133 in apply_rotary_pos_emb_torch, code: cos, sin = cos[offset:q.shape[0] + offset, ...], sin[offset:q.shape[0] + offset, ...]
            slice_108: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_18, 0, 0, 6);  alias_18 = None
            slice_109: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_19, 0, 0, 6);  alias_19 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_122: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_110, slice_108)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_110: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_110, 2, 0, 80)
            slice_111: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_110, 2, 80, 9223372036854775807);  view_110 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_18: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_111);  slice_111 = None
            cat_28: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_18, slice_110], 2);  neg_18 = slice_110 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_123: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_28, slice_109);  cat_28 = None
            add_55: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_122, mul_123);  mul_122 = mul_123 = None
            mul_124: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_111, slice_108);  slice_108 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_112: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_111, 2, 0, 80)
            slice_113: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_111, 2, 80, 9223372036854775807);  view_111 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_19: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_113);  slice_113 = None
            cat_29: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_19, slice_112], 2);  neg_19 = slice_112 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_125: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_29, slice_109);  cat_29 = slice_109 = None
            add_56: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_124, mul_125);  mul_124 = mul_125 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:448 in forward, code: query_layer = query_layer.reshape((s_query, bz, head, dim))
            view_112: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_55, [6, 1, 32, 160]);  add_55 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:449 in forward, code: key_layer = key_layer.reshape((s_key, bz, head, dim))
            view_113: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_56, [6, 1, 32, 160]);  add_56 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:458 in forward, code: query_layer = query_layer.reshape(s_query, bz * self.num_heads, dim)
            view_114: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_112, [6, 32, 160]);  view_112 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:460 in forward, code: key_layer = key_layer.reshape(s_key, bz * self.num_heads, dim)
            view_115: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_113, [6, 32, 160]);  view_113 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            transpose_46: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_114, 0, 1);  view_114 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:462 in forward, code: key_layer.transpose(0, 1).transpose(1, 2))
            transpose_47: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_115, 0, 1);  view_115 = None
            transpose_48: "f32[32, 160, 6]" = torch.ops.aten.transpose.int(transpose_47, 1, 2);  transpose_47 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            einsum_19: "f32[32, 6, 6]" = torch.ops.aten.einsum.default('bik,bkj->bij', [transpose_46, transpose_48]);  transpose_46 = transpose_48 = None
            mul_126: "f32[32, 6, 6]" = torch.ops.aten.mul.Tensor(einsum_19, 0.07905694150420949);  einsum_19 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:464 in forward, code: attention_scores = matmul_result.view(bz, self.num_heads, s_query, s_key)
            view_116: "f32[1, 32, 6, 6]" = torch.ops.aten.view.default(mul_126, [1, 32, 6, 6]);  mul_126 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:469 in forward, code: attn_weights = torch.masked_fill(attention_scores, attention_mask, torch.finfo(attention_scores.dtype).min)
            masked_fill_9: "f32[1, 32, 6, 6]" = torch.ops.aten.masked_fill.Scalar(view_116, or_1, -3.4028234663852886e+38);  view_116 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:470 in forward, code: attention_probs = F.softmax(attn_weights, dim=-1).to(input_dtype)  ##dtype = torch.float32
            softmax_9: "f32[1, 32, 6, 6]" = torch.ops.aten.softmax.int(masked_fill_9, -1);  masked_fill_9 = None
            _to_copy_88: "f32[1, 32, 6, 6]" = torch.ops.aten._to_copy.default(softmax_9, dtype = torch.float32);  softmax_9 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:471 in forward, code: attention_probs = self.attention_dropout(attention_probs)
            dropout_27: "f32[1, 32, 6, 6]" = torch.ops.aten.dropout.default(_to_copy_88, 0.0, False);  _to_copy_88 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:472 in forward, code: attention_probs_reshaped = attention_probs.view(bz * self.num_heads, s_query, s_key)
            view_117: "f32[32, 6, 6]" = torch.ops.aten.view.default(dropout_27, [32, 6, 6]);  dropout_27 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:474 in forward, code: value_layer = value_layer.reshape(s_key, bz * self.num_heads, dim)
            view_118: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_19, [6, 32, 160]);  getitem_19 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:475 in forward, code: context_layer = torch.bmm(attention_probs_reshaped, value_layer.transpose(0, 1))
            transpose_49: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_118, 0, 1);  view_118 = None
            bmm_9: "f32[32, 6, 160]" = torch.ops.aten.bmm.default(view_117, transpose_49);  view_117 = transpose_49 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:391 in _merge_heads, code: x = x.view(batch_size, self.num_heads, seq_length, self.head_dim)
            view_119: "f32[1, 32, 6, 160]" = torch.ops.aten.view.default(bmm_9, [1, 32, 6, 160]);  bmm_9 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:392 in _merge_heads, code: x = x.permute(0, 2, 1, 3)
            permute_9: "f32[1, 6, 32, 160]" = torch.ops.aten.permute.default(view_119, [0, 2, 1, 3]);  view_119 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:393 in _merge_heads, code: return x.reshape(batch_size, seq_length, self.num_heads * self.head_dim)
            clone_9: "f32[1, 6, 32, 160]" = torch.ops.aten.clone.default(permute_9, memory_format = torch.contiguous_format);  permute_9 = None
            _unsafe_view_9: "f32[1, 6, 5120]" = torch.ops.aten._unsafe_view.default(clone_9, [1, 6, 5120]);  clone_9 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:478 in forward, code: output_tensor = self.dense(context_layer)
            linear_56: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(_unsafe_view_9, p_h_9_self_attention_dense_weight, p_h_9_self_attention_dense_bias);  _unsafe_view_9 = p_h_9_self_attention_dense_weight = p_h_9_self_attention_dense_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_28: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_56, 0.0, False);  linear_56 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_57: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_53, dropout_28);  add_53 = dropout_28 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_89: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_57, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_30: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_89, 2)
            mean_19: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_30, [-1], True);  pow_30 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_58: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_19, 1e-05);  mean_19 = None
            rsqrt_19: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_58);  add_58 = None
            mul_127: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_89, rsqrt_19);  _to_copy_89 = rsqrt_19 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_90: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_127, dtype = torch.float32);  mul_127 = None
            mul_128: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_9_post_attention_layernorm_weight, _to_copy_90);  p_h_9_post_attention_layernorm_weight = _to_copy_90 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:499 in forward, code: intermediate_output = self.down_proj(F.silu(self.gate_proj(hidden_states)) * self.up_proj(hidden_states))
            linear_57: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_128, p_h_9_mlp_gate_proj_weight);  p_h_9_mlp_gate_proj_weight = None
            silu_9: "f32[1, 6, 12288]" = torch.ops.aten.silu.default(linear_57);  linear_57 = None
            linear_58: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_128, p_h_9_mlp_up_proj_weight);  mul_128 = p_h_9_mlp_up_proj_weight = None
            mul_129: "f32[1, 6, 12288]" = torch.ops.aten.mul.Tensor(silu_9, linear_58);  silu_9 = linear_58 = None
            linear_59: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(mul_129, p_h_9_mlp_down_proj_weight, p_h_9_mlp_down_proj_bias);  mul_129 = p_h_9_mlp_down_proj_weight = p_h_9_mlp_down_proj_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_29: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_59, 0.0, False);  linear_59 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_59: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_57, dropout_29);  add_57 = dropout_29 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_91: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_59, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_31: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_91, 2)
            mean_20: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_31, [-1], True);  pow_31 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_60: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_20, 1e-05);  mean_20 = None
            rsqrt_20: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_60);  add_60 = None
            mul_130: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_91, rsqrt_20);  _to_copy_91 = rsqrt_20 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_92: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_130, dtype = torch.float32);  mul_130 = None
            mul_131: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_10_input_layernorm_weight, _to_copy_92);  p_h_10_input_layernorm_weight = _to_copy_92 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:404 in forward, code: hidden_states = hidden_states.transpose(1, 0)
            transpose_50: "f32[6, 1, 5120]" = torch.ops.aten.transpose.int(mul_131, 1, 0);  mul_131 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:405 in forward, code: query_layer = self.query(hidden_states)
            linear_60: "f32[6, 1, 5120]" = torch.ops.aten.linear.default(transpose_50, p_h_10_self_attention_query_weight);  p_h_10_self_attention_query_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:409 in forward, code: query_layer = query_layer.view(*new_tensor_shape)
            view_120: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(linear_60, [6, 1, 32, 160]);  linear_60 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:411 in forward, code: mixed_kv_layer = self.key_value(hidden_states)
            linear_61: "f32[6, 1, 10240]" = torch.ops.aten.linear.default(transpose_50, p_h_10_self_attention_key_value_weight);  transpose_50 = p_h_10_self_attention_key_value_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:415 in forward, code: mixed_kv_layer = mixed_kv_layer.view(*new_tensor_shape)
            view_121: "f32[6, 1, 32, 320]" = torch.ops.aten.view.default(linear_61, [6, 1, 32, 320]);  linear_61 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:381 in split_tensor_along_last_dim, code: tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)
            split_10 = torch.ops.aten.split.Tensor(view_121, 160, 3);  view_121 = None
            getitem_20: "f32[6, 1, 32, 160]" = split_10[0]
            getitem_21: "f32[6, 1, 32, 160]" = split_10[1];  split_10 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:423 in forward, code: query_layer = query_layer.view(output_size[2], output_size[0] * output_size[1], -1)
            view_122: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_120, [6, 32, -1]);  view_120 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:424 in forward, code: key_layer = key_layer.view(output_size[3], output_size[0] * output_size[1], -1)
            view_123: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_20, [6, 32, -1]);  getitem_20 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:112 in forward, code: self.inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, device=x.device).float() / self.dim))
            arange_21: "i64[80]" = torch.ops.aten.arange.start_step(0, 160, 2, device = device(type='cpu'), pin_memory = False)
            _to_copy_93: "f32[80]" = torch.ops.aten._to_copy.default(arange_21, dtype = torch.float32);  arange_21 = None
            div_10: "f32[80]" = torch.ops.aten.div.Tensor(_to_copy_93, 160);  _to_copy_93 = None
            pow_32: "f32[80]" = torch.ops.aten.pow.Scalar(30420.108888514722, div_10);  div_10 = None
            reciprocal_10: "f32[80]" = torch.ops.aten.reciprocal.default(pow_32);  pow_32 = None
            mul_132: "f32[80]" = torch.ops.aten.mul.Tensor(reciprocal_10, 1.0);  reciprocal_10 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:114 in forward, code: t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)
            arange_22: "f32[8192]" = torch.ops.aten.arange.default(8192, dtype = torch.float32, device = device(type='cpu'), pin_memory = False)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:115 in forward, code: freqs = torch.einsum('i,j->ij', t, self.inv_freq)
            einsum_20: "f32[8192, 80]" = torch.ops.aten.einsum.default('i,j->ij', [arange_22, mul_132]);  arange_22 = mul_132 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:117 in forward, code: emb = torch.cat((freqs, freqs), dim=-1).to(x.device)
            cat_30: "f32[8192, 160]" = torch.ops.aten.cat.default([einsum_20, einsum_20], -1);  einsum_20 = None
            _to_copy_94: "f32[8192, 160]" = torch.ops.aten._to_copy.default(cat_30, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  cat_30 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:121 in forward, code: self.cos_cached = self.mscale * emb.cos()[:, None, :].to(dtype)
            cos_10: "f32[8192, 160]" = torch.ops.aten.cos.default(_to_copy_94)
            slice_114: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(cos_10, 0, 0, 9223372036854775807);  cos_10 = None
            unsqueeze_28: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_114, 1);  slice_114 = None
            slice_115: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_28, 2, 0, 9223372036854775807);  unsqueeze_28 = None
            _to_copy_95: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_115, dtype = torch.float32);  slice_115 = None
            mul_133: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_95, 1.0);  _to_copy_95 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:122 in forward, code: self.sin_cached = self.mscale * emb.sin()[:, None, :].to(dtype)
            sin_10: "f32[8192, 160]" = torch.ops.aten.sin.default(_to_copy_94);  _to_copy_94 = None
            slice_116: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(sin_10, 0, 0, 9223372036854775807);  sin_10 = None
            unsqueeze_29: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_116, 1);  slice_116 = None
            slice_117: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_29, 2, 0, 9223372036854775807);  unsqueeze_29 = None
            _to_copy_96: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_117, dtype = torch.float32);  slice_117 = None
            mul_134: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_96, 1.0);  _to_copy_96 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:123 in forward, code: return self.cos_cached[:seq_len, ...], self.sin_cached[:seq_len, ...]
            alias_20: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_133);  mul_133 = None
            alias_21: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_134);  mul_134 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:133 in apply_rotary_pos_emb_torch, code: cos, sin = cos[offset:q.shape[0] + offset, ...], sin[offset:q.shape[0] + offset, ...]
            slice_118: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_20, 0, 0, 6);  alias_20 = None
            slice_119: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_21, 0, 0, 6);  alias_21 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_135: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_122, slice_118)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_120: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_122, 2, 0, 80)
            slice_121: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_122, 2, 80, 9223372036854775807);  view_122 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_20: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_121);  slice_121 = None
            cat_31: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_20, slice_120], 2);  neg_20 = slice_120 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_136: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_31, slice_119);  cat_31 = None
            add_61: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_135, mul_136);  mul_135 = mul_136 = None
            mul_137: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_123, slice_118);  slice_118 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_122: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_123, 2, 0, 80)
            slice_123: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_123, 2, 80, 9223372036854775807);  view_123 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_21: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_123);  slice_123 = None
            cat_32: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_21, slice_122], 2);  neg_21 = slice_122 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_138: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_32, slice_119);  cat_32 = slice_119 = None
            add_62: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_137, mul_138);  mul_137 = mul_138 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:448 in forward, code: query_layer = query_layer.reshape((s_query, bz, head, dim))
            view_124: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_61, [6, 1, 32, 160]);  add_61 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:449 in forward, code: key_layer = key_layer.reshape((s_key, bz, head, dim))
            view_125: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_62, [6, 1, 32, 160]);  add_62 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:458 in forward, code: query_layer = query_layer.reshape(s_query, bz * self.num_heads, dim)
            view_126: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_124, [6, 32, 160]);  view_124 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:460 in forward, code: key_layer = key_layer.reshape(s_key, bz * self.num_heads, dim)
            view_127: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_125, [6, 32, 160]);  view_125 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            transpose_51: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_126, 0, 1);  view_126 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:462 in forward, code: key_layer.transpose(0, 1).transpose(1, 2))
            transpose_52: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_127, 0, 1);  view_127 = None
            transpose_53: "f32[32, 160, 6]" = torch.ops.aten.transpose.int(transpose_52, 1, 2);  transpose_52 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            einsum_21: "f32[32, 6, 6]" = torch.ops.aten.einsum.default('bik,bkj->bij', [transpose_51, transpose_53]);  transpose_51 = transpose_53 = None
            mul_139: "f32[32, 6, 6]" = torch.ops.aten.mul.Tensor(einsum_21, 0.07905694150420949);  einsum_21 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:464 in forward, code: attention_scores = matmul_result.view(bz, self.num_heads, s_query, s_key)
            view_128: "f32[1, 32, 6, 6]" = torch.ops.aten.view.default(mul_139, [1, 32, 6, 6]);  mul_139 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:469 in forward, code: attn_weights = torch.masked_fill(attention_scores, attention_mask, torch.finfo(attention_scores.dtype).min)
            masked_fill_10: "f32[1, 32, 6, 6]" = torch.ops.aten.masked_fill.Scalar(view_128, or_1, -3.4028234663852886e+38);  view_128 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:470 in forward, code: attention_probs = F.softmax(attn_weights, dim=-1).to(input_dtype)  ##dtype = torch.float32
            softmax_10: "f32[1, 32, 6, 6]" = torch.ops.aten.softmax.int(masked_fill_10, -1);  masked_fill_10 = None
            _to_copy_97: "f32[1, 32, 6, 6]" = torch.ops.aten._to_copy.default(softmax_10, dtype = torch.float32);  softmax_10 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:471 in forward, code: attention_probs = self.attention_dropout(attention_probs)
            dropout_30: "f32[1, 32, 6, 6]" = torch.ops.aten.dropout.default(_to_copy_97, 0.0, False);  _to_copy_97 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:472 in forward, code: attention_probs_reshaped = attention_probs.view(bz * self.num_heads, s_query, s_key)
            view_129: "f32[32, 6, 6]" = torch.ops.aten.view.default(dropout_30, [32, 6, 6]);  dropout_30 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:474 in forward, code: value_layer = value_layer.reshape(s_key, bz * self.num_heads, dim)
            view_130: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_21, [6, 32, 160]);  getitem_21 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:475 in forward, code: context_layer = torch.bmm(attention_probs_reshaped, value_layer.transpose(0, 1))
            transpose_54: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_130, 0, 1);  view_130 = None
            bmm_10: "f32[32, 6, 160]" = torch.ops.aten.bmm.default(view_129, transpose_54);  view_129 = transpose_54 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:391 in _merge_heads, code: x = x.view(batch_size, self.num_heads, seq_length, self.head_dim)
            view_131: "f32[1, 32, 6, 160]" = torch.ops.aten.view.default(bmm_10, [1, 32, 6, 160]);  bmm_10 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:392 in _merge_heads, code: x = x.permute(0, 2, 1, 3)
            permute_10: "f32[1, 6, 32, 160]" = torch.ops.aten.permute.default(view_131, [0, 2, 1, 3]);  view_131 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:393 in _merge_heads, code: return x.reshape(batch_size, seq_length, self.num_heads * self.head_dim)
            clone_10: "f32[1, 6, 32, 160]" = torch.ops.aten.clone.default(permute_10, memory_format = torch.contiguous_format);  permute_10 = None
            _unsafe_view_10: "f32[1, 6, 5120]" = torch.ops.aten._unsafe_view.default(clone_10, [1, 6, 5120]);  clone_10 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:478 in forward, code: output_tensor = self.dense(context_layer)
            linear_62: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(_unsafe_view_10, p_h_10_self_attention_dense_weight, p_h_10_self_attention_dense_bias);  _unsafe_view_10 = p_h_10_self_attention_dense_weight = p_h_10_self_attention_dense_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_31: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_62, 0.0, False);  linear_62 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_63: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_59, dropout_31);  add_59 = dropout_31 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_98: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_63, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_33: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_98, 2)
            mean_21: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_33, [-1], True);  pow_33 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_64: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_21, 1e-05);  mean_21 = None
            rsqrt_21: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_64);  add_64 = None
            mul_140: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_98, rsqrt_21);  _to_copy_98 = rsqrt_21 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_99: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_140, dtype = torch.float32);  mul_140 = None
            mul_141: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_10_post_attention_layernorm_weight, _to_copy_99);  p_h_10_post_attention_layernorm_weight = _to_copy_99 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:499 in forward, code: intermediate_output = self.down_proj(F.silu(self.gate_proj(hidden_states)) * self.up_proj(hidden_states))
            linear_63: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_141, p_h_10_mlp_gate_proj_weight);  p_h_10_mlp_gate_proj_weight = None
            silu_10: "f32[1, 6, 12288]" = torch.ops.aten.silu.default(linear_63);  linear_63 = None
            linear_64: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_141, p_h_10_mlp_up_proj_weight);  mul_141 = p_h_10_mlp_up_proj_weight = None
            mul_142: "f32[1, 6, 12288]" = torch.ops.aten.mul.Tensor(silu_10, linear_64);  silu_10 = linear_64 = None
            linear_65: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(mul_142, p_h_10_mlp_down_proj_weight, p_h_10_mlp_down_proj_bias);  mul_142 = p_h_10_mlp_down_proj_weight = p_h_10_mlp_down_proj_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_32: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_65, 0.0, False);  linear_65 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_65: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_63, dropout_32);  add_63 = dropout_32 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_100: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_65, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_34: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_100, 2)
            mean_22: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_34, [-1], True);  pow_34 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_66: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_22, 1e-05);  mean_22 = None
            rsqrt_22: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_66);  add_66 = None
            mul_143: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_100, rsqrt_22);  _to_copy_100 = rsqrt_22 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_101: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_143, dtype = torch.float32);  mul_143 = None
            mul_144: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_11_input_layernorm_weight, _to_copy_101);  p_h_11_input_layernorm_weight = _to_copy_101 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:404 in forward, code: hidden_states = hidden_states.transpose(1, 0)
            transpose_55: "f32[6, 1, 5120]" = torch.ops.aten.transpose.int(mul_144, 1, 0);  mul_144 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:405 in forward, code: query_layer = self.query(hidden_states)
            linear_66: "f32[6, 1, 5120]" = torch.ops.aten.linear.default(transpose_55, p_h_11_self_attention_query_weight);  p_h_11_self_attention_query_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:409 in forward, code: query_layer = query_layer.view(*new_tensor_shape)
            view_132: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(linear_66, [6, 1, 32, 160]);  linear_66 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:411 in forward, code: mixed_kv_layer = self.key_value(hidden_states)
            linear_67: "f32[6, 1, 10240]" = torch.ops.aten.linear.default(transpose_55, p_h_11_self_attention_key_value_weight);  transpose_55 = p_h_11_self_attention_key_value_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:415 in forward, code: mixed_kv_layer = mixed_kv_layer.view(*new_tensor_shape)
            view_133: "f32[6, 1, 32, 320]" = torch.ops.aten.view.default(linear_67, [6, 1, 32, 320]);  linear_67 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:381 in split_tensor_along_last_dim, code: tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)
            split_11 = torch.ops.aten.split.Tensor(view_133, 160, 3);  view_133 = None
            getitem_22: "f32[6, 1, 32, 160]" = split_11[0]
            getitem_23: "f32[6, 1, 32, 160]" = split_11[1];  split_11 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:423 in forward, code: query_layer = query_layer.view(output_size[2], output_size[0] * output_size[1], -1)
            view_134: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_132, [6, 32, -1]);  view_132 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:424 in forward, code: key_layer = key_layer.view(output_size[3], output_size[0] * output_size[1], -1)
            view_135: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_22, [6, 32, -1]);  getitem_22 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:112 in forward, code: self.inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, device=x.device).float() / self.dim))
            arange_23: "i64[80]" = torch.ops.aten.arange.start_step(0, 160, 2, device = device(type='cpu'), pin_memory = False)
            _to_copy_102: "f32[80]" = torch.ops.aten._to_copy.default(arange_23, dtype = torch.float32);  arange_23 = None
            div_11: "f32[80]" = torch.ops.aten.div.Tensor(_to_copy_102, 160);  _to_copy_102 = None
            pow_35: "f32[80]" = torch.ops.aten.pow.Scalar(30420.108888514722, div_11);  div_11 = None
            reciprocal_11: "f32[80]" = torch.ops.aten.reciprocal.default(pow_35);  pow_35 = None
            mul_145: "f32[80]" = torch.ops.aten.mul.Tensor(reciprocal_11, 1.0);  reciprocal_11 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:114 in forward, code: t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)
            arange_24: "f32[8192]" = torch.ops.aten.arange.default(8192, dtype = torch.float32, device = device(type='cpu'), pin_memory = False)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:115 in forward, code: freqs = torch.einsum('i,j->ij', t, self.inv_freq)
            einsum_22: "f32[8192, 80]" = torch.ops.aten.einsum.default('i,j->ij', [arange_24, mul_145]);  arange_24 = mul_145 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:117 in forward, code: emb = torch.cat((freqs, freqs), dim=-1).to(x.device)
            cat_33: "f32[8192, 160]" = torch.ops.aten.cat.default([einsum_22, einsum_22], -1);  einsum_22 = None
            _to_copy_103: "f32[8192, 160]" = torch.ops.aten._to_copy.default(cat_33, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  cat_33 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:121 in forward, code: self.cos_cached = self.mscale * emb.cos()[:, None, :].to(dtype)
            cos_11: "f32[8192, 160]" = torch.ops.aten.cos.default(_to_copy_103)
            slice_124: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(cos_11, 0, 0, 9223372036854775807);  cos_11 = None
            unsqueeze_30: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_124, 1);  slice_124 = None
            slice_125: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_30, 2, 0, 9223372036854775807);  unsqueeze_30 = None
            _to_copy_104: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_125, dtype = torch.float32);  slice_125 = None
            mul_146: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_104, 1.0);  _to_copy_104 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:122 in forward, code: self.sin_cached = self.mscale * emb.sin()[:, None, :].to(dtype)
            sin_11: "f32[8192, 160]" = torch.ops.aten.sin.default(_to_copy_103);  _to_copy_103 = None
            slice_126: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(sin_11, 0, 0, 9223372036854775807);  sin_11 = None
            unsqueeze_31: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_126, 1);  slice_126 = None
            slice_127: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_31, 2, 0, 9223372036854775807);  unsqueeze_31 = None
            _to_copy_105: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_127, dtype = torch.float32);  slice_127 = None
            mul_147: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_105, 1.0);  _to_copy_105 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:123 in forward, code: return self.cos_cached[:seq_len, ...], self.sin_cached[:seq_len, ...]
            alias_22: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_146);  mul_146 = None
            alias_23: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_147);  mul_147 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:133 in apply_rotary_pos_emb_torch, code: cos, sin = cos[offset:q.shape[0] + offset, ...], sin[offset:q.shape[0] + offset, ...]
            slice_128: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_22, 0, 0, 6);  alias_22 = None
            slice_129: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_23, 0, 0, 6);  alias_23 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_148: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_134, slice_128)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_130: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_134, 2, 0, 80)
            slice_131: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_134, 2, 80, 9223372036854775807);  view_134 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_22: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_131);  slice_131 = None
            cat_34: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_22, slice_130], 2);  neg_22 = slice_130 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_149: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_34, slice_129);  cat_34 = None
            add_67: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_148, mul_149);  mul_148 = mul_149 = None
            mul_150: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_135, slice_128);  slice_128 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_132: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_135, 2, 0, 80)
            slice_133: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_135, 2, 80, 9223372036854775807);  view_135 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_23: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_133);  slice_133 = None
            cat_35: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_23, slice_132], 2);  neg_23 = slice_132 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_151: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_35, slice_129);  cat_35 = slice_129 = None
            add_68: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_150, mul_151);  mul_150 = mul_151 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:448 in forward, code: query_layer = query_layer.reshape((s_query, bz, head, dim))
            view_136: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_67, [6, 1, 32, 160]);  add_67 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:449 in forward, code: key_layer = key_layer.reshape((s_key, bz, head, dim))
            view_137: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_68, [6, 1, 32, 160]);  add_68 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:458 in forward, code: query_layer = query_layer.reshape(s_query, bz * self.num_heads, dim)
            view_138: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_136, [6, 32, 160]);  view_136 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:460 in forward, code: key_layer = key_layer.reshape(s_key, bz * self.num_heads, dim)
            view_139: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_137, [6, 32, 160]);  view_137 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            transpose_56: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_138, 0, 1);  view_138 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:462 in forward, code: key_layer.transpose(0, 1).transpose(1, 2))
            transpose_57: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_139, 0, 1);  view_139 = None
            transpose_58: "f32[32, 160, 6]" = torch.ops.aten.transpose.int(transpose_57, 1, 2);  transpose_57 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            einsum_23: "f32[32, 6, 6]" = torch.ops.aten.einsum.default('bik,bkj->bij', [transpose_56, transpose_58]);  transpose_56 = transpose_58 = None
            mul_152: "f32[32, 6, 6]" = torch.ops.aten.mul.Tensor(einsum_23, 0.07905694150420949);  einsum_23 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:464 in forward, code: attention_scores = matmul_result.view(bz, self.num_heads, s_query, s_key)
            view_140: "f32[1, 32, 6, 6]" = torch.ops.aten.view.default(mul_152, [1, 32, 6, 6]);  mul_152 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:469 in forward, code: attn_weights = torch.masked_fill(attention_scores, attention_mask, torch.finfo(attention_scores.dtype).min)
            masked_fill_11: "f32[1, 32, 6, 6]" = torch.ops.aten.masked_fill.Scalar(view_140, or_1, -3.4028234663852886e+38);  view_140 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:470 in forward, code: attention_probs = F.softmax(attn_weights, dim=-1).to(input_dtype)  ##dtype = torch.float32
            softmax_11: "f32[1, 32, 6, 6]" = torch.ops.aten.softmax.int(masked_fill_11, -1);  masked_fill_11 = None
            _to_copy_106: "f32[1, 32, 6, 6]" = torch.ops.aten._to_copy.default(softmax_11, dtype = torch.float32);  softmax_11 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:471 in forward, code: attention_probs = self.attention_dropout(attention_probs)
            dropout_33: "f32[1, 32, 6, 6]" = torch.ops.aten.dropout.default(_to_copy_106, 0.0, False);  _to_copy_106 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:472 in forward, code: attention_probs_reshaped = attention_probs.view(bz * self.num_heads, s_query, s_key)
            view_141: "f32[32, 6, 6]" = torch.ops.aten.view.default(dropout_33, [32, 6, 6]);  dropout_33 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:474 in forward, code: value_layer = value_layer.reshape(s_key, bz * self.num_heads, dim)
            view_142: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_23, [6, 32, 160]);  getitem_23 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:475 in forward, code: context_layer = torch.bmm(attention_probs_reshaped, value_layer.transpose(0, 1))
            transpose_59: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_142, 0, 1);  view_142 = None
            bmm_11: "f32[32, 6, 160]" = torch.ops.aten.bmm.default(view_141, transpose_59);  view_141 = transpose_59 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:391 in _merge_heads, code: x = x.view(batch_size, self.num_heads, seq_length, self.head_dim)
            view_143: "f32[1, 32, 6, 160]" = torch.ops.aten.view.default(bmm_11, [1, 32, 6, 160]);  bmm_11 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:392 in _merge_heads, code: x = x.permute(0, 2, 1, 3)
            permute_11: "f32[1, 6, 32, 160]" = torch.ops.aten.permute.default(view_143, [0, 2, 1, 3]);  view_143 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:393 in _merge_heads, code: return x.reshape(batch_size, seq_length, self.num_heads * self.head_dim)
            clone_11: "f32[1, 6, 32, 160]" = torch.ops.aten.clone.default(permute_11, memory_format = torch.contiguous_format);  permute_11 = None
            _unsafe_view_11: "f32[1, 6, 5120]" = torch.ops.aten._unsafe_view.default(clone_11, [1, 6, 5120]);  clone_11 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:478 in forward, code: output_tensor = self.dense(context_layer)
            linear_68: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(_unsafe_view_11, p_h_11_self_attention_dense_weight, p_h_11_self_attention_dense_bias);  _unsafe_view_11 = p_h_11_self_attention_dense_weight = p_h_11_self_attention_dense_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_34: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_68, 0.0, False);  linear_68 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_69: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_65, dropout_34);  add_65 = dropout_34 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_107: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_69, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_36: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_107, 2)
            mean_23: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_36, [-1], True);  pow_36 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_70: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_23, 1e-05);  mean_23 = None
            rsqrt_23: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_70);  add_70 = None
            mul_153: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_107, rsqrt_23);  _to_copy_107 = rsqrt_23 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_108: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_153, dtype = torch.float32);  mul_153 = None
            mul_154: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_11_post_attention_layernorm_weight, _to_copy_108);  p_h_11_post_attention_layernorm_weight = _to_copy_108 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:499 in forward, code: intermediate_output = self.down_proj(F.silu(self.gate_proj(hidden_states)) * self.up_proj(hidden_states))
            linear_69: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_154, p_h_11_mlp_gate_proj_weight);  p_h_11_mlp_gate_proj_weight = None
            silu_11: "f32[1, 6, 12288]" = torch.ops.aten.silu.default(linear_69);  linear_69 = None
            linear_70: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_154, p_h_11_mlp_up_proj_weight);  mul_154 = p_h_11_mlp_up_proj_weight = None
            mul_155: "f32[1, 6, 12288]" = torch.ops.aten.mul.Tensor(silu_11, linear_70);  silu_11 = linear_70 = None
            linear_71: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(mul_155, p_h_11_mlp_down_proj_weight, p_h_11_mlp_down_proj_bias);  mul_155 = p_h_11_mlp_down_proj_weight = p_h_11_mlp_down_proj_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_35: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_71, 0.0, False);  linear_71 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_71: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_69, dropout_35);  add_69 = dropout_35 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_109: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_71, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_37: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_109, 2)
            mean_24: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_37, [-1], True);  pow_37 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_72: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_24, 1e-05);  mean_24 = None
            rsqrt_24: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_72);  add_72 = None
            mul_156: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_109, rsqrt_24);  _to_copy_109 = rsqrt_24 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_110: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_156, dtype = torch.float32);  mul_156 = None
            mul_157: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_12_input_layernorm_weight, _to_copy_110);  p_h_12_input_layernorm_weight = _to_copy_110 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:404 in forward, code: hidden_states = hidden_states.transpose(1, 0)
            transpose_60: "f32[6, 1, 5120]" = torch.ops.aten.transpose.int(mul_157, 1, 0);  mul_157 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:405 in forward, code: query_layer = self.query(hidden_states)
            linear_72: "f32[6, 1, 5120]" = torch.ops.aten.linear.default(transpose_60, p_h_12_self_attention_query_weight);  p_h_12_self_attention_query_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:409 in forward, code: query_layer = query_layer.view(*new_tensor_shape)
            view_144: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(linear_72, [6, 1, 32, 160]);  linear_72 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:411 in forward, code: mixed_kv_layer = self.key_value(hidden_states)
            linear_73: "f32[6, 1, 10240]" = torch.ops.aten.linear.default(transpose_60, p_h_12_self_attention_key_value_weight);  transpose_60 = p_h_12_self_attention_key_value_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:415 in forward, code: mixed_kv_layer = mixed_kv_layer.view(*new_tensor_shape)
            view_145: "f32[6, 1, 32, 320]" = torch.ops.aten.view.default(linear_73, [6, 1, 32, 320]);  linear_73 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:381 in split_tensor_along_last_dim, code: tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)
            split_12 = torch.ops.aten.split.Tensor(view_145, 160, 3);  view_145 = None
            getitem_24: "f32[6, 1, 32, 160]" = split_12[0]
            getitem_25: "f32[6, 1, 32, 160]" = split_12[1];  split_12 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:423 in forward, code: query_layer = query_layer.view(output_size[2], output_size[0] * output_size[1], -1)
            view_146: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_144, [6, 32, -1]);  view_144 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:424 in forward, code: key_layer = key_layer.view(output_size[3], output_size[0] * output_size[1], -1)
            view_147: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_24, [6, 32, -1]);  getitem_24 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:112 in forward, code: self.inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, device=x.device).float() / self.dim))
            arange_25: "i64[80]" = torch.ops.aten.arange.start_step(0, 160, 2, device = device(type='cpu'), pin_memory = False)
            _to_copy_111: "f32[80]" = torch.ops.aten._to_copy.default(arange_25, dtype = torch.float32);  arange_25 = None
            div_12: "f32[80]" = torch.ops.aten.div.Tensor(_to_copy_111, 160);  _to_copy_111 = None
            pow_38: "f32[80]" = torch.ops.aten.pow.Scalar(30420.108888514722, div_12);  div_12 = None
            reciprocal_12: "f32[80]" = torch.ops.aten.reciprocal.default(pow_38);  pow_38 = None
            mul_158: "f32[80]" = torch.ops.aten.mul.Tensor(reciprocal_12, 1.0);  reciprocal_12 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:114 in forward, code: t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)
            arange_26: "f32[8192]" = torch.ops.aten.arange.default(8192, dtype = torch.float32, device = device(type='cpu'), pin_memory = False)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:115 in forward, code: freqs = torch.einsum('i,j->ij', t, self.inv_freq)
            einsum_24: "f32[8192, 80]" = torch.ops.aten.einsum.default('i,j->ij', [arange_26, mul_158]);  arange_26 = mul_158 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:117 in forward, code: emb = torch.cat((freqs, freqs), dim=-1).to(x.device)
            cat_36: "f32[8192, 160]" = torch.ops.aten.cat.default([einsum_24, einsum_24], -1);  einsum_24 = None
            _to_copy_112: "f32[8192, 160]" = torch.ops.aten._to_copy.default(cat_36, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  cat_36 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:121 in forward, code: self.cos_cached = self.mscale * emb.cos()[:, None, :].to(dtype)
            cos_12: "f32[8192, 160]" = torch.ops.aten.cos.default(_to_copy_112)
            slice_134: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(cos_12, 0, 0, 9223372036854775807);  cos_12 = None
            unsqueeze_32: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_134, 1);  slice_134 = None
            slice_135: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_32, 2, 0, 9223372036854775807);  unsqueeze_32 = None
            _to_copy_113: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_135, dtype = torch.float32);  slice_135 = None
            mul_159: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_113, 1.0);  _to_copy_113 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:122 in forward, code: self.sin_cached = self.mscale * emb.sin()[:, None, :].to(dtype)
            sin_12: "f32[8192, 160]" = torch.ops.aten.sin.default(_to_copy_112);  _to_copy_112 = None
            slice_136: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(sin_12, 0, 0, 9223372036854775807);  sin_12 = None
            unsqueeze_33: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_136, 1);  slice_136 = None
            slice_137: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_33, 2, 0, 9223372036854775807);  unsqueeze_33 = None
            _to_copy_114: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_137, dtype = torch.float32);  slice_137 = None
            mul_160: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_114, 1.0);  _to_copy_114 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:123 in forward, code: return self.cos_cached[:seq_len, ...], self.sin_cached[:seq_len, ...]
            alias_24: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_159);  mul_159 = None
            alias_25: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_160);  mul_160 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:133 in apply_rotary_pos_emb_torch, code: cos, sin = cos[offset:q.shape[0] + offset, ...], sin[offset:q.shape[0] + offset, ...]
            slice_138: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_24, 0, 0, 6);  alias_24 = None
            slice_139: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_25, 0, 0, 6);  alias_25 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_161: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_146, slice_138)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_140: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_146, 2, 0, 80)
            slice_141: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_146, 2, 80, 9223372036854775807);  view_146 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_24: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_141);  slice_141 = None
            cat_37: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_24, slice_140], 2);  neg_24 = slice_140 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_162: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_37, slice_139);  cat_37 = None
            add_73: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_161, mul_162);  mul_161 = mul_162 = None
            mul_163: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_147, slice_138);  slice_138 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_142: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_147, 2, 0, 80)
            slice_143: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_147, 2, 80, 9223372036854775807);  view_147 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_25: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_143);  slice_143 = None
            cat_38: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_25, slice_142], 2);  neg_25 = slice_142 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_164: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_38, slice_139);  cat_38 = slice_139 = None
            add_74: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_163, mul_164);  mul_163 = mul_164 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:448 in forward, code: query_layer = query_layer.reshape((s_query, bz, head, dim))
            view_148: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_73, [6, 1, 32, 160]);  add_73 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:449 in forward, code: key_layer = key_layer.reshape((s_key, bz, head, dim))
            view_149: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_74, [6, 1, 32, 160]);  add_74 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:458 in forward, code: query_layer = query_layer.reshape(s_query, bz * self.num_heads, dim)
            view_150: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_148, [6, 32, 160]);  view_148 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:460 in forward, code: key_layer = key_layer.reshape(s_key, bz * self.num_heads, dim)
            view_151: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_149, [6, 32, 160]);  view_149 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            transpose_61: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_150, 0, 1);  view_150 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:462 in forward, code: key_layer.transpose(0, 1).transpose(1, 2))
            transpose_62: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_151, 0, 1);  view_151 = None
            transpose_63: "f32[32, 160, 6]" = torch.ops.aten.transpose.int(transpose_62, 1, 2);  transpose_62 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            einsum_25: "f32[32, 6, 6]" = torch.ops.aten.einsum.default('bik,bkj->bij', [transpose_61, transpose_63]);  transpose_61 = transpose_63 = None
            mul_165: "f32[32, 6, 6]" = torch.ops.aten.mul.Tensor(einsum_25, 0.07905694150420949);  einsum_25 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:464 in forward, code: attention_scores = matmul_result.view(bz, self.num_heads, s_query, s_key)
            view_152: "f32[1, 32, 6, 6]" = torch.ops.aten.view.default(mul_165, [1, 32, 6, 6]);  mul_165 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:469 in forward, code: attn_weights = torch.masked_fill(attention_scores, attention_mask, torch.finfo(attention_scores.dtype).min)
            masked_fill_12: "f32[1, 32, 6, 6]" = torch.ops.aten.masked_fill.Scalar(view_152, or_1, -3.4028234663852886e+38);  view_152 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:470 in forward, code: attention_probs = F.softmax(attn_weights, dim=-1).to(input_dtype)  ##dtype = torch.float32
            softmax_12: "f32[1, 32, 6, 6]" = torch.ops.aten.softmax.int(masked_fill_12, -1);  masked_fill_12 = None
            _to_copy_115: "f32[1, 32, 6, 6]" = torch.ops.aten._to_copy.default(softmax_12, dtype = torch.float32);  softmax_12 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:471 in forward, code: attention_probs = self.attention_dropout(attention_probs)
            dropout_36: "f32[1, 32, 6, 6]" = torch.ops.aten.dropout.default(_to_copy_115, 0.0, False);  _to_copy_115 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:472 in forward, code: attention_probs_reshaped = attention_probs.view(bz * self.num_heads, s_query, s_key)
            view_153: "f32[32, 6, 6]" = torch.ops.aten.view.default(dropout_36, [32, 6, 6]);  dropout_36 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:474 in forward, code: value_layer = value_layer.reshape(s_key, bz * self.num_heads, dim)
            view_154: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_25, [6, 32, 160]);  getitem_25 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:475 in forward, code: context_layer = torch.bmm(attention_probs_reshaped, value_layer.transpose(0, 1))
            transpose_64: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_154, 0, 1);  view_154 = None
            bmm_12: "f32[32, 6, 160]" = torch.ops.aten.bmm.default(view_153, transpose_64);  view_153 = transpose_64 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:391 in _merge_heads, code: x = x.view(batch_size, self.num_heads, seq_length, self.head_dim)
            view_155: "f32[1, 32, 6, 160]" = torch.ops.aten.view.default(bmm_12, [1, 32, 6, 160]);  bmm_12 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:392 in _merge_heads, code: x = x.permute(0, 2, 1, 3)
            permute_12: "f32[1, 6, 32, 160]" = torch.ops.aten.permute.default(view_155, [0, 2, 1, 3]);  view_155 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:393 in _merge_heads, code: return x.reshape(batch_size, seq_length, self.num_heads * self.head_dim)
            clone_12: "f32[1, 6, 32, 160]" = torch.ops.aten.clone.default(permute_12, memory_format = torch.contiguous_format);  permute_12 = None
            _unsafe_view_12: "f32[1, 6, 5120]" = torch.ops.aten._unsafe_view.default(clone_12, [1, 6, 5120]);  clone_12 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:478 in forward, code: output_tensor = self.dense(context_layer)
            linear_74: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(_unsafe_view_12, p_h_12_self_attention_dense_weight, p_h_12_self_attention_dense_bias);  _unsafe_view_12 = p_h_12_self_attention_dense_weight = p_h_12_self_attention_dense_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_37: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_74, 0.0, False);  linear_74 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_75: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_71, dropout_37);  add_71 = dropout_37 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_116: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_75, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_39: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_116, 2)
            mean_25: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_39, [-1], True);  pow_39 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_76: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_25, 1e-05);  mean_25 = None
            rsqrt_25: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_76);  add_76 = None
            mul_166: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_116, rsqrt_25);  _to_copy_116 = rsqrt_25 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_117: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_166, dtype = torch.float32);  mul_166 = None
            mul_167: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_12_post_attention_layernorm_weight, _to_copy_117);  p_h_12_post_attention_layernorm_weight = _to_copy_117 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:499 in forward, code: intermediate_output = self.down_proj(F.silu(self.gate_proj(hidden_states)) * self.up_proj(hidden_states))
            linear_75: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_167, p_h_12_mlp_gate_proj_weight);  p_h_12_mlp_gate_proj_weight = None
            silu_12: "f32[1, 6, 12288]" = torch.ops.aten.silu.default(linear_75);  linear_75 = None
            linear_76: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_167, p_h_12_mlp_up_proj_weight);  mul_167 = p_h_12_mlp_up_proj_weight = None
            mul_168: "f32[1, 6, 12288]" = torch.ops.aten.mul.Tensor(silu_12, linear_76);  silu_12 = linear_76 = None
            linear_77: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(mul_168, p_h_12_mlp_down_proj_weight, p_h_12_mlp_down_proj_bias);  mul_168 = p_h_12_mlp_down_proj_weight = p_h_12_mlp_down_proj_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_38: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_77, 0.0, False);  linear_77 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_77: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_75, dropout_38);  add_75 = dropout_38 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_118: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_77, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_40: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_118, 2)
            mean_26: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_40, [-1], True);  pow_40 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_78: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_26, 1e-05);  mean_26 = None
            rsqrt_26: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_78);  add_78 = None
            mul_169: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_118, rsqrt_26);  _to_copy_118 = rsqrt_26 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_119: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_169, dtype = torch.float32);  mul_169 = None
            mul_170: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_13_input_layernorm_weight, _to_copy_119);  p_h_13_input_layernorm_weight = _to_copy_119 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:404 in forward, code: hidden_states = hidden_states.transpose(1, 0)
            transpose_65: "f32[6, 1, 5120]" = torch.ops.aten.transpose.int(mul_170, 1, 0);  mul_170 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:405 in forward, code: query_layer = self.query(hidden_states)
            linear_78: "f32[6, 1, 5120]" = torch.ops.aten.linear.default(transpose_65, p_h_13_self_attention_query_weight);  p_h_13_self_attention_query_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:409 in forward, code: query_layer = query_layer.view(*new_tensor_shape)
            view_156: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(linear_78, [6, 1, 32, 160]);  linear_78 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:411 in forward, code: mixed_kv_layer = self.key_value(hidden_states)
            linear_79: "f32[6, 1, 10240]" = torch.ops.aten.linear.default(transpose_65, p_h_13_self_attention_key_value_weight);  transpose_65 = p_h_13_self_attention_key_value_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:415 in forward, code: mixed_kv_layer = mixed_kv_layer.view(*new_tensor_shape)
            view_157: "f32[6, 1, 32, 320]" = torch.ops.aten.view.default(linear_79, [6, 1, 32, 320]);  linear_79 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:381 in split_tensor_along_last_dim, code: tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)
            split_13 = torch.ops.aten.split.Tensor(view_157, 160, 3);  view_157 = None
            getitem_26: "f32[6, 1, 32, 160]" = split_13[0]
            getitem_27: "f32[6, 1, 32, 160]" = split_13[1];  split_13 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:423 in forward, code: query_layer = query_layer.view(output_size[2], output_size[0] * output_size[1], -1)
            view_158: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_156, [6, 32, -1]);  view_156 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:424 in forward, code: key_layer = key_layer.view(output_size[3], output_size[0] * output_size[1], -1)
            view_159: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_26, [6, 32, -1]);  getitem_26 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:112 in forward, code: self.inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, device=x.device).float() / self.dim))
            arange_27: "i64[80]" = torch.ops.aten.arange.start_step(0, 160, 2, device = device(type='cpu'), pin_memory = False)
            _to_copy_120: "f32[80]" = torch.ops.aten._to_copy.default(arange_27, dtype = torch.float32);  arange_27 = None
            div_13: "f32[80]" = torch.ops.aten.div.Tensor(_to_copy_120, 160);  _to_copy_120 = None
            pow_41: "f32[80]" = torch.ops.aten.pow.Scalar(30420.108888514722, div_13);  div_13 = None
            reciprocal_13: "f32[80]" = torch.ops.aten.reciprocal.default(pow_41);  pow_41 = None
            mul_171: "f32[80]" = torch.ops.aten.mul.Tensor(reciprocal_13, 1.0);  reciprocal_13 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:114 in forward, code: t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)
            arange_28: "f32[8192]" = torch.ops.aten.arange.default(8192, dtype = torch.float32, device = device(type='cpu'), pin_memory = False)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:115 in forward, code: freqs = torch.einsum('i,j->ij', t, self.inv_freq)
            einsum_26: "f32[8192, 80]" = torch.ops.aten.einsum.default('i,j->ij', [arange_28, mul_171]);  arange_28 = mul_171 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:117 in forward, code: emb = torch.cat((freqs, freqs), dim=-1).to(x.device)
            cat_39: "f32[8192, 160]" = torch.ops.aten.cat.default([einsum_26, einsum_26], -1);  einsum_26 = None
            _to_copy_121: "f32[8192, 160]" = torch.ops.aten._to_copy.default(cat_39, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  cat_39 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:121 in forward, code: self.cos_cached = self.mscale * emb.cos()[:, None, :].to(dtype)
            cos_13: "f32[8192, 160]" = torch.ops.aten.cos.default(_to_copy_121)
            slice_144: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(cos_13, 0, 0, 9223372036854775807);  cos_13 = None
            unsqueeze_34: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_144, 1);  slice_144 = None
            slice_145: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_34, 2, 0, 9223372036854775807);  unsqueeze_34 = None
            _to_copy_122: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_145, dtype = torch.float32);  slice_145 = None
            mul_172: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_122, 1.0);  _to_copy_122 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:122 in forward, code: self.sin_cached = self.mscale * emb.sin()[:, None, :].to(dtype)
            sin_13: "f32[8192, 160]" = torch.ops.aten.sin.default(_to_copy_121);  _to_copy_121 = None
            slice_146: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(sin_13, 0, 0, 9223372036854775807);  sin_13 = None
            unsqueeze_35: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_146, 1);  slice_146 = None
            slice_147: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_35, 2, 0, 9223372036854775807);  unsqueeze_35 = None
            _to_copy_123: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_147, dtype = torch.float32);  slice_147 = None
            mul_173: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_123, 1.0);  _to_copy_123 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:123 in forward, code: return self.cos_cached[:seq_len, ...], self.sin_cached[:seq_len, ...]
            alias_26: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_172);  mul_172 = None
            alias_27: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_173);  mul_173 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:133 in apply_rotary_pos_emb_torch, code: cos, sin = cos[offset:q.shape[0] + offset, ...], sin[offset:q.shape[0] + offset, ...]
            slice_148: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_26, 0, 0, 6);  alias_26 = None
            slice_149: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_27, 0, 0, 6);  alias_27 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_174: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_158, slice_148)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_150: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_158, 2, 0, 80)
            slice_151: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_158, 2, 80, 9223372036854775807);  view_158 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_26: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_151);  slice_151 = None
            cat_40: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_26, slice_150], 2);  neg_26 = slice_150 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_175: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_40, slice_149);  cat_40 = None
            add_79: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_174, mul_175);  mul_174 = mul_175 = None
            mul_176: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_159, slice_148);  slice_148 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_152: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_159, 2, 0, 80)
            slice_153: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_159, 2, 80, 9223372036854775807);  view_159 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_27: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_153);  slice_153 = None
            cat_41: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_27, slice_152], 2);  neg_27 = slice_152 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_177: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_41, slice_149);  cat_41 = slice_149 = None
            add_80: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_176, mul_177);  mul_176 = mul_177 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:448 in forward, code: query_layer = query_layer.reshape((s_query, bz, head, dim))
            view_160: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_79, [6, 1, 32, 160]);  add_79 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:449 in forward, code: key_layer = key_layer.reshape((s_key, bz, head, dim))
            view_161: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_80, [6, 1, 32, 160]);  add_80 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:458 in forward, code: query_layer = query_layer.reshape(s_query, bz * self.num_heads, dim)
            view_162: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_160, [6, 32, 160]);  view_160 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:460 in forward, code: key_layer = key_layer.reshape(s_key, bz * self.num_heads, dim)
            view_163: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_161, [6, 32, 160]);  view_161 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            transpose_66: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_162, 0, 1);  view_162 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:462 in forward, code: key_layer.transpose(0, 1).transpose(1, 2))
            transpose_67: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_163, 0, 1);  view_163 = None
            transpose_68: "f32[32, 160, 6]" = torch.ops.aten.transpose.int(transpose_67, 1, 2);  transpose_67 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            einsum_27: "f32[32, 6, 6]" = torch.ops.aten.einsum.default('bik,bkj->bij', [transpose_66, transpose_68]);  transpose_66 = transpose_68 = None
            mul_178: "f32[32, 6, 6]" = torch.ops.aten.mul.Tensor(einsum_27, 0.07905694150420949);  einsum_27 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:464 in forward, code: attention_scores = matmul_result.view(bz, self.num_heads, s_query, s_key)
            view_164: "f32[1, 32, 6, 6]" = torch.ops.aten.view.default(mul_178, [1, 32, 6, 6]);  mul_178 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:469 in forward, code: attn_weights = torch.masked_fill(attention_scores, attention_mask, torch.finfo(attention_scores.dtype).min)
            masked_fill_13: "f32[1, 32, 6, 6]" = torch.ops.aten.masked_fill.Scalar(view_164, or_1, -3.4028234663852886e+38);  view_164 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:470 in forward, code: attention_probs = F.softmax(attn_weights, dim=-1).to(input_dtype)  ##dtype = torch.float32
            softmax_13: "f32[1, 32, 6, 6]" = torch.ops.aten.softmax.int(masked_fill_13, -1);  masked_fill_13 = None
            _to_copy_124: "f32[1, 32, 6, 6]" = torch.ops.aten._to_copy.default(softmax_13, dtype = torch.float32);  softmax_13 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:471 in forward, code: attention_probs = self.attention_dropout(attention_probs)
            dropout_39: "f32[1, 32, 6, 6]" = torch.ops.aten.dropout.default(_to_copy_124, 0.0, False);  _to_copy_124 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:472 in forward, code: attention_probs_reshaped = attention_probs.view(bz * self.num_heads, s_query, s_key)
            view_165: "f32[32, 6, 6]" = torch.ops.aten.view.default(dropout_39, [32, 6, 6]);  dropout_39 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:474 in forward, code: value_layer = value_layer.reshape(s_key, bz * self.num_heads, dim)
            view_166: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_27, [6, 32, 160]);  getitem_27 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:475 in forward, code: context_layer = torch.bmm(attention_probs_reshaped, value_layer.transpose(0, 1))
            transpose_69: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_166, 0, 1);  view_166 = None
            bmm_13: "f32[32, 6, 160]" = torch.ops.aten.bmm.default(view_165, transpose_69);  view_165 = transpose_69 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:391 in _merge_heads, code: x = x.view(batch_size, self.num_heads, seq_length, self.head_dim)
            view_167: "f32[1, 32, 6, 160]" = torch.ops.aten.view.default(bmm_13, [1, 32, 6, 160]);  bmm_13 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:392 in _merge_heads, code: x = x.permute(0, 2, 1, 3)
            permute_13: "f32[1, 6, 32, 160]" = torch.ops.aten.permute.default(view_167, [0, 2, 1, 3]);  view_167 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:393 in _merge_heads, code: return x.reshape(batch_size, seq_length, self.num_heads * self.head_dim)
            clone_13: "f32[1, 6, 32, 160]" = torch.ops.aten.clone.default(permute_13, memory_format = torch.contiguous_format);  permute_13 = None
            _unsafe_view_13: "f32[1, 6, 5120]" = torch.ops.aten._unsafe_view.default(clone_13, [1, 6, 5120]);  clone_13 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:478 in forward, code: output_tensor = self.dense(context_layer)
            linear_80: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(_unsafe_view_13, p_h_13_self_attention_dense_weight, p_h_13_self_attention_dense_bias);  _unsafe_view_13 = p_h_13_self_attention_dense_weight = p_h_13_self_attention_dense_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_40: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_80, 0.0, False);  linear_80 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_81: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_77, dropout_40);  add_77 = dropout_40 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_125: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_81, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_42: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_125, 2)
            mean_27: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_42, [-1], True);  pow_42 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_82: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_27, 1e-05);  mean_27 = None
            rsqrt_27: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_82);  add_82 = None
            mul_179: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_125, rsqrt_27);  _to_copy_125 = rsqrt_27 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_126: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_179, dtype = torch.float32);  mul_179 = None
            mul_180: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_13_post_attention_layernorm_weight, _to_copy_126);  p_h_13_post_attention_layernorm_weight = _to_copy_126 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:499 in forward, code: intermediate_output = self.down_proj(F.silu(self.gate_proj(hidden_states)) * self.up_proj(hidden_states))
            linear_81: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_180, p_h_13_mlp_gate_proj_weight);  p_h_13_mlp_gate_proj_weight = None
            silu_13: "f32[1, 6, 12288]" = torch.ops.aten.silu.default(linear_81);  linear_81 = None
            linear_82: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_180, p_h_13_mlp_up_proj_weight);  mul_180 = p_h_13_mlp_up_proj_weight = None
            mul_181: "f32[1, 6, 12288]" = torch.ops.aten.mul.Tensor(silu_13, linear_82);  silu_13 = linear_82 = None
            linear_83: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(mul_181, p_h_13_mlp_down_proj_weight, p_h_13_mlp_down_proj_bias);  mul_181 = p_h_13_mlp_down_proj_weight = p_h_13_mlp_down_proj_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_41: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_83, 0.0, False);  linear_83 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_83: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_81, dropout_41);  add_81 = dropout_41 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_127: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_83, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_43: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_127, 2)
            mean_28: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_43, [-1], True);  pow_43 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_84: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_28, 1e-05);  mean_28 = None
            rsqrt_28: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_84);  add_84 = None
            mul_182: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_127, rsqrt_28);  _to_copy_127 = rsqrt_28 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_128: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_182, dtype = torch.float32);  mul_182 = None
            mul_183: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_14_input_layernorm_weight, _to_copy_128);  p_h_14_input_layernorm_weight = _to_copy_128 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:404 in forward, code: hidden_states = hidden_states.transpose(1, 0)
            transpose_70: "f32[6, 1, 5120]" = torch.ops.aten.transpose.int(mul_183, 1, 0);  mul_183 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:405 in forward, code: query_layer = self.query(hidden_states)
            linear_84: "f32[6, 1, 5120]" = torch.ops.aten.linear.default(transpose_70, p_h_14_self_attention_query_weight);  p_h_14_self_attention_query_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:409 in forward, code: query_layer = query_layer.view(*new_tensor_shape)
            view_168: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(linear_84, [6, 1, 32, 160]);  linear_84 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:411 in forward, code: mixed_kv_layer = self.key_value(hidden_states)
            linear_85: "f32[6, 1, 10240]" = torch.ops.aten.linear.default(transpose_70, p_h_14_self_attention_key_value_weight);  transpose_70 = p_h_14_self_attention_key_value_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:415 in forward, code: mixed_kv_layer = mixed_kv_layer.view(*new_tensor_shape)
            view_169: "f32[6, 1, 32, 320]" = torch.ops.aten.view.default(linear_85, [6, 1, 32, 320]);  linear_85 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:381 in split_tensor_along_last_dim, code: tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)
            split_14 = torch.ops.aten.split.Tensor(view_169, 160, 3);  view_169 = None
            getitem_28: "f32[6, 1, 32, 160]" = split_14[0]
            getitem_29: "f32[6, 1, 32, 160]" = split_14[1];  split_14 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:423 in forward, code: query_layer = query_layer.view(output_size[2], output_size[0] * output_size[1], -1)
            view_170: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_168, [6, 32, -1]);  view_168 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:424 in forward, code: key_layer = key_layer.view(output_size[3], output_size[0] * output_size[1], -1)
            view_171: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_28, [6, 32, -1]);  getitem_28 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:112 in forward, code: self.inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, device=x.device).float() / self.dim))
            arange_29: "i64[80]" = torch.ops.aten.arange.start_step(0, 160, 2, device = device(type='cpu'), pin_memory = False)
            _to_copy_129: "f32[80]" = torch.ops.aten._to_copy.default(arange_29, dtype = torch.float32);  arange_29 = None
            div_14: "f32[80]" = torch.ops.aten.div.Tensor(_to_copy_129, 160);  _to_copy_129 = None
            pow_44: "f32[80]" = torch.ops.aten.pow.Scalar(30420.108888514722, div_14);  div_14 = None
            reciprocal_14: "f32[80]" = torch.ops.aten.reciprocal.default(pow_44);  pow_44 = None
            mul_184: "f32[80]" = torch.ops.aten.mul.Tensor(reciprocal_14, 1.0);  reciprocal_14 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:114 in forward, code: t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)
            arange_30: "f32[8192]" = torch.ops.aten.arange.default(8192, dtype = torch.float32, device = device(type='cpu'), pin_memory = False)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:115 in forward, code: freqs = torch.einsum('i,j->ij', t, self.inv_freq)
            einsum_28: "f32[8192, 80]" = torch.ops.aten.einsum.default('i,j->ij', [arange_30, mul_184]);  arange_30 = mul_184 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:117 in forward, code: emb = torch.cat((freqs, freqs), dim=-1).to(x.device)
            cat_42: "f32[8192, 160]" = torch.ops.aten.cat.default([einsum_28, einsum_28], -1);  einsum_28 = None
            _to_copy_130: "f32[8192, 160]" = torch.ops.aten._to_copy.default(cat_42, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  cat_42 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:121 in forward, code: self.cos_cached = self.mscale * emb.cos()[:, None, :].to(dtype)
            cos_14: "f32[8192, 160]" = torch.ops.aten.cos.default(_to_copy_130)
            slice_154: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(cos_14, 0, 0, 9223372036854775807);  cos_14 = None
            unsqueeze_36: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_154, 1);  slice_154 = None
            slice_155: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_36, 2, 0, 9223372036854775807);  unsqueeze_36 = None
            _to_copy_131: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_155, dtype = torch.float32);  slice_155 = None
            mul_185: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_131, 1.0);  _to_copy_131 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:122 in forward, code: self.sin_cached = self.mscale * emb.sin()[:, None, :].to(dtype)
            sin_14: "f32[8192, 160]" = torch.ops.aten.sin.default(_to_copy_130);  _to_copy_130 = None
            slice_156: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(sin_14, 0, 0, 9223372036854775807);  sin_14 = None
            unsqueeze_37: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_156, 1);  slice_156 = None
            slice_157: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_37, 2, 0, 9223372036854775807);  unsqueeze_37 = None
            _to_copy_132: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_157, dtype = torch.float32);  slice_157 = None
            mul_186: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_132, 1.0);  _to_copy_132 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:123 in forward, code: return self.cos_cached[:seq_len, ...], self.sin_cached[:seq_len, ...]
            alias_28: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_185);  mul_185 = None
            alias_29: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_186);  mul_186 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:133 in apply_rotary_pos_emb_torch, code: cos, sin = cos[offset:q.shape[0] + offset, ...], sin[offset:q.shape[0] + offset, ...]
            slice_158: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_28, 0, 0, 6);  alias_28 = None
            slice_159: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_29, 0, 0, 6);  alias_29 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_187: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_170, slice_158)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_160: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_170, 2, 0, 80)
            slice_161: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_170, 2, 80, 9223372036854775807);  view_170 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_28: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_161);  slice_161 = None
            cat_43: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_28, slice_160], 2);  neg_28 = slice_160 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_188: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_43, slice_159);  cat_43 = None
            add_85: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_187, mul_188);  mul_187 = mul_188 = None
            mul_189: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_171, slice_158);  slice_158 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_162: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_171, 2, 0, 80)
            slice_163: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_171, 2, 80, 9223372036854775807);  view_171 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_29: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_163);  slice_163 = None
            cat_44: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_29, slice_162], 2);  neg_29 = slice_162 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_190: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_44, slice_159);  cat_44 = slice_159 = None
            add_86: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_189, mul_190);  mul_189 = mul_190 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:448 in forward, code: query_layer = query_layer.reshape((s_query, bz, head, dim))
            view_172: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_85, [6, 1, 32, 160]);  add_85 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:449 in forward, code: key_layer = key_layer.reshape((s_key, bz, head, dim))
            view_173: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_86, [6, 1, 32, 160]);  add_86 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:458 in forward, code: query_layer = query_layer.reshape(s_query, bz * self.num_heads, dim)
            view_174: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_172, [6, 32, 160]);  view_172 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:460 in forward, code: key_layer = key_layer.reshape(s_key, bz * self.num_heads, dim)
            view_175: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_173, [6, 32, 160]);  view_173 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            transpose_71: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_174, 0, 1);  view_174 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:462 in forward, code: key_layer.transpose(0, 1).transpose(1, 2))
            transpose_72: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_175, 0, 1);  view_175 = None
            transpose_73: "f32[32, 160, 6]" = torch.ops.aten.transpose.int(transpose_72, 1, 2);  transpose_72 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            einsum_29: "f32[32, 6, 6]" = torch.ops.aten.einsum.default('bik,bkj->bij', [transpose_71, transpose_73]);  transpose_71 = transpose_73 = None
            mul_191: "f32[32, 6, 6]" = torch.ops.aten.mul.Tensor(einsum_29, 0.07905694150420949);  einsum_29 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:464 in forward, code: attention_scores = matmul_result.view(bz, self.num_heads, s_query, s_key)
            view_176: "f32[1, 32, 6, 6]" = torch.ops.aten.view.default(mul_191, [1, 32, 6, 6]);  mul_191 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:469 in forward, code: attn_weights = torch.masked_fill(attention_scores, attention_mask, torch.finfo(attention_scores.dtype).min)
            masked_fill_14: "f32[1, 32, 6, 6]" = torch.ops.aten.masked_fill.Scalar(view_176, or_1, -3.4028234663852886e+38);  view_176 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:470 in forward, code: attention_probs = F.softmax(attn_weights, dim=-1).to(input_dtype)  ##dtype = torch.float32
            softmax_14: "f32[1, 32, 6, 6]" = torch.ops.aten.softmax.int(masked_fill_14, -1);  masked_fill_14 = None
            _to_copy_133: "f32[1, 32, 6, 6]" = torch.ops.aten._to_copy.default(softmax_14, dtype = torch.float32);  softmax_14 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:471 in forward, code: attention_probs = self.attention_dropout(attention_probs)
            dropout_42: "f32[1, 32, 6, 6]" = torch.ops.aten.dropout.default(_to_copy_133, 0.0, False);  _to_copy_133 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:472 in forward, code: attention_probs_reshaped = attention_probs.view(bz * self.num_heads, s_query, s_key)
            view_177: "f32[32, 6, 6]" = torch.ops.aten.view.default(dropout_42, [32, 6, 6]);  dropout_42 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:474 in forward, code: value_layer = value_layer.reshape(s_key, bz * self.num_heads, dim)
            view_178: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_29, [6, 32, 160]);  getitem_29 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:475 in forward, code: context_layer = torch.bmm(attention_probs_reshaped, value_layer.transpose(0, 1))
            transpose_74: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_178, 0, 1);  view_178 = None
            bmm_14: "f32[32, 6, 160]" = torch.ops.aten.bmm.default(view_177, transpose_74);  view_177 = transpose_74 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:391 in _merge_heads, code: x = x.view(batch_size, self.num_heads, seq_length, self.head_dim)
            view_179: "f32[1, 32, 6, 160]" = torch.ops.aten.view.default(bmm_14, [1, 32, 6, 160]);  bmm_14 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:392 in _merge_heads, code: x = x.permute(0, 2, 1, 3)
            permute_14: "f32[1, 6, 32, 160]" = torch.ops.aten.permute.default(view_179, [0, 2, 1, 3]);  view_179 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:393 in _merge_heads, code: return x.reshape(batch_size, seq_length, self.num_heads * self.head_dim)
            clone_14: "f32[1, 6, 32, 160]" = torch.ops.aten.clone.default(permute_14, memory_format = torch.contiguous_format);  permute_14 = None
            _unsafe_view_14: "f32[1, 6, 5120]" = torch.ops.aten._unsafe_view.default(clone_14, [1, 6, 5120]);  clone_14 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:478 in forward, code: output_tensor = self.dense(context_layer)
            linear_86: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(_unsafe_view_14, p_h_14_self_attention_dense_weight, p_h_14_self_attention_dense_bias);  _unsafe_view_14 = p_h_14_self_attention_dense_weight = p_h_14_self_attention_dense_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_43: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_86, 0.0, False);  linear_86 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_87: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_83, dropout_43);  add_83 = dropout_43 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_134: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_87, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_45: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_134, 2)
            mean_29: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_45, [-1], True);  pow_45 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_88: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_29, 1e-05);  mean_29 = None
            rsqrt_29: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_88);  add_88 = None
            mul_192: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_134, rsqrt_29);  _to_copy_134 = rsqrt_29 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_135: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_192, dtype = torch.float32);  mul_192 = None
            mul_193: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_14_post_attention_layernorm_weight, _to_copy_135);  p_h_14_post_attention_layernorm_weight = _to_copy_135 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:499 in forward, code: intermediate_output = self.down_proj(F.silu(self.gate_proj(hidden_states)) * self.up_proj(hidden_states))
            linear_87: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_193, p_h_14_mlp_gate_proj_weight);  p_h_14_mlp_gate_proj_weight = None
            silu_14: "f32[1, 6, 12288]" = torch.ops.aten.silu.default(linear_87);  linear_87 = None
            linear_88: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_193, p_h_14_mlp_up_proj_weight);  mul_193 = p_h_14_mlp_up_proj_weight = None
            mul_194: "f32[1, 6, 12288]" = torch.ops.aten.mul.Tensor(silu_14, linear_88);  silu_14 = linear_88 = None
            linear_89: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(mul_194, p_h_14_mlp_down_proj_weight, p_h_14_mlp_down_proj_bias);  mul_194 = p_h_14_mlp_down_proj_weight = p_h_14_mlp_down_proj_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_44: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_89, 0.0, False);  linear_89 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_89: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_87, dropout_44);  add_87 = dropout_44 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_136: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_89, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_46: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_136, 2)
            mean_30: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_46, [-1], True);  pow_46 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_90: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_30, 1e-05);  mean_30 = None
            rsqrt_30: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_90);  add_90 = None
            mul_195: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_136, rsqrt_30);  _to_copy_136 = rsqrt_30 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_137: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_195, dtype = torch.float32);  mul_195 = None
            mul_196: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_15_input_layernorm_weight, _to_copy_137);  p_h_15_input_layernorm_weight = _to_copy_137 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:404 in forward, code: hidden_states = hidden_states.transpose(1, 0)
            transpose_75: "f32[6, 1, 5120]" = torch.ops.aten.transpose.int(mul_196, 1, 0);  mul_196 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:405 in forward, code: query_layer = self.query(hidden_states)
            linear_90: "f32[6, 1, 5120]" = torch.ops.aten.linear.default(transpose_75, p_h_15_self_attention_query_weight);  p_h_15_self_attention_query_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:409 in forward, code: query_layer = query_layer.view(*new_tensor_shape)
            view_180: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(linear_90, [6, 1, 32, 160]);  linear_90 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:411 in forward, code: mixed_kv_layer = self.key_value(hidden_states)
            linear_91: "f32[6, 1, 10240]" = torch.ops.aten.linear.default(transpose_75, p_h_15_self_attention_key_value_weight);  transpose_75 = p_h_15_self_attention_key_value_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:415 in forward, code: mixed_kv_layer = mixed_kv_layer.view(*new_tensor_shape)
            view_181: "f32[6, 1, 32, 320]" = torch.ops.aten.view.default(linear_91, [6, 1, 32, 320]);  linear_91 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:381 in split_tensor_along_last_dim, code: tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)
            split_15 = torch.ops.aten.split.Tensor(view_181, 160, 3);  view_181 = None
            getitem_30: "f32[6, 1, 32, 160]" = split_15[0]
            getitem_31: "f32[6, 1, 32, 160]" = split_15[1];  split_15 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:423 in forward, code: query_layer = query_layer.view(output_size[2], output_size[0] * output_size[1], -1)
            view_182: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_180, [6, 32, -1]);  view_180 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:424 in forward, code: key_layer = key_layer.view(output_size[3], output_size[0] * output_size[1], -1)
            view_183: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_30, [6, 32, -1]);  getitem_30 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:112 in forward, code: self.inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, device=x.device).float() / self.dim))
            arange_31: "i64[80]" = torch.ops.aten.arange.start_step(0, 160, 2, device = device(type='cpu'), pin_memory = False)
            _to_copy_138: "f32[80]" = torch.ops.aten._to_copy.default(arange_31, dtype = torch.float32);  arange_31 = None
            div_15: "f32[80]" = torch.ops.aten.div.Tensor(_to_copy_138, 160);  _to_copy_138 = None
            pow_47: "f32[80]" = torch.ops.aten.pow.Scalar(30420.108888514722, div_15);  div_15 = None
            reciprocal_15: "f32[80]" = torch.ops.aten.reciprocal.default(pow_47);  pow_47 = None
            mul_197: "f32[80]" = torch.ops.aten.mul.Tensor(reciprocal_15, 1.0);  reciprocal_15 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:114 in forward, code: t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)
            arange_32: "f32[8192]" = torch.ops.aten.arange.default(8192, dtype = torch.float32, device = device(type='cpu'), pin_memory = False)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:115 in forward, code: freqs = torch.einsum('i,j->ij', t, self.inv_freq)
            einsum_30: "f32[8192, 80]" = torch.ops.aten.einsum.default('i,j->ij', [arange_32, mul_197]);  arange_32 = mul_197 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:117 in forward, code: emb = torch.cat((freqs, freqs), dim=-1).to(x.device)
            cat_45: "f32[8192, 160]" = torch.ops.aten.cat.default([einsum_30, einsum_30], -1);  einsum_30 = None
            _to_copy_139: "f32[8192, 160]" = torch.ops.aten._to_copy.default(cat_45, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  cat_45 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:121 in forward, code: self.cos_cached = self.mscale * emb.cos()[:, None, :].to(dtype)
            cos_15: "f32[8192, 160]" = torch.ops.aten.cos.default(_to_copy_139)
            slice_164: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(cos_15, 0, 0, 9223372036854775807);  cos_15 = None
            unsqueeze_38: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_164, 1);  slice_164 = None
            slice_165: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_38, 2, 0, 9223372036854775807);  unsqueeze_38 = None
            _to_copy_140: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_165, dtype = torch.float32);  slice_165 = None
            mul_198: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_140, 1.0);  _to_copy_140 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:122 in forward, code: self.sin_cached = self.mscale * emb.sin()[:, None, :].to(dtype)
            sin_15: "f32[8192, 160]" = torch.ops.aten.sin.default(_to_copy_139);  _to_copy_139 = None
            slice_166: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(sin_15, 0, 0, 9223372036854775807);  sin_15 = None
            unsqueeze_39: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_166, 1);  slice_166 = None
            slice_167: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_39, 2, 0, 9223372036854775807);  unsqueeze_39 = None
            _to_copy_141: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_167, dtype = torch.float32);  slice_167 = None
            mul_199: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_141, 1.0);  _to_copy_141 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:123 in forward, code: return self.cos_cached[:seq_len, ...], self.sin_cached[:seq_len, ...]
            alias_30: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_198);  mul_198 = None
            alias_31: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_199);  mul_199 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:133 in apply_rotary_pos_emb_torch, code: cos, sin = cos[offset:q.shape[0] + offset, ...], sin[offset:q.shape[0] + offset, ...]
            slice_168: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_30, 0, 0, 6);  alias_30 = None
            slice_169: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_31, 0, 0, 6);  alias_31 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_200: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_182, slice_168)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_170: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_182, 2, 0, 80)
            slice_171: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_182, 2, 80, 9223372036854775807);  view_182 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_30: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_171);  slice_171 = None
            cat_46: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_30, slice_170], 2);  neg_30 = slice_170 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_201: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_46, slice_169);  cat_46 = None
            add_91: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_200, mul_201);  mul_200 = mul_201 = None
            mul_202: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_183, slice_168);  slice_168 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_172: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_183, 2, 0, 80)
            slice_173: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_183, 2, 80, 9223372036854775807);  view_183 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_31: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_173);  slice_173 = None
            cat_47: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_31, slice_172], 2);  neg_31 = slice_172 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_203: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_47, slice_169);  cat_47 = slice_169 = None
            add_92: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_202, mul_203);  mul_202 = mul_203 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:448 in forward, code: query_layer = query_layer.reshape((s_query, bz, head, dim))
            view_184: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_91, [6, 1, 32, 160]);  add_91 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:449 in forward, code: key_layer = key_layer.reshape((s_key, bz, head, dim))
            view_185: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_92, [6, 1, 32, 160]);  add_92 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:458 in forward, code: query_layer = query_layer.reshape(s_query, bz * self.num_heads, dim)
            view_186: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_184, [6, 32, 160]);  view_184 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:460 in forward, code: key_layer = key_layer.reshape(s_key, bz * self.num_heads, dim)
            view_187: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_185, [6, 32, 160]);  view_185 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            transpose_76: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_186, 0, 1);  view_186 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:462 in forward, code: key_layer.transpose(0, 1).transpose(1, 2))
            transpose_77: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_187, 0, 1);  view_187 = None
            transpose_78: "f32[32, 160, 6]" = torch.ops.aten.transpose.int(transpose_77, 1, 2);  transpose_77 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            einsum_31: "f32[32, 6, 6]" = torch.ops.aten.einsum.default('bik,bkj->bij', [transpose_76, transpose_78]);  transpose_76 = transpose_78 = None
            mul_204: "f32[32, 6, 6]" = torch.ops.aten.mul.Tensor(einsum_31, 0.07905694150420949);  einsum_31 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:464 in forward, code: attention_scores = matmul_result.view(bz, self.num_heads, s_query, s_key)
            view_188: "f32[1, 32, 6, 6]" = torch.ops.aten.view.default(mul_204, [1, 32, 6, 6]);  mul_204 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:469 in forward, code: attn_weights = torch.masked_fill(attention_scores, attention_mask, torch.finfo(attention_scores.dtype).min)
            masked_fill_15: "f32[1, 32, 6, 6]" = torch.ops.aten.masked_fill.Scalar(view_188, or_1, -3.4028234663852886e+38);  view_188 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:470 in forward, code: attention_probs = F.softmax(attn_weights, dim=-1).to(input_dtype)  ##dtype = torch.float32
            softmax_15: "f32[1, 32, 6, 6]" = torch.ops.aten.softmax.int(masked_fill_15, -1);  masked_fill_15 = None
            _to_copy_142: "f32[1, 32, 6, 6]" = torch.ops.aten._to_copy.default(softmax_15, dtype = torch.float32);  softmax_15 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:471 in forward, code: attention_probs = self.attention_dropout(attention_probs)
            dropout_45: "f32[1, 32, 6, 6]" = torch.ops.aten.dropout.default(_to_copy_142, 0.0, False);  _to_copy_142 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:472 in forward, code: attention_probs_reshaped = attention_probs.view(bz * self.num_heads, s_query, s_key)
            view_189: "f32[32, 6, 6]" = torch.ops.aten.view.default(dropout_45, [32, 6, 6]);  dropout_45 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:474 in forward, code: value_layer = value_layer.reshape(s_key, bz * self.num_heads, dim)
            view_190: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_31, [6, 32, 160]);  getitem_31 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:475 in forward, code: context_layer = torch.bmm(attention_probs_reshaped, value_layer.transpose(0, 1))
            transpose_79: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_190, 0, 1);  view_190 = None
            bmm_15: "f32[32, 6, 160]" = torch.ops.aten.bmm.default(view_189, transpose_79);  view_189 = transpose_79 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:391 in _merge_heads, code: x = x.view(batch_size, self.num_heads, seq_length, self.head_dim)
            view_191: "f32[1, 32, 6, 160]" = torch.ops.aten.view.default(bmm_15, [1, 32, 6, 160]);  bmm_15 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:392 in _merge_heads, code: x = x.permute(0, 2, 1, 3)
            permute_15: "f32[1, 6, 32, 160]" = torch.ops.aten.permute.default(view_191, [0, 2, 1, 3]);  view_191 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:393 in _merge_heads, code: return x.reshape(batch_size, seq_length, self.num_heads * self.head_dim)
            clone_15: "f32[1, 6, 32, 160]" = torch.ops.aten.clone.default(permute_15, memory_format = torch.contiguous_format);  permute_15 = None
            _unsafe_view_15: "f32[1, 6, 5120]" = torch.ops.aten._unsafe_view.default(clone_15, [1, 6, 5120]);  clone_15 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:478 in forward, code: output_tensor = self.dense(context_layer)
            linear_92: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(_unsafe_view_15, p_h_15_self_attention_dense_weight, p_h_15_self_attention_dense_bias);  _unsafe_view_15 = p_h_15_self_attention_dense_weight = p_h_15_self_attention_dense_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_46: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_92, 0.0, False);  linear_92 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_93: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_89, dropout_46);  add_89 = dropout_46 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_143: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_93, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_48: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_143, 2)
            mean_31: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_48, [-1], True);  pow_48 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_94: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_31, 1e-05);  mean_31 = None
            rsqrt_31: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_94);  add_94 = None
            mul_205: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_143, rsqrt_31);  _to_copy_143 = rsqrt_31 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_144: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_205, dtype = torch.float32);  mul_205 = None
            mul_206: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_15_post_attention_layernorm_weight, _to_copy_144);  p_h_15_post_attention_layernorm_weight = _to_copy_144 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:499 in forward, code: intermediate_output = self.down_proj(F.silu(self.gate_proj(hidden_states)) * self.up_proj(hidden_states))
            linear_93: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_206, p_h_15_mlp_gate_proj_weight);  p_h_15_mlp_gate_proj_weight = None
            silu_15: "f32[1, 6, 12288]" = torch.ops.aten.silu.default(linear_93);  linear_93 = None
            linear_94: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_206, p_h_15_mlp_up_proj_weight);  mul_206 = p_h_15_mlp_up_proj_weight = None
            mul_207: "f32[1, 6, 12288]" = torch.ops.aten.mul.Tensor(silu_15, linear_94);  silu_15 = linear_94 = None
            linear_95: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(mul_207, p_h_15_mlp_down_proj_weight, p_h_15_mlp_down_proj_bias);  mul_207 = p_h_15_mlp_down_proj_weight = p_h_15_mlp_down_proj_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_47: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_95, 0.0, False);  linear_95 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_95: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_93, dropout_47);  add_93 = dropout_47 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_145: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_95, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_49: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_145, 2)
            mean_32: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_49, [-1], True);  pow_49 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_96: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_32, 1e-05);  mean_32 = None
            rsqrt_32: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_96);  add_96 = None
            mul_208: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_145, rsqrt_32);  _to_copy_145 = rsqrt_32 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_146: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_208, dtype = torch.float32);  mul_208 = None
            mul_209: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_16_input_layernorm_weight, _to_copy_146);  p_h_16_input_layernorm_weight = _to_copy_146 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:404 in forward, code: hidden_states = hidden_states.transpose(1, 0)
            transpose_80: "f32[6, 1, 5120]" = torch.ops.aten.transpose.int(mul_209, 1, 0);  mul_209 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:405 in forward, code: query_layer = self.query(hidden_states)
            linear_96: "f32[6, 1, 5120]" = torch.ops.aten.linear.default(transpose_80, p_h_16_self_attention_query_weight);  p_h_16_self_attention_query_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:409 in forward, code: query_layer = query_layer.view(*new_tensor_shape)
            view_192: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(linear_96, [6, 1, 32, 160]);  linear_96 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:411 in forward, code: mixed_kv_layer = self.key_value(hidden_states)
            linear_97: "f32[6, 1, 10240]" = torch.ops.aten.linear.default(transpose_80, p_h_16_self_attention_key_value_weight);  transpose_80 = p_h_16_self_attention_key_value_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:415 in forward, code: mixed_kv_layer = mixed_kv_layer.view(*new_tensor_shape)
            view_193: "f32[6, 1, 32, 320]" = torch.ops.aten.view.default(linear_97, [6, 1, 32, 320]);  linear_97 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:381 in split_tensor_along_last_dim, code: tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)
            split_16 = torch.ops.aten.split.Tensor(view_193, 160, 3);  view_193 = None
            getitem_32: "f32[6, 1, 32, 160]" = split_16[0]
            getitem_33: "f32[6, 1, 32, 160]" = split_16[1];  split_16 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:423 in forward, code: query_layer = query_layer.view(output_size[2], output_size[0] * output_size[1], -1)
            view_194: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_192, [6, 32, -1]);  view_192 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:424 in forward, code: key_layer = key_layer.view(output_size[3], output_size[0] * output_size[1], -1)
            view_195: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_32, [6, 32, -1]);  getitem_32 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:112 in forward, code: self.inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, device=x.device).float() / self.dim))
            arange_33: "i64[80]" = torch.ops.aten.arange.start_step(0, 160, 2, device = device(type='cpu'), pin_memory = False)
            _to_copy_147: "f32[80]" = torch.ops.aten._to_copy.default(arange_33, dtype = torch.float32);  arange_33 = None
            div_16: "f32[80]" = torch.ops.aten.div.Tensor(_to_copy_147, 160);  _to_copy_147 = None
            pow_50: "f32[80]" = torch.ops.aten.pow.Scalar(30420.108888514722, div_16);  div_16 = None
            reciprocal_16: "f32[80]" = torch.ops.aten.reciprocal.default(pow_50);  pow_50 = None
            mul_210: "f32[80]" = torch.ops.aten.mul.Tensor(reciprocal_16, 1.0);  reciprocal_16 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:114 in forward, code: t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)
            arange_34: "f32[8192]" = torch.ops.aten.arange.default(8192, dtype = torch.float32, device = device(type='cpu'), pin_memory = False)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:115 in forward, code: freqs = torch.einsum('i,j->ij', t, self.inv_freq)
            einsum_32: "f32[8192, 80]" = torch.ops.aten.einsum.default('i,j->ij', [arange_34, mul_210]);  arange_34 = mul_210 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:117 in forward, code: emb = torch.cat((freqs, freqs), dim=-1).to(x.device)
            cat_48: "f32[8192, 160]" = torch.ops.aten.cat.default([einsum_32, einsum_32], -1);  einsum_32 = None
            _to_copy_148: "f32[8192, 160]" = torch.ops.aten._to_copy.default(cat_48, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  cat_48 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:121 in forward, code: self.cos_cached = self.mscale * emb.cos()[:, None, :].to(dtype)
            cos_16: "f32[8192, 160]" = torch.ops.aten.cos.default(_to_copy_148)
            slice_174: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(cos_16, 0, 0, 9223372036854775807);  cos_16 = None
            unsqueeze_40: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_174, 1);  slice_174 = None
            slice_175: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_40, 2, 0, 9223372036854775807);  unsqueeze_40 = None
            _to_copy_149: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_175, dtype = torch.float32);  slice_175 = None
            mul_211: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_149, 1.0);  _to_copy_149 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:122 in forward, code: self.sin_cached = self.mscale * emb.sin()[:, None, :].to(dtype)
            sin_16: "f32[8192, 160]" = torch.ops.aten.sin.default(_to_copy_148);  _to_copy_148 = None
            slice_176: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(sin_16, 0, 0, 9223372036854775807);  sin_16 = None
            unsqueeze_41: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_176, 1);  slice_176 = None
            slice_177: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_41, 2, 0, 9223372036854775807);  unsqueeze_41 = None
            _to_copy_150: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_177, dtype = torch.float32);  slice_177 = None
            mul_212: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_150, 1.0);  _to_copy_150 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:123 in forward, code: return self.cos_cached[:seq_len, ...], self.sin_cached[:seq_len, ...]
            alias_32: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_211);  mul_211 = None
            alias_33: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_212);  mul_212 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:133 in apply_rotary_pos_emb_torch, code: cos, sin = cos[offset:q.shape[0] + offset, ...], sin[offset:q.shape[0] + offset, ...]
            slice_178: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_32, 0, 0, 6);  alias_32 = None
            slice_179: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_33, 0, 0, 6);  alias_33 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_213: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_194, slice_178)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_180: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_194, 2, 0, 80)
            slice_181: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_194, 2, 80, 9223372036854775807);  view_194 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_32: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_181);  slice_181 = None
            cat_49: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_32, slice_180], 2);  neg_32 = slice_180 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_214: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_49, slice_179);  cat_49 = None
            add_97: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_213, mul_214);  mul_213 = mul_214 = None
            mul_215: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_195, slice_178);  slice_178 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_182: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_195, 2, 0, 80)
            slice_183: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_195, 2, 80, 9223372036854775807);  view_195 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_33: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_183);  slice_183 = None
            cat_50: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_33, slice_182], 2);  neg_33 = slice_182 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_216: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_50, slice_179);  cat_50 = slice_179 = None
            add_98: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_215, mul_216);  mul_215 = mul_216 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:448 in forward, code: query_layer = query_layer.reshape((s_query, bz, head, dim))
            view_196: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_97, [6, 1, 32, 160]);  add_97 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:449 in forward, code: key_layer = key_layer.reshape((s_key, bz, head, dim))
            view_197: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_98, [6, 1, 32, 160]);  add_98 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:458 in forward, code: query_layer = query_layer.reshape(s_query, bz * self.num_heads, dim)
            view_198: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_196, [6, 32, 160]);  view_196 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:460 in forward, code: key_layer = key_layer.reshape(s_key, bz * self.num_heads, dim)
            view_199: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_197, [6, 32, 160]);  view_197 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            transpose_81: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_198, 0, 1);  view_198 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:462 in forward, code: key_layer.transpose(0, 1).transpose(1, 2))
            transpose_82: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_199, 0, 1);  view_199 = None
            transpose_83: "f32[32, 160, 6]" = torch.ops.aten.transpose.int(transpose_82, 1, 2);  transpose_82 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            einsum_33: "f32[32, 6, 6]" = torch.ops.aten.einsum.default('bik,bkj->bij', [transpose_81, transpose_83]);  transpose_81 = transpose_83 = None
            mul_217: "f32[32, 6, 6]" = torch.ops.aten.mul.Tensor(einsum_33, 0.07905694150420949);  einsum_33 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:464 in forward, code: attention_scores = matmul_result.view(bz, self.num_heads, s_query, s_key)
            view_200: "f32[1, 32, 6, 6]" = torch.ops.aten.view.default(mul_217, [1, 32, 6, 6]);  mul_217 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:469 in forward, code: attn_weights = torch.masked_fill(attention_scores, attention_mask, torch.finfo(attention_scores.dtype).min)
            masked_fill_16: "f32[1, 32, 6, 6]" = torch.ops.aten.masked_fill.Scalar(view_200, or_1, -3.4028234663852886e+38);  view_200 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:470 in forward, code: attention_probs = F.softmax(attn_weights, dim=-1).to(input_dtype)  ##dtype = torch.float32
            softmax_16: "f32[1, 32, 6, 6]" = torch.ops.aten.softmax.int(masked_fill_16, -1);  masked_fill_16 = None
            _to_copy_151: "f32[1, 32, 6, 6]" = torch.ops.aten._to_copy.default(softmax_16, dtype = torch.float32);  softmax_16 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:471 in forward, code: attention_probs = self.attention_dropout(attention_probs)
            dropout_48: "f32[1, 32, 6, 6]" = torch.ops.aten.dropout.default(_to_copy_151, 0.0, False);  _to_copy_151 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:472 in forward, code: attention_probs_reshaped = attention_probs.view(bz * self.num_heads, s_query, s_key)
            view_201: "f32[32, 6, 6]" = torch.ops.aten.view.default(dropout_48, [32, 6, 6]);  dropout_48 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:474 in forward, code: value_layer = value_layer.reshape(s_key, bz * self.num_heads, dim)
            view_202: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_33, [6, 32, 160]);  getitem_33 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:475 in forward, code: context_layer = torch.bmm(attention_probs_reshaped, value_layer.transpose(0, 1))
            transpose_84: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_202, 0, 1);  view_202 = None
            bmm_16: "f32[32, 6, 160]" = torch.ops.aten.bmm.default(view_201, transpose_84);  view_201 = transpose_84 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:391 in _merge_heads, code: x = x.view(batch_size, self.num_heads, seq_length, self.head_dim)
            view_203: "f32[1, 32, 6, 160]" = torch.ops.aten.view.default(bmm_16, [1, 32, 6, 160]);  bmm_16 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:392 in _merge_heads, code: x = x.permute(0, 2, 1, 3)
            permute_16: "f32[1, 6, 32, 160]" = torch.ops.aten.permute.default(view_203, [0, 2, 1, 3]);  view_203 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:393 in _merge_heads, code: return x.reshape(batch_size, seq_length, self.num_heads * self.head_dim)
            clone_16: "f32[1, 6, 32, 160]" = torch.ops.aten.clone.default(permute_16, memory_format = torch.contiguous_format);  permute_16 = None
            _unsafe_view_16: "f32[1, 6, 5120]" = torch.ops.aten._unsafe_view.default(clone_16, [1, 6, 5120]);  clone_16 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:478 in forward, code: output_tensor = self.dense(context_layer)
            linear_98: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(_unsafe_view_16, p_h_16_self_attention_dense_weight, p_h_16_self_attention_dense_bias);  _unsafe_view_16 = p_h_16_self_attention_dense_weight = p_h_16_self_attention_dense_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_49: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_98, 0.0, False);  linear_98 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_99: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_95, dropout_49);  add_95 = dropout_49 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_152: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_99, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_51: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_152, 2)
            mean_33: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_51, [-1], True);  pow_51 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_100: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_33, 1e-05);  mean_33 = None
            rsqrt_33: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_100);  add_100 = None
            mul_218: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_152, rsqrt_33);  _to_copy_152 = rsqrt_33 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_153: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_218, dtype = torch.float32);  mul_218 = None
            mul_219: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_16_post_attention_layernorm_weight, _to_copy_153);  p_h_16_post_attention_layernorm_weight = _to_copy_153 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:499 in forward, code: intermediate_output = self.down_proj(F.silu(self.gate_proj(hidden_states)) * self.up_proj(hidden_states))
            linear_99: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_219, p_h_16_mlp_gate_proj_weight);  p_h_16_mlp_gate_proj_weight = None
            silu_16: "f32[1, 6, 12288]" = torch.ops.aten.silu.default(linear_99);  linear_99 = None
            linear_100: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_219, p_h_16_mlp_up_proj_weight);  mul_219 = p_h_16_mlp_up_proj_weight = None
            mul_220: "f32[1, 6, 12288]" = torch.ops.aten.mul.Tensor(silu_16, linear_100);  silu_16 = linear_100 = None
            linear_101: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(mul_220, p_h_16_mlp_down_proj_weight, p_h_16_mlp_down_proj_bias);  mul_220 = p_h_16_mlp_down_proj_weight = p_h_16_mlp_down_proj_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_50: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_101, 0.0, False);  linear_101 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_101: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_99, dropout_50);  add_99 = dropout_50 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_154: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_101, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_52: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_154, 2)
            mean_34: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_52, [-1], True);  pow_52 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_102: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_34, 1e-05);  mean_34 = None
            rsqrt_34: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_102);  add_102 = None
            mul_221: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_154, rsqrt_34);  _to_copy_154 = rsqrt_34 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_155: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_221, dtype = torch.float32);  mul_221 = None
            mul_222: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_17_input_layernorm_weight, _to_copy_155);  p_h_17_input_layernorm_weight = _to_copy_155 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:404 in forward, code: hidden_states = hidden_states.transpose(1, 0)
            transpose_85: "f32[6, 1, 5120]" = torch.ops.aten.transpose.int(mul_222, 1, 0);  mul_222 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:405 in forward, code: query_layer = self.query(hidden_states)
            linear_102: "f32[6, 1, 5120]" = torch.ops.aten.linear.default(transpose_85, p_h_17_self_attention_query_weight);  p_h_17_self_attention_query_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:409 in forward, code: query_layer = query_layer.view(*new_tensor_shape)
            view_204: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(linear_102, [6, 1, 32, 160]);  linear_102 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:411 in forward, code: mixed_kv_layer = self.key_value(hidden_states)
            linear_103: "f32[6, 1, 10240]" = torch.ops.aten.linear.default(transpose_85, p_h_17_self_attention_key_value_weight);  transpose_85 = p_h_17_self_attention_key_value_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:415 in forward, code: mixed_kv_layer = mixed_kv_layer.view(*new_tensor_shape)
            view_205: "f32[6, 1, 32, 320]" = torch.ops.aten.view.default(linear_103, [6, 1, 32, 320]);  linear_103 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:381 in split_tensor_along_last_dim, code: tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)
            split_17 = torch.ops.aten.split.Tensor(view_205, 160, 3);  view_205 = None
            getitem_34: "f32[6, 1, 32, 160]" = split_17[0]
            getitem_35: "f32[6, 1, 32, 160]" = split_17[1];  split_17 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:423 in forward, code: query_layer = query_layer.view(output_size[2], output_size[0] * output_size[1], -1)
            view_206: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_204, [6, 32, -1]);  view_204 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:424 in forward, code: key_layer = key_layer.view(output_size[3], output_size[0] * output_size[1], -1)
            view_207: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_34, [6, 32, -1]);  getitem_34 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:112 in forward, code: self.inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, device=x.device).float() / self.dim))
            arange_35: "i64[80]" = torch.ops.aten.arange.start_step(0, 160, 2, device = device(type='cpu'), pin_memory = False)
            _to_copy_156: "f32[80]" = torch.ops.aten._to_copy.default(arange_35, dtype = torch.float32);  arange_35 = None
            div_17: "f32[80]" = torch.ops.aten.div.Tensor(_to_copy_156, 160);  _to_copy_156 = None
            pow_53: "f32[80]" = torch.ops.aten.pow.Scalar(30420.108888514722, div_17);  div_17 = None
            reciprocal_17: "f32[80]" = torch.ops.aten.reciprocal.default(pow_53);  pow_53 = None
            mul_223: "f32[80]" = torch.ops.aten.mul.Tensor(reciprocal_17, 1.0);  reciprocal_17 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:114 in forward, code: t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)
            arange_36: "f32[8192]" = torch.ops.aten.arange.default(8192, dtype = torch.float32, device = device(type='cpu'), pin_memory = False)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:115 in forward, code: freqs = torch.einsum('i,j->ij', t, self.inv_freq)
            einsum_34: "f32[8192, 80]" = torch.ops.aten.einsum.default('i,j->ij', [arange_36, mul_223]);  arange_36 = mul_223 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:117 in forward, code: emb = torch.cat((freqs, freqs), dim=-1).to(x.device)
            cat_51: "f32[8192, 160]" = torch.ops.aten.cat.default([einsum_34, einsum_34], -1);  einsum_34 = None
            _to_copy_157: "f32[8192, 160]" = torch.ops.aten._to_copy.default(cat_51, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  cat_51 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:121 in forward, code: self.cos_cached = self.mscale * emb.cos()[:, None, :].to(dtype)
            cos_17: "f32[8192, 160]" = torch.ops.aten.cos.default(_to_copy_157)
            slice_184: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(cos_17, 0, 0, 9223372036854775807);  cos_17 = None
            unsqueeze_42: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_184, 1);  slice_184 = None
            slice_185: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_42, 2, 0, 9223372036854775807);  unsqueeze_42 = None
            _to_copy_158: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_185, dtype = torch.float32);  slice_185 = None
            mul_224: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_158, 1.0);  _to_copy_158 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:122 in forward, code: self.sin_cached = self.mscale * emb.sin()[:, None, :].to(dtype)
            sin_17: "f32[8192, 160]" = torch.ops.aten.sin.default(_to_copy_157);  _to_copy_157 = None
            slice_186: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(sin_17, 0, 0, 9223372036854775807);  sin_17 = None
            unsqueeze_43: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_186, 1);  slice_186 = None
            slice_187: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_43, 2, 0, 9223372036854775807);  unsqueeze_43 = None
            _to_copy_159: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_187, dtype = torch.float32);  slice_187 = None
            mul_225: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_159, 1.0);  _to_copy_159 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:123 in forward, code: return self.cos_cached[:seq_len, ...], self.sin_cached[:seq_len, ...]
            alias_34: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_224);  mul_224 = None
            alias_35: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_225);  mul_225 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:133 in apply_rotary_pos_emb_torch, code: cos, sin = cos[offset:q.shape[0] + offset, ...], sin[offset:q.shape[0] + offset, ...]
            slice_188: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_34, 0, 0, 6);  alias_34 = None
            slice_189: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_35, 0, 0, 6);  alias_35 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_226: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_206, slice_188)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_190: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_206, 2, 0, 80)
            slice_191: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_206, 2, 80, 9223372036854775807);  view_206 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_34: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_191);  slice_191 = None
            cat_52: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_34, slice_190], 2);  neg_34 = slice_190 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_227: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_52, slice_189);  cat_52 = None
            add_103: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_226, mul_227);  mul_226 = mul_227 = None
            mul_228: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_207, slice_188);  slice_188 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_192: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_207, 2, 0, 80)
            slice_193: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_207, 2, 80, 9223372036854775807);  view_207 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_35: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_193);  slice_193 = None
            cat_53: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_35, slice_192], 2);  neg_35 = slice_192 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_229: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_53, slice_189);  cat_53 = slice_189 = None
            add_104: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_228, mul_229);  mul_228 = mul_229 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:448 in forward, code: query_layer = query_layer.reshape((s_query, bz, head, dim))
            view_208: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_103, [6, 1, 32, 160]);  add_103 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:449 in forward, code: key_layer = key_layer.reshape((s_key, bz, head, dim))
            view_209: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_104, [6, 1, 32, 160]);  add_104 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:458 in forward, code: query_layer = query_layer.reshape(s_query, bz * self.num_heads, dim)
            view_210: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_208, [6, 32, 160]);  view_208 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:460 in forward, code: key_layer = key_layer.reshape(s_key, bz * self.num_heads, dim)
            view_211: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_209, [6, 32, 160]);  view_209 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            transpose_86: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_210, 0, 1);  view_210 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:462 in forward, code: key_layer.transpose(0, 1).transpose(1, 2))
            transpose_87: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_211, 0, 1);  view_211 = None
            transpose_88: "f32[32, 160, 6]" = torch.ops.aten.transpose.int(transpose_87, 1, 2);  transpose_87 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            einsum_35: "f32[32, 6, 6]" = torch.ops.aten.einsum.default('bik,bkj->bij', [transpose_86, transpose_88]);  transpose_86 = transpose_88 = None
            mul_230: "f32[32, 6, 6]" = torch.ops.aten.mul.Tensor(einsum_35, 0.07905694150420949);  einsum_35 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:464 in forward, code: attention_scores = matmul_result.view(bz, self.num_heads, s_query, s_key)
            view_212: "f32[1, 32, 6, 6]" = torch.ops.aten.view.default(mul_230, [1, 32, 6, 6]);  mul_230 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:469 in forward, code: attn_weights = torch.masked_fill(attention_scores, attention_mask, torch.finfo(attention_scores.dtype).min)
            masked_fill_17: "f32[1, 32, 6, 6]" = torch.ops.aten.masked_fill.Scalar(view_212, or_1, -3.4028234663852886e+38);  view_212 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:470 in forward, code: attention_probs = F.softmax(attn_weights, dim=-1).to(input_dtype)  ##dtype = torch.float32
            softmax_17: "f32[1, 32, 6, 6]" = torch.ops.aten.softmax.int(masked_fill_17, -1);  masked_fill_17 = None
            _to_copy_160: "f32[1, 32, 6, 6]" = torch.ops.aten._to_copy.default(softmax_17, dtype = torch.float32);  softmax_17 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:471 in forward, code: attention_probs = self.attention_dropout(attention_probs)
            dropout_51: "f32[1, 32, 6, 6]" = torch.ops.aten.dropout.default(_to_copy_160, 0.0, False);  _to_copy_160 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:472 in forward, code: attention_probs_reshaped = attention_probs.view(bz * self.num_heads, s_query, s_key)
            view_213: "f32[32, 6, 6]" = torch.ops.aten.view.default(dropout_51, [32, 6, 6]);  dropout_51 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:474 in forward, code: value_layer = value_layer.reshape(s_key, bz * self.num_heads, dim)
            view_214: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_35, [6, 32, 160]);  getitem_35 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:475 in forward, code: context_layer = torch.bmm(attention_probs_reshaped, value_layer.transpose(0, 1))
            transpose_89: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_214, 0, 1);  view_214 = None
            bmm_17: "f32[32, 6, 160]" = torch.ops.aten.bmm.default(view_213, transpose_89);  view_213 = transpose_89 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:391 in _merge_heads, code: x = x.view(batch_size, self.num_heads, seq_length, self.head_dim)
            view_215: "f32[1, 32, 6, 160]" = torch.ops.aten.view.default(bmm_17, [1, 32, 6, 160]);  bmm_17 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:392 in _merge_heads, code: x = x.permute(0, 2, 1, 3)
            permute_17: "f32[1, 6, 32, 160]" = torch.ops.aten.permute.default(view_215, [0, 2, 1, 3]);  view_215 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:393 in _merge_heads, code: return x.reshape(batch_size, seq_length, self.num_heads * self.head_dim)
            clone_17: "f32[1, 6, 32, 160]" = torch.ops.aten.clone.default(permute_17, memory_format = torch.contiguous_format);  permute_17 = None
            _unsafe_view_17: "f32[1, 6, 5120]" = torch.ops.aten._unsafe_view.default(clone_17, [1, 6, 5120]);  clone_17 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:478 in forward, code: output_tensor = self.dense(context_layer)
            linear_104: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(_unsafe_view_17, p_h_17_self_attention_dense_weight, p_h_17_self_attention_dense_bias);  _unsafe_view_17 = p_h_17_self_attention_dense_weight = p_h_17_self_attention_dense_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_52: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_104, 0.0, False);  linear_104 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_105: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_101, dropout_52);  add_101 = dropout_52 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_161: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_105, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_54: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_161, 2)
            mean_35: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_54, [-1], True);  pow_54 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_106: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_35, 1e-05);  mean_35 = None
            rsqrt_35: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_106);  add_106 = None
            mul_231: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_161, rsqrt_35);  _to_copy_161 = rsqrt_35 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_162: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_231, dtype = torch.float32);  mul_231 = None
            mul_232: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_17_post_attention_layernorm_weight, _to_copy_162);  p_h_17_post_attention_layernorm_weight = _to_copy_162 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:499 in forward, code: intermediate_output = self.down_proj(F.silu(self.gate_proj(hidden_states)) * self.up_proj(hidden_states))
            linear_105: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_232, p_h_17_mlp_gate_proj_weight);  p_h_17_mlp_gate_proj_weight = None
            silu_17: "f32[1, 6, 12288]" = torch.ops.aten.silu.default(linear_105);  linear_105 = None
            linear_106: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_232, p_h_17_mlp_up_proj_weight);  mul_232 = p_h_17_mlp_up_proj_weight = None
            mul_233: "f32[1, 6, 12288]" = torch.ops.aten.mul.Tensor(silu_17, linear_106);  silu_17 = linear_106 = None
            linear_107: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(mul_233, p_h_17_mlp_down_proj_weight, p_h_17_mlp_down_proj_bias);  mul_233 = p_h_17_mlp_down_proj_weight = p_h_17_mlp_down_proj_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_53: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_107, 0.0, False);  linear_107 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_107: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_105, dropout_53);  add_105 = dropout_53 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_163: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_107, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_55: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_163, 2)
            mean_36: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_55, [-1], True);  pow_55 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_108: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_36, 1e-05);  mean_36 = None
            rsqrt_36: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_108);  add_108 = None
            mul_234: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_163, rsqrt_36);  _to_copy_163 = rsqrt_36 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_164: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_234, dtype = torch.float32);  mul_234 = None
            mul_235: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_18_input_layernorm_weight, _to_copy_164);  p_h_18_input_layernorm_weight = _to_copy_164 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:404 in forward, code: hidden_states = hidden_states.transpose(1, 0)
            transpose_90: "f32[6, 1, 5120]" = torch.ops.aten.transpose.int(mul_235, 1, 0);  mul_235 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:405 in forward, code: query_layer = self.query(hidden_states)
            linear_108: "f32[6, 1, 5120]" = torch.ops.aten.linear.default(transpose_90, p_h_18_self_attention_query_weight);  p_h_18_self_attention_query_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:409 in forward, code: query_layer = query_layer.view(*new_tensor_shape)
            view_216: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(linear_108, [6, 1, 32, 160]);  linear_108 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:411 in forward, code: mixed_kv_layer = self.key_value(hidden_states)
            linear_109: "f32[6, 1, 10240]" = torch.ops.aten.linear.default(transpose_90, p_h_18_self_attention_key_value_weight);  transpose_90 = p_h_18_self_attention_key_value_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:415 in forward, code: mixed_kv_layer = mixed_kv_layer.view(*new_tensor_shape)
            view_217: "f32[6, 1, 32, 320]" = torch.ops.aten.view.default(linear_109, [6, 1, 32, 320]);  linear_109 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:381 in split_tensor_along_last_dim, code: tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)
            split_18 = torch.ops.aten.split.Tensor(view_217, 160, 3);  view_217 = None
            getitem_36: "f32[6, 1, 32, 160]" = split_18[0]
            getitem_37: "f32[6, 1, 32, 160]" = split_18[1];  split_18 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:423 in forward, code: query_layer = query_layer.view(output_size[2], output_size[0] * output_size[1], -1)
            view_218: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_216, [6, 32, -1]);  view_216 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:424 in forward, code: key_layer = key_layer.view(output_size[3], output_size[0] * output_size[1], -1)
            view_219: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_36, [6, 32, -1]);  getitem_36 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:112 in forward, code: self.inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, device=x.device).float() / self.dim))
            arange_37: "i64[80]" = torch.ops.aten.arange.start_step(0, 160, 2, device = device(type='cpu'), pin_memory = False)
            _to_copy_165: "f32[80]" = torch.ops.aten._to_copy.default(arange_37, dtype = torch.float32);  arange_37 = None
            div_18: "f32[80]" = torch.ops.aten.div.Tensor(_to_copy_165, 160);  _to_copy_165 = None
            pow_56: "f32[80]" = torch.ops.aten.pow.Scalar(30420.108888514722, div_18);  div_18 = None
            reciprocal_18: "f32[80]" = torch.ops.aten.reciprocal.default(pow_56);  pow_56 = None
            mul_236: "f32[80]" = torch.ops.aten.mul.Tensor(reciprocal_18, 1.0);  reciprocal_18 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:114 in forward, code: t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)
            arange_38: "f32[8192]" = torch.ops.aten.arange.default(8192, dtype = torch.float32, device = device(type='cpu'), pin_memory = False)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:115 in forward, code: freqs = torch.einsum('i,j->ij', t, self.inv_freq)
            einsum_36: "f32[8192, 80]" = torch.ops.aten.einsum.default('i,j->ij', [arange_38, mul_236]);  arange_38 = mul_236 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:117 in forward, code: emb = torch.cat((freqs, freqs), dim=-1).to(x.device)
            cat_54: "f32[8192, 160]" = torch.ops.aten.cat.default([einsum_36, einsum_36], -1);  einsum_36 = None
            _to_copy_166: "f32[8192, 160]" = torch.ops.aten._to_copy.default(cat_54, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  cat_54 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:121 in forward, code: self.cos_cached = self.mscale * emb.cos()[:, None, :].to(dtype)
            cos_18: "f32[8192, 160]" = torch.ops.aten.cos.default(_to_copy_166)
            slice_194: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(cos_18, 0, 0, 9223372036854775807);  cos_18 = None
            unsqueeze_44: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_194, 1);  slice_194 = None
            slice_195: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_44, 2, 0, 9223372036854775807);  unsqueeze_44 = None
            _to_copy_167: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_195, dtype = torch.float32);  slice_195 = None
            mul_237: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_167, 1.0);  _to_copy_167 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:122 in forward, code: self.sin_cached = self.mscale * emb.sin()[:, None, :].to(dtype)
            sin_18: "f32[8192, 160]" = torch.ops.aten.sin.default(_to_copy_166);  _to_copy_166 = None
            slice_196: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(sin_18, 0, 0, 9223372036854775807);  sin_18 = None
            unsqueeze_45: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_196, 1);  slice_196 = None
            slice_197: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_45, 2, 0, 9223372036854775807);  unsqueeze_45 = None
            _to_copy_168: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_197, dtype = torch.float32);  slice_197 = None
            mul_238: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_168, 1.0);  _to_copy_168 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:123 in forward, code: return self.cos_cached[:seq_len, ...], self.sin_cached[:seq_len, ...]
            alias_36: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_237);  mul_237 = None
            alias_37: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_238);  mul_238 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:133 in apply_rotary_pos_emb_torch, code: cos, sin = cos[offset:q.shape[0] + offset, ...], sin[offset:q.shape[0] + offset, ...]
            slice_198: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_36, 0, 0, 6);  alias_36 = None
            slice_199: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_37, 0, 0, 6);  alias_37 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_239: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_218, slice_198)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_200: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_218, 2, 0, 80)
            slice_201: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_218, 2, 80, 9223372036854775807);  view_218 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_36: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_201);  slice_201 = None
            cat_55: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_36, slice_200], 2);  neg_36 = slice_200 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_240: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_55, slice_199);  cat_55 = None
            add_109: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_239, mul_240);  mul_239 = mul_240 = None
            mul_241: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_219, slice_198);  slice_198 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_202: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_219, 2, 0, 80)
            slice_203: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_219, 2, 80, 9223372036854775807);  view_219 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_37: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_203);  slice_203 = None
            cat_56: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_37, slice_202], 2);  neg_37 = slice_202 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_242: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_56, slice_199);  cat_56 = slice_199 = None
            add_110: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_241, mul_242);  mul_241 = mul_242 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:448 in forward, code: query_layer = query_layer.reshape((s_query, bz, head, dim))
            view_220: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_109, [6, 1, 32, 160]);  add_109 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:449 in forward, code: key_layer = key_layer.reshape((s_key, bz, head, dim))
            view_221: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_110, [6, 1, 32, 160]);  add_110 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:458 in forward, code: query_layer = query_layer.reshape(s_query, bz * self.num_heads, dim)
            view_222: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_220, [6, 32, 160]);  view_220 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:460 in forward, code: key_layer = key_layer.reshape(s_key, bz * self.num_heads, dim)
            view_223: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_221, [6, 32, 160]);  view_221 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            transpose_91: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_222, 0, 1);  view_222 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:462 in forward, code: key_layer.transpose(0, 1).transpose(1, 2))
            transpose_92: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_223, 0, 1);  view_223 = None
            transpose_93: "f32[32, 160, 6]" = torch.ops.aten.transpose.int(transpose_92, 1, 2);  transpose_92 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            einsum_37: "f32[32, 6, 6]" = torch.ops.aten.einsum.default('bik,bkj->bij', [transpose_91, transpose_93]);  transpose_91 = transpose_93 = None
            mul_243: "f32[32, 6, 6]" = torch.ops.aten.mul.Tensor(einsum_37, 0.07905694150420949);  einsum_37 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:464 in forward, code: attention_scores = matmul_result.view(bz, self.num_heads, s_query, s_key)
            view_224: "f32[1, 32, 6, 6]" = torch.ops.aten.view.default(mul_243, [1, 32, 6, 6]);  mul_243 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:469 in forward, code: attn_weights = torch.masked_fill(attention_scores, attention_mask, torch.finfo(attention_scores.dtype).min)
            masked_fill_18: "f32[1, 32, 6, 6]" = torch.ops.aten.masked_fill.Scalar(view_224, or_1, -3.4028234663852886e+38);  view_224 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:470 in forward, code: attention_probs = F.softmax(attn_weights, dim=-1).to(input_dtype)  ##dtype = torch.float32
            softmax_18: "f32[1, 32, 6, 6]" = torch.ops.aten.softmax.int(masked_fill_18, -1);  masked_fill_18 = None
            _to_copy_169: "f32[1, 32, 6, 6]" = torch.ops.aten._to_copy.default(softmax_18, dtype = torch.float32);  softmax_18 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:471 in forward, code: attention_probs = self.attention_dropout(attention_probs)
            dropout_54: "f32[1, 32, 6, 6]" = torch.ops.aten.dropout.default(_to_copy_169, 0.0, False);  _to_copy_169 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:472 in forward, code: attention_probs_reshaped = attention_probs.view(bz * self.num_heads, s_query, s_key)
            view_225: "f32[32, 6, 6]" = torch.ops.aten.view.default(dropout_54, [32, 6, 6]);  dropout_54 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:474 in forward, code: value_layer = value_layer.reshape(s_key, bz * self.num_heads, dim)
            view_226: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_37, [6, 32, 160]);  getitem_37 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:475 in forward, code: context_layer = torch.bmm(attention_probs_reshaped, value_layer.transpose(0, 1))
            transpose_94: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_226, 0, 1);  view_226 = None
            bmm_18: "f32[32, 6, 160]" = torch.ops.aten.bmm.default(view_225, transpose_94);  view_225 = transpose_94 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:391 in _merge_heads, code: x = x.view(batch_size, self.num_heads, seq_length, self.head_dim)
            view_227: "f32[1, 32, 6, 160]" = torch.ops.aten.view.default(bmm_18, [1, 32, 6, 160]);  bmm_18 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:392 in _merge_heads, code: x = x.permute(0, 2, 1, 3)
            permute_18: "f32[1, 6, 32, 160]" = torch.ops.aten.permute.default(view_227, [0, 2, 1, 3]);  view_227 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:393 in _merge_heads, code: return x.reshape(batch_size, seq_length, self.num_heads * self.head_dim)
            clone_18: "f32[1, 6, 32, 160]" = torch.ops.aten.clone.default(permute_18, memory_format = torch.contiguous_format);  permute_18 = None
            _unsafe_view_18: "f32[1, 6, 5120]" = torch.ops.aten._unsafe_view.default(clone_18, [1, 6, 5120]);  clone_18 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:478 in forward, code: output_tensor = self.dense(context_layer)
            linear_110: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(_unsafe_view_18, p_h_18_self_attention_dense_weight, p_h_18_self_attention_dense_bias);  _unsafe_view_18 = p_h_18_self_attention_dense_weight = p_h_18_self_attention_dense_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_55: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_110, 0.0, False);  linear_110 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_111: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_107, dropout_55);  add_107 = dropout_55 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_170: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_111, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_57: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_170, 2)
            mean_37: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_57, [-1], True);  pow_57 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_112: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_37, 1e-05);  mean_37 = None
            rsqrt_37: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_112);  add_112 = None
            mul_244: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_170, rsqrt_37);  _to_copy_170 = rsqrt_37 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_171: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_244, dtype = torch.float32);  mul_244 = None
            mul_245: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_18_post_attention_layernorm_weight, _to_copy_171);  p_h_18_post_attention_layernorm_weight = _to_copy_171 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:499 in forward, code: intermediate_output = self.down_proj(F.silu(self.gate_proj(hidden_states)) * self.up_proj(hidden_states))
            linear_111: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_245, p_h_18_mlp_gate_proj_weight);  p_h_18_mlp_gate_proj_weight = None
            silu_18: "f32[1, 6, 12288]" = torch.ops.aten.silu.default(linear_111);  linear_111 = None
            linear_112: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_245, p_h_18_mlp_up_proj_weight);  mul_245 = p_h_18_mlp_up_proj_weight = None
            mul_246: "f32[1, 6, 12288]" = torch.ops.aten.mul.Tensor(silu_18, linear_112);  silu_18 = linear_112 = None
            linear_113: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(mul_246, p_h_18_mlp_down_proj_weight, p_h_18_mlp_down_proj_bias);  mul_246 = p_h_18_mlp_down_proj_weight = p_h_18_mlp_down_proj_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_56: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_113, 0.0, False);  linear_113 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_113: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_111, dropout_56);  add_111 = dropout_56 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_172: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_113, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_58: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_172, 2)
            mean_38: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_58, [-1], True);  pow_58 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_114: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_38, 1e-05);  mean_38 = None
            rsqrt_38: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_114);  add_114 = None
            mul_247: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_172, rsqrt_38);  _to_copy_172 = rsqrt_38 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_173: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_247, dtype = torch.float32);  mul_247 = None
            mul_248: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_19_input_layernorm_weight, _to_copy_173);  p_h_19_input_layernorm_weight = _to_copy_173 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:404 in forward, code: hidden_states = hidden_states.transpose(1, 0)
            transpose_95: "f32[6, 1, 5120]" = torch.ops.aten.transpose.int(mul_248, 1, 0);  mul_248 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:405 in forward, code: query_layer = self.query(hidden_states)
            linear_114: "f32[6, 1, 5120]" = torch.ops.aten.linear.default(transpose_95, p_h_19_self_attention_query_weight);  p_h_19_self_attention_query_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:409 in forward, code: query_layer = query_layer.view(*new_tensor_shape)
            view_228: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(linear_114, [6, 1, 32, 160]);  linear_114 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:411 in forward, code: mixed_kv_layer = self.key_value(hidden_states)
            linear_115: "f32[6, 1, 10240]" = torch.ops.aten.linear.default(transpose_95, p_h_19_self_attention_key_value_weight);  transpose_95 = p_h_19_self_attention_key_value_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:415 in forward, code: mixed_kv_layer = mixed_kv_layer.view(*new_tensor_shape)
            view_229: "f32[6, 1, 32, 320]" = torch.ops.aten.view.default(linear_115, [6, 1, 32, 320]);  linear_115 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:381 in split_tensor_along_last_dim, code: tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)
            split_19 = torch.ops.aten.split.Tensor(view_229, 160, 3);  view_229 = None
            getitem_38: "f32[6, 1, 32, 160]" = split_19[0]
            getitem_39: "f32[6, 1, 32, 160]" = split_19[1];  split_19 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:423 in forward, code: query_layer = query_layer.view(output_size[2], output_size[0] * output_size[1], -1)
            view_230: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_228, [6, 32, -1]);  view_228 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:424 in forward, code: key_layer = key_layer.view(output_size[3], output_size[0] * output_size[1], -1)
            view_231: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_38, [6, 32, -1]);  getitem_38 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:112 in forward, code: self.inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, device=x.device).float() / self.dim))
            arange_39: "i64[80]" = torch.ops.aten.arange.start_step(0, 160, 2, device = device(type='cpu'), pin_memory = False)
            _to_copy_174: "f32[80]" = torch.ops.aten._to_copy.default(arange_39, dtype = torch.float32);  arange_39 = None
            div_19: "f32[80]" = torch.ops.aten.div.Tensor(_to_copy_174, 160);  _to_copy_174 = None
            pow_59: "f32[80]" = torch.ops.aten.pow.Scalar(30420.108888514722, div_19);  div_19 = None
            reciprocal_19: "f32[80]" = torch.ops.aten.reciprocal.default(pow_59);  pow_59 = None
            mul_249: "f32[80]" = torch.ops.aten.mul.Tensor(reciprocal_19, 1.0);  reciprocal_19 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:114 in forward, code: t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)
            arange_40: "f32[8192]" = torch.ops.aten.arange.default(8192, dtype = torch.float32, device = device(type='cpu'), pin_memory = False)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:115 in forward, code: freqs = torch.einsum('i,j->ij', t, self.inv_freq)
            einsum_38: "f32[8192, 80]" = torch.ops.aten.einsum.default('i,j->ij', [arange_40, mul_249]);  arange_40 = mul_249 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:117 in forward, code: emb = torch.cat((freqs, freqs), dim=-1).to(x.device)
            cat_57: "f32[8192, 160]" = torch.ops.aten.cat.default([einsum_38, einsum_38], -1);  einsum_38 = None
            _to_copy_175: "f32[8192, 160]" = torch.ops.aten._to_copy.default(cat_57, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  cat_57 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:121 in forward, code: self.cos_cached = self.mscale * emb.cos()[:, None, :].to(dtype)
            cos_19: "f32[8192, 160]" = torch.ops.aten.cos.default(_to_copy_175)
            slice_204: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(cos_19, 0, 0, 9223372036854775807);  cos_19 = None
            unsqueeze_46: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_204, 1);  slice_204 = None
            slice_205: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_46, 2, 0, 9223372036854775807);  unsqueeze_46 = None
            _to_copy_176: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_205, dtype = torch.float32);  slice_205 = None
            mul_250: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_176, 1.0);  _to_copy_176 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:122 in forward, code: self.sin_cached = self.mscale * emb.sin()[:, None, :].to(dtype)
            sin_19: "f32[8192, 160]" = torch.ops.aten.sin.default(_to_copy_175);  _to_copy_175 = None
            slice_206: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(sin_19, 0, 0, 9223372036854775807);  sin_19 = None
            unsqueeze_47: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_206, 1);  slice_206 = None
            slice_207: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_47, 2, 0, 9223372036854775807);  unsqueeze_47 = None
            _to_copy_177: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_207, dtype = torch.float32);  slice_207 = None
            mul_251: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_177, 1.0);  _to_copy_177 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:123 in forward, code: return self.cos_cached[:seq_len, ...], self.sin_cached[:seq_len, ...]
            alias_38: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_250);  mul_250 = None
            alias_39: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_251);  mul_251 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:133 in apply_rotary_pos_emb_torch, code: cos, sin = cos[offset:q.shape[0] + offset, ...], sin[offset:q.shape[0] + offset, ...]
            slice_208: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_38, 0, 0, 6);  alias_38 = None
            slice_209: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_39, 0, 0, 6);  alias_39 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_252: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_230, slice_208)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_210: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_230, 2, 0, 80)
            slice_211: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_230, 2, 80, 9223372036854775807);  view_230 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_38: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_211);  slice_211 = None
            cat_58: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_38, slice_210], 2);  neg_38 = slice_210 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_253: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_58, slice_209);  cat_58 = None
            add_115: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_252, mul_253);  mul_252 = mul_253 = None
            mul_254: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_231, slice_208);  slice_208 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_212: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_231, 2, 0, 80)
            slice_213: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_231, 2, 80, 9223372036854775807);  view_231 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_39: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_213);  slice_213 = None
            cat_59: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_39, slice_212], 2);  neg_39 = slice_212 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_255: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_59, slice_209);  cat_59 = slice_209 = None
            add_116: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_254, mul_255);  mul_254 = mul_255 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:448 in forward, code: query_layer = query_layer.reshape((s_query, bz, head, dim))
            view_232: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_115, [6, 1, 32, 160]);  add_115 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:449 in forward, code: key_layer = key_layer.reshape((s_key, bz, head, dim))
            view_233: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_116, [6, 1, 32, 160]);  add_116 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:458 in forward, code: query_layer = query_layer.reshape(s_query, bz * self.num_heads, dim)
            view_234: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_232, [6, 32, 160]);  view_232 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:460 in forward, code: key_layer = key_layer.reshape(s_key, bz * self.num_heads, dim)
            view_235: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_233, [6, 32, 160]);  view_233 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            transpose_96: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_234, 0, 1);  view_234 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:462 in forward, code: key_layer.transpose(0, 1).transpose(1, 2))
            transpose_97: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_235, 0, 1);  view_235 = None
            transpose_98: "f32[32, 160, 6]" = torch.ops.aten.transpose.int(transpose_97, 1, 2);  transpose_97 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            einsum_39: "f32[32, 6, 6]" = torch.ops.aten.einsum.default('bik,bkj->bij', [transpose_96, transpose_98]);  transpose_96 = transpose_98 = None
            mul_256: "f32[32, 6, 6]" = torch.ops.aten.mul.Tensor(einsum_39, 0.07905694150420949);  einsum_39 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:464 in forward, code: attention_scores = matmul_result.view(bz, self.num_heads, s_query, s_key)
            view_236: "f32[1, 32, 6, 6]" = torch.ops.aten.view.default(mul_256, [1, 32, 6, 6]);  mul_256 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:469 in forward, code: attn_weights = torch.masked_fill(attention_scores, attention_mask, torch.finfo(attention_scores.dtype).min)
            masked_fill_19: "f32[1, 32, 6, 6]" = torch.ops.aten.masked_fill.Scalar(view_236, or_1, -3.4028234663852886e+38);  view_236 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:470 in forward, code: attention_probs = F.softmax(attn_weights, dim=-1).to(input_dtype)  ##dtype = torch.float32
            softmax_19: "f32[1, 32, 6, 6]" = torch.ops.aten.softmax.int(masked_fill_19, -1);  masked_fill_19 = None
            _to_copy_178: "f32[1, 32, 6, 6]" = torch.ops.aten._to_copy.default(softmax_19, dtype = torch.float32);  softmax_19 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:471 in forward, code: attention_probs = self.attention_dropout(attention_probs)
            dropout_57: "f32[1, 32, 6, 6]" = torch.ops.aten.dropout.default(_to_copy_178, 0.0, False);  _to_copy_178 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:472 in forward, code: attention_probs_reshaped = attention_probs.view(bz * self.num_heads, s_query, s_key)
            view_237: "f32[32, 6, 6]" = torch.ops.aten.view.default(dropout_57, [32, 6, 6]);  dropout_57 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:474 in forward, code: value_layer = value_layer.reshape(s_key, bz * self.num_heads, dim)
            view_238: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_39, [6, 32, 160]);  getitem_39 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:475 in forward, code: context_layer = torch.bmm(attention_probs_reshaped, value_layer.transpose(0, 1))
            transpose_99: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_238, 0, 1);  view_238 = None
            bmm_19: "f32[32, 6, 160]" = torch.ops.aten.bmm.default(view_237, transpose_99);  view_237 = transpose_99 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:391 in _merge_heads, code: x = x.view(batch_size, self.num_heads, seq_length, self.head_dim)
            view_239: "f32[1, 32, 6, 160]" = torch.ops.aten.view.default(bmm_19, [1, 32, 6, 160]);  bmm_19 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:392 in _merge_heads, code: x = x.permute(0, 2, 1, 3)
            permute_19: "f32[1, 6, 32, 160]" = torch.ops.aten.permute.default(view_239, [0, 2, 1, 3]);  view_239 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:393 in _merge_heads, code: return x.reshape(batch_size, seq_length, self.num_heads * self.head_dim)
            clone_19: "f32[1, 6, 32, 160]" = torch.ops.aten.clone.default(permute_19, memory_format = torch.contiguous_format);  permute_19 = None
            _unsafe_view_19: "f32[1, 6, 5120]" = torch.ops.aten._unsafe_view.default(clone_19, [1, 6, 5120]);  clone_19 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:478 in forward, code: output_tensor = self.dense(context_layer)
            linear_116: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(_unsafe_view_19, p_h_19_self_attention_dense_weight, p_h_19_self_attention_dense_bias);  _unsafe_view_19 = p_h_19_self_attention_dense_weight = p_h_19_self_attention_dense_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_58: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_116, 0.0, False);  linear_116 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_117: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_113, dropout_58);  add_113 = dropout_58 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_179: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_117, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_60: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_179, 2)
            mean_39: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_60, [-1], True);  pow_60 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_118: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_39, 1e-05);  mean_39 = None
            rsqrt_39: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_118);  add_118 = None
            mul_257: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_179, rsqrt_39);  _to_copy_179 = rsqrt_39 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_180: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_257, dtype = torch.float32);  mul_257 = None
            mul_258: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_19_post_attention_layernorm_weight, _to_copy_180);  p_h_19_post_attention_layernorm_weight = _to_copy_180 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:499 in forward, code: intermediate_output = self.down_proj(F.silu(self.gate_proj(hidden_states)) * self.up_proj(hidden_states))
            linear_117: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_258, p_h_19_mlp_gate_proj_weight);  p_h_19_mlp_gate_proj_weight = None
            silu_19: "f32[1, 6, 12288]" = torch.ops.aten.silu.default(linear_117);  linear_117 = None
            linear_118: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_258, p_h_19_mlp_up_proj_weight);  mul_258 = p_h_19_mlp_up_proj_weight = None
            mul_259: "f32[1, 6, 12288]" = torch.ops.aten.mul.Tensor(silu_19, linear_118);  silu_19 = linear_118 = None
            linear_119: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(mul_259, p_h_19_mlp_down_proj_weight, p_h_19_mlp_down_proj_bias);  mul_259 = p_h_19_mlp_down_proj_weight = p_h_19_mlp_down_proj_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_59: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_119, 0.0, False);  linear_119 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_119: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_117, dropout_59);  add_117 = dropout_59 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_181: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_119, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_61: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_181, 2)
            mean_40: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_61, [-1], True);  pow_61 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_120: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_40, 1e-05);  mean_40 = None
            rsqrt_40: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_120);  add_120 = None
            mul_260: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_181, rsqrt_40);  _to_copy_181 = rsqrt_40 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_182: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_260, dtype = torch.float32);  mul_260 = None
            mul_261: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_20_input_layernorm_weight, _to_copy_182);  p_h_20_input_layernorm_weight = _to_copy_182 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:404 in forward, code: hidden_states = hidden_states.transpose(1, 0)
            transpose_100: "f32[6, 1, 5120]" = torch.ops.aten.transpose.int(mul_261, 1, 0);  mul_261 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:405 in forward, code: query_layer = self.query(hidden_states)
            linear_120: "f32[6, 1, 5120]" = torch.ops.aten.linear.default(transpose_100, p_h_20_self_attention_query_weight);  p_h_20_self_attention_query_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:409 in forward, code: query_layer = query_layer.view(*new_tensor_shape)
            view_240: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(linear_120, [6, 1, 32, 160]);  linear_120 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:411 in forward, code: mixed_kv_layer = self.key_value(hidden_states)
            linear_121: "f32[6, 1, 10240]" = torch.ops.aten.linear.default(transpose_100, p_h_20_self_attention_key_value_weight);  transpose_100 = p_h_20_self_attention_key_value_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:415 in forward, code: mixed_kv_layer = mixed_kv_layer.view(*new_tensor_shape)
            view_241: "f32[6, 1, 32, 320]" = torch.ops.aten.view.default(linear_121, [6, 1, 32, 320]);  linear_121 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:381 in split_tensor_along_last_dim, code: tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)
            split_20 = torch.ops.aten.split.Tensor(view_241, 160, 3);  view_241 = None
            getitem_40: "f32[6, 1, 32, 160]" = split_20[0]
            getitem_41: "f32[6, 1, 32, 160]" = split_20[1];  split_20 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:423 in forward, code: query_layer = query_layer.view(output_size[2], output_size[0] * output_size[1], -1)
            view_242: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_240, [6, 32, -1]);  view_240 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:424 in forward, code: key_layer = key_layer.view(output_size[3], output_size[0] * output_size[1], -1)
            view_243: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_40, [6, 32, -1]);  getitem_40 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:112 in forward, code: self.inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, device=x.device).float() / self.dim))
            arange_41: "i64[80]" = torch.ops.aten.arange.start_step(0, 160, 2, device = device(type='cpu'), pin_memory = False)
            _to_copy_183: "f32[80]" = torch.ops.aten._to_copy.default(arange_41, dtype = torch.float32);  arange_41 = None
            div_20: "f32[80]" = torch.ops.aten.div.Tensor(_to_copy_183, 160);  _to_copy_183 = None
            pow_62: "f32[80]" = torch.ops.aten.pow.Scalar(30420.108888514722, div_20);  div_20 = None
            reciprocal_20: "f32[80]" = torch.ops.aten.reciprocal.default(pow_62);  pow_62 = None
            mul_262: "f32[80]" = torch.ops.aten.mul.Tensor(reciprocal_20, 1.0);  reciprocal_20 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:114 in forward, code: t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)
            arange_42: "f32[8192]" = torch.ops.aten.arange.default(8192, dtype = torch.float32, device = device(type='cpu'), pin_memory = False)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:115 in forward, code: freqs = torch.einsum('i,j->ij', t, self.inv_freq)
            einsum_40: "f32[8192, 80]" = torch.ops.aten.einsum.default('i,j->ij', [arange_42, mul_262]);  arange_42 = mul_262 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:117 in forward, code: emb = torch.cat((freqs, freqs), dim=-1).to(x.device)
            cat_60: "f32[8192, 160]" = torch.ops.aten.cat.default([einsum_40, einsum_40], -1);  einsum_40 = None
            _to_copy_184: "f32[8192, 160]" = torch.ops.aten._to_copy.default(cat_60, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  cat_60 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:121 in forward, code: self.cos_cached = self.mscale * emb.cos()[:, None, :].to(dtype)
            cos_20: "f32[8192, 160]" = torch.ops.aten.cos.default(_to_copy_184)
            slice_214: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(cos_20, 0, 0, 9223372036854775807);  cos_20 = None
            unsqueeze_48: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_214, 1);  slice_214 = None
            slice_215: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_48, 2, 0, 9223372036854775807);  unsqueeze_48 = None
            _to_copy_185: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_215, dtype = torch.float32);  slice_215 = None
            mul_263: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_185, 1.0);  _to_copy_185 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:122 in forward, code: self.sin_cached = self.mscale * emb.sin()[:, None, :].to(dtype)
            sin_20: "f32[8192, 160]" = torch.ops.aten.sin.default(_to_copy_184);  _to_copy_184 = None
            slice_216: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(sin_20, 0, 0, 9223372036854775807);  sin_20 = None
            unsqueeze_49: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_216, 1);  slice_216 = None
            slice_217: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_49, 2, 0, 9223372036854775807);  unsqueeze_49 = None
            _to_copy_186: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_217, dtype = torch.float32);  slice_217 = None
            mul_264: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_186, 1.0);  _to_copy_186 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:123 in forward, code: return self.cos_cached[:seq_len, ...], self.sin_cached[:seq_len, ...]
            alias_40: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_263);  mul_263 = None
            alias_41: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_264);  mul_264 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:133 in apply_rotary_pos_emb_torch, code: cos, sin = cos[offset:q.shape[0] + offset, ...], sin[offset:q.shape[0] + offset, ...]
            slice_218: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_40, 0, 0, 6);  alias_40 = None
            slice_219: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_41, 0, 0, 6);  alias_41 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_265: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_242, slice_218)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_220: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_242, 2, 0, 80)
            slice_221: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_242, 2, 80, 9223372036854775807);  view_242 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_40: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_221);  slice_221 = None
            cat_61: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_40, slice_220], 2);  neg_40 = slice_220 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_266: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_61, slice_219);  cat_61 = None
            add_121: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_265, mul_266);  mul_265 = mul_266 = None
            mul_267: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_243, slice_218);  slice_218 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_222: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_243, 2, 0, 80)
            slice_223: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_243, 2, 80, 9223372036854775807);  view_243 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_41: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_223);  slice_223 = None
            cat_62: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_41, slice_222], 2);  neg_41 = slice_222 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_268: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_62, slice_219);  cat_62 = slice_219 = None
            add_122: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_267, mul_268);  mul_267 = mul_268 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:448 in forward, code: query_layer = query_layer.reshape((s_query, bz, head, dim))
            view_244: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_121, [6, 1, 32, 160]);  add_121 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:449 in forward, code: key_layer = key_layer.reshape((s_key, bz, head, dim))
            view_245: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_122, [6, 1, 32, 160]);  add_122 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:458 in forward, code: query_layer = query_layer.reshape(s_query, bz * self.num_heads, dim)
            view_246: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_244, [6, 32, 160]);  view_244 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:460 in forward, code: key_layer = key_layer.reshape(s_key, bz * self.num_heads, dim)
            view_247: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_245, [6, 32, 160]);  view_245 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            transpose_101: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_246, 0, 1);  view_246 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:462 in forward, code: key_layer.transpose(0, 1).transpose(1, 2))
            transpose_102: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_247, 0, 1);  view_247 = None
            transpose_103: "f32[32, 160, 6]" = torch.ops.aten.transpose.int(transpose_102, 1, 2);  transpose_102 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            einsum_41: "f32[32, 6, 6]" = torch.ops.aten.einsum.default('bik,bkj->bij', [transpose_101, transpose_103]);  transpose_101 = transpose_103 = None
            mul_269: "f32[32, 6, 6]" = torch.ops.aten.mul.Tensor(einsum_41, 0.07905694150420949);  einsum_41 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:464 in forward, code: attention_scores = matmul_result.view(bz, self.num_heads, s_query, s_key)
            view_248: "f32[1, 32, 6, 6]" = torch.ops.aten.view.default(mul_269, [1, 32, 6, 6]);  mul_269 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:469 in forward, code: attn_weights = torch.masked_fill(attention_scores, attention_mask, torch.finfo(attention_scores.dtype).min)
            masked_fill_20: "f32[1, 32, 6, 6]" = torch.ops.aten.masked_fill.Scalar(view_248, or_1, -3.4028234663852886e+38);  view_248 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:470 in forward, code: attention_probs = F.softmax(attn_weights, dim=-1).to(input_dtype)  ##dtype = torch.float32
            softmax_20: "f32[1, 32, 6, 6]" = torch.ops.aten.softmax.int(masked_fill_20, -1);  masked_fill_20 = None
            _to_copy_187: "f32[1, 32, 6, 6]" = torch.ops.aten._to_copy.default(softmax_20, dtype = torch.float32);  softmax_20 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:471 in forward, code: attention_probs = self.attention_dropout(attention_probs)
            dropout_60: "f32[1, 32, 6, 6]" = torch.ops.aten.dropout.default(_to_copy_187, 0.0, False);  _to_copy_187 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:472 in forward, code: attention_probs_reshaped = attention_probs.view(bz * self.num_heads, s_query, s_key)
            view_249: "f32[32, 6, 6]" = torch.ops.aten.view.default(dropout_60, [32, 6, 6]);  dropout_60 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:474 in forward, code: value_layer = value_layer.reshape(s_key, bz * self.num_heads, dim)
            view_250: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_41, [6, 32, 160]);  getitem_41 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:475 in forward, code: context_layer = torch.bmm(attention_probs_reshaped, value_layer.transpose(0, 1))
            transpose_104: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_250, 0, 1);  view_250 = None
            bmm_20: "f32[32, 6, 160]" = torch.ops.aten.bmm.default(view_249, transpose_104);  view_249 = transpose_104 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:391 in _merge_heads, code: x = x.view(batch_size, self.num_heads, seq_length, self.head_dim)
            view_251: "f32[1, 32, 6, 160]" = torch.ops.aten.view.default(bmm_20, [1, 32, 6, 160]);  bmm_20 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:392 in _merge_heads, code: x = x.permute(0, 2, 1, 3)
            permute_20: "f32[1, 6, 32, 160]" = torch.ops.aten.permute.default(view_251, [0, 2, 1, 3]);  view_251 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:393 in _merge_heads, code: return x.reshape(batch_size, seq_length, self.num_heads * self.head_dim)
            clone_20: "f32[1, 6, 32, 160]" = torch.ops.aten.clone.default(permute_20, memory_format = torch.contiguous_format);  permute_20 = None
            _unsafe_view_20: "f32[1, 6, 5120]" = torch.ops.aten._unsafe_view.default(clone_20, [1, 6, 5120]);  clone_20 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:478 in forward, code: output_tensor = self.dense(context_layer)
            linear_122: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(_unsafe_view_20, p_h_20_self_attention_dense_weight, p_h_20_self_attention_dense_bias);  _unsafe_view_20 = p_h_20_self_attention_dense_weight = p_h_20_self_attention_dense_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_61: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_122, 0.0, False);  linear_122 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_123: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_119, dropout_61);  add_119 = dropout_61 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_188: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_123, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_63: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_188, 2)
            mean_41: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_63, [-1], True);  pow_63 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_124: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_41, 1e-05);  mean_41 = None
            rsqrt_41: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_124);  add_124 = None
            mul_270: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_188, rsqrt_41);  _to_copy_188 = rsqrt_41 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_189: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_270, dtype = torch.float32);  mul_270 = None
            mul_271: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_20_post_attention_layernorm_weight, _to_copy_189);  p_h_20_post_attention_layernorm_weight = _to_copy_189 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:499 in forward, code: intermediate_output = self.down_proj(F.silu(self.gate_proj(hidden_states)) * self.up_proj(hidden_states))
            linear_123: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_271, p_h_20_mlp_gate_proj_weight);  p_h_20_mlp_gate_proj_weight = None
            silu_20: "f32[1, 6, 12288]" = torch.ops.aten.silu.default(linear_123);  linear_123 = None
            linear_124: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_271, p_h_20_mlp_up_proj_weight);  mul_271 = p_h_20_mlp_up_proj_weight = None
            mul_272: "f32[1, 6, 12288]" = torch.ops.aten.mul.Tensor(silu_20, linear_124);  silu_20 = linear_124 = None
            linear_125: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(mul_272, p_h_20_mlp_down_proj_weight, p_h_20_mlp_down_proj_bias);  mul_272 = p_h_20_mlp_down_proj_weight = p_h_20_mlp_down_proj_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_62: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_125, 0.0, False);  linear_125 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_125: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_123, dropout_62);  add_123 = dropout_62 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_190: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_125, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_64: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_190, 2)
            mean_42: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_64, [-1], True);  pow_64 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_126: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_42, 1e-05);  mean_42 = None
            rsqrt_42: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_126);  add_126 = None
            mul_273: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_190, rsqrt_42);  _to_copy_190 = rsqrt_42 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_191: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_273, dtype = torch.float32);  mul_273 = None
            mul_274: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_21_input_layernorm_weight, _to_copy_191);  p_h_21_input_layernorm_weight = _to_copy_191 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:404 in forward, code: hidden_states = hidden_states.transpose(1, 0)
            transpose_105: "f32[6, 1, 5120]" = torch.ops.aten.transpose.int(mul_274, 1, 0);  mul_274 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:405 in forward, code: query_layer = self.query(hidden_states)
            linear_126: "f32[6, 1, 5120]" = torch.ops.aten.linear.default(transpose_105, p_h_21_self_attention_query_weight);  p_h_21_self_attention_query_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:409 in forward, code: query_layer = query_layer.view(*new_tensor_shape)
            view_252: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(linear_126, [6, 1, 32, 160]);  linear_126 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:411 in forward, code: mixed_kv_layer = self.key_value(hidden_states)
            linear_127: "f32[6, 1, 10240]" = torch.ops.aten.linear.default(transpose_105, p_h_21_self_attention_key_value_weight);  transpose_105 = p_h_21_self_attention_key_value_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:415 in forward, code: mixed_kv_layer = mixed_kv_layer.view(*new_tensor_shape)
            view_253: "f32[6, 1, 32, 320]" = torch.ops.aten.view.default(linear_127, [6, 1, 32, 320]);  linear_127 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:381 in split_tensor_along_last_dim, code: tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)
            split_21 = torch.ops.aten.split.Tensor(view_253, 160, 3);  view_253 = None
            getitem_42: "f32[6, 1, 32, 160]" = split_21[0]
            getitem_43: "f32[6, 1, 32, 160]" = split_21[1];  split_21 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:423 in forward, code: query_layer = query_layer.view(output_size[2], output_size[0] * output_size[1], -1)
            view_254: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_252, [6, 32, -1]);  view_252 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:424 in forward, code: key_layer = key_layer.view(output_size[3], output_size[0] * output_size[1], -1)
            view_255: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_42, [6, 32, -1]);  getitem_42 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:112 in forward, code: self.inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, device=x.device).float() / self.dim))
            arange_43: "i64[80]" = torch.ops.aten.arange.start_step(0, 160, 2, device = device(type='cpu'), pin_memory = False)
            _to_copy_192: "f32[80]" = torch.ops.aten._to_copy.default(arange_43, dtype = torch.float32);  arange_43 = None
            div_21: "f32[80]" = torch.ops.aten.div.Tensor(_to_copy_192, 160);  _to_copy_192 = None
            pow_65: "f32[80]" = torch.ops.aten.pow.Scalar(30420.108888514722, div_21);  div_21 = None
            reciprocal_21: "f32[80]" = torch.ops.aten.reciprocal.default(pow_65);  pow_65 = None
            mul_275: "f32[80]" = torch.ops.aten.mul.Tensor(reciprocal_21, 1.0);  reciprocal_21 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:114 in forward, code: t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)
            arange_44: "f32[8192]" = torch.ops.aten.arange.default(8192, dtype = torch.float32, device = device(type='cpu'), pin_memory = False)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:115 in forward, code: freqs = torch.einsum('i,j->ij', t, self.inv_freq)
            einsum_42: "f32[8192, 80]" = torch.ops.aten.einsum.default('i,j->ij', [arange_44, mul_275]);  arange_44 = mul_275 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:117 in forward, code: emb = torch.cat((freqs, freqs), dim=-1).to(x.device)
            cat_63: "f32[8192, 160]" = torch.ops.aten.cat.default([einsum_42, einsum_42], -1);  einsum_42 = None
            _to_copy_193: "f32[8192, 160]" = torch.ops.aten._to_copy.default(cat_63, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  cat_63 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:121 in forward, code: self.cos_cached = self.mscale * emb.cos()[:, None, :].to(dtype)
            cos_21: "f32[8192, 160]" = torch.ops.aten.cos.default(_to_copy_193)
            slice_224: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(cos_21, 0, 0, 9223372036854775807);  cos_21 = None
            unsqueeze_50: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_224, 1);  slice_224 = None
            slice_225: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_50, 2, 0, 9223372036854775807);  unsqueeze_50 = None
            _to_copy_194: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_225, dtype = torch.float32);  slice_225 = None
            mul_276: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_194, 1.0);  _to_copy_194 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:122 in forward, code: self.sin_cached = self.mscale * emb.sin()[:, None, :].to(dtype)
            sin_21: "f32[8192, 160]" = torch.ops.aten.sin.default(_to_copy_193);  _to_copy_193 = None
            slice_226: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(sin_21, 0, 0, 9223372036854775807);  sin_21 = None
            unsqueeze_51: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_226, 1);  slice_226 = None
            slice_227: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_51, 2, 0, 9223372036854775807);  unsqueeze_51 = None
            _to_copy_195: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_227, dtype = torch.float32);  slice_227 = None
            mul_277: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_195, 1.0);  _to_copy_195 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:123 in forward, code: return self.cos_cached[:seq_len, ...], self.sin_cached[:seq_len, ...]
            alias_42: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_276);  mul_276 = None
            alias_43: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_277);  mul_277 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:133 in apply_rotary_pos_emb_torch, code: cos, sin = cos[offset:q.shape[0] + offset, ...], sin[offset:q.shape[0] + offset, ...]
            slice_228: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_42, 0, 0, 6);  alias_42 = None
            slice_229: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_43, 0, 0, 6);  alias_43 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_278: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_254, slice_228)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_230: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_254, 2, 0, 80)
            slice_231: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_254, 2, 80, 9223372036854775807);  view_254 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_42: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_231);  slice_231 = None
            cat_64: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_42, slice_230], 2);  neg_42 = slice_230 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_279: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_64, slice_229);  cat_64 = None
            add_127: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_278, mul_279);  mul_278 = mul_279 = None
            mul_280: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_255, slice_228);  slice_228 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_232: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_255, 2, 0, 80)
            slice_233: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_255, 2, 80, 9223372036854775807);  view_255 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_43: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_233);  slice_233 = None
            cat_65: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_43, slice_232], 2);  neg_43 = slice_232 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_281: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_65, slice_229);  cat_65 = slice_229 = None
            add_128: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_280, mul_281);  mul_280 = mul_281 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:448 in forward, code: query_layer = query_layer.reshape((s_query, bz, head, dim))
            view_256: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_127, [6, 1, 32, 160]);  add_127 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:449 in forward, code: key_layer = key_layer.reshape((s_key, bz, head, dim))
            view_257: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_128, [6, 1, 32, 160]);  add_128 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:458 in forward, code: query_layer = query_layer.reshape(s_query, bz * self.num_heads, dim)
            view_258: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_256, [6, 32, 160]);  view_256 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:460 in forward, code: key_layer = key_layer.reshape(s_key, bz * self.num_heads, dim)
            view_259: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_257, [6, 32, 160]);  view_257 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            transpose_106: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_258, 0, 1);  view_258 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:462 in forward, code: key_layer.transpose(0, 1).transpose(1, 2))
            transpose_107: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_259, 0, 1);  view_259 = None
            transpose_108: "f32[32, 160, 6]" = torch.ops.aten.transpose.int(transpose_107, 1, 2);  transpose_107 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            einsum_43: "f32[32, 6, 6]" = torch.ops.aten.einsum.default('bik,bkj->bij', [transpose_106, transpose_108]);  transpose_106 = transpose_108 = None
            mul_282: "f32[32, 6, 6]" = torch.ops.aten.mul.Tensor(einsum_43, 0.07905694150420949);  einsum_43 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:464 in forward, code: attention_scores = matmul_result.view(bz, self.num_heads, s_query, s_key)
            view_260: "f32[1, 32, 6, 6]" = torch.ops.aten.view.default(mul_282, [1, 32, 6, 6]);  mul_282 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:469 in forward, code: attn_weights = torch.masked_fill(attention_scores, attention_mask, torch.finfo(attention_scores.dtype).min)
            masked_fill_21: "f32[1, 32, 6, 6]" = torch.ops.aten.masked_fill.Scalar(view_260, or_1, -3.4028234663852886e+38);  view_260 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:470 in forward, code: attention_probs = F.softmax(attn_weights, dim=-1).to(input_dtype)  ##dtype = torch.float32
            softmax_21: "f32[1, 32, 6, 6]" = torch.ops.aten.softmax.int(masked_fill_21, -1);  masked_fill_21 = None
            _to_copy_196: "f32[1, 32, 6, 6]" = torch.ops.aten._to_copy.default(softmax_21, dtype = torch.float32);  softmax_21 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:471 in forward, code: attention_probs = self.attention_dropout(attention_probs)
            dropout_63: "f32[1, 32, 6, 6]" = torch.ops.aten.dropout.default(_to_copy_196, 0.0, False);  _to_copy_196 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:472 in forward, code: attention_probs_reshaped = attention_probs.view(bz * self.num_heads, s_query, s_key)
            view_261: "f32[32, 6, 6]" = torch.ops.aten.view.default(dropout_63, [32, 6, 6]);  dropout_63 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:474 in forward, code: value_layer = value_layer.reshape(s_key, bz * self.num_heads, dim)
            view_262: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_43, [6, 32, 160]);  getitem_43 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:475 in forward, code: context_layer = torch.bmm(attention_probs_reshaped, value_layer.transpose(0, 1))
            transpose_109: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_262, 0, 1);  view_262 = None
            bmm_21: "f32[32, 6, 160]" = torch.ops.aten.bmm.default(view_261, transpose_109);  view_261 = transpose_109 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:391 in _merge_heads, code: x = x.view(batch_size, self.num_heads, seq_length, self.head_dim)
            view_263: "f32[1, 32, 6, 160]" = torch.ops.aten.view.default(bmm_21, [1, 32, 6, 160]);  bmm_21 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:392 in _merge_heads, code: x = x.permute(0, 2, 1, 3)
            permute_21: "f32[1, 6, 32, 160]" = torch.ops.aten.permute.default(view_263, [0, 2, 1, 3]);  view_263 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:393 in _merge_heads, code: return x.reshape(batch_size, seq_length, self.num_heads * self.head_dim)
            clone_21: "f32[1, 6, 32, 160]" = torch.ops.aten.clone.default(permute_21, memory_format = torch.contiguous_format);  permute_21 = None
            _unsafe_view_21: "f32[1, 6, 5120]" = torch.ops.aten._unsafe_view.default(clone_21, [1, 6, 5120]);  clone_21 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:478 in forward, code: output_tensor = self.dense(context_layer)
            linear_128: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(_unsafe_view_21, p_h_21_self_attention_dense_weight, p_h_21_self_attention_dense_bias);  _unsafe_view_21 = p_h_21_self_attention_dense_weight = p_h_21_self_attention_dense_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_64: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_128, 0.0, False);  linear_128 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_129: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_125, dropout_64);  add_125 = dropout_64 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_197: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_129, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_66: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_197, 2)
            mean_43: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_66, [-1], True);  pow_66 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_130: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_43, 1e-05);  mean_43 = None
            rsqrt_43: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_130);  add_130 = None
            mul_283: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_197, rsqrt_43);  _to_copy_197 = rsqrt_43 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_198: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_283, dtype = torch.float32);  mul_283 = None
            mul_284: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_21_post_attention_layernorm_weight, _to_copy_198);  p_h_21_post_attention_layernorm_weight = _to_copy_198 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:499 in forward, code: intermediate_output = self.down_proj(F.silu(self.gate_proj(hidden_states)) * self.up_proj(hidden_states))
            linear_129: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_284, p_h_21_mlp_gate_proj_weight);  p_h_21_mlp_gate_proj_weight = None
            silu_21: "f32[1, 6, 12288]" = torch.ops.aten.silu.default(linear_129);  linear_129 = None
            linear_130: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_284, p_h_21_mlp_up_proj_weight);  mul_284 = p_h_21_mlp_up_proj_weight = None
            mul_285: "f32[1, 6, 12288]" = torch.ops.aten.mul.Tensor(silu_21, linear_130);  silu_21 = linear_130 = None
            linear_131: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(mul_285, p_h_21_mlp_down_proj_weight, p_h_21_mlp_down_proj_bias);  mul_285 = p_h_21_mlp_down_proj_weight = p_h_21_mlp_down_proj_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_65: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_131, 0.0, False);  linear_131 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_131: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_129, dropout_65);  add_129 = dropout_65 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_199: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_131, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_67: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_199, 2)
            mean_44: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_67, [-1], True);  pow_67 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_132: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_44, 1e-05);  mean_44 = None
            rsqrt_44: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_132);  add_132 = None
            mul_286: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_199, rsqrt_44);  _to_copy_199 = rsqrt_44 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_200: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_286, dtype = torch.float32);  mul_286 = None
            mul_287: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_22_input_layernorm_weight, _to_copy_200);  p_h_22_input_layernorm_weight = _to_copy_200 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:404 in forward, code: hidden_states = hidden_states.transpose(1, 0)
            transpose_110: "f32[6, 1, 5120]" = torch.ops.aten.transpose.int(mul_287, 1, 0);  mul_287 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:405 in forward, code: query_layer = self.query(hidden_states)
            linear_132: "f32[6, 1, 5120]" = torch.ops.aten.linear.default(transpose_110, p_h_22_self_attention_query_weight);  p_h_22_self_attention_query_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:409 in forward, code: query_layer = query_layer.view(*new_tensor_shape)
            view_264: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(linear_132, [6, 1, 32, 160]);  linear_132 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:411 in forward, code: mixed_kv_layer = self.key_value(hidden_states)
            linear_133: "f32[6, 1, 10240]" = torch.ops.aten.linear.default(transpose_110, p_h_22_self_attention_key_value_weight);  transpose_110 = p_h_22_self_attention_key_value_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:415 in forward, code: mixed_kv_layer = mixed_kv_layer.view(*new_tensor_shape)
            view_265: "f32[6, 1, 32, 320]" = torch.ops.aten.view.default(linear_133, [6, 1, 32, 320]);  linear_133 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:381 in split_tensor_along_last_dim, code: tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)
            split_22 = torch.ops.aten.split.Tensor(view_265, 160, 3);  view_265 = None
            getitem_44: "f32[6, 1, 32, 160]" = split_22[0]
            getitem_45: "f32[6, 1, 32, 160]" = split_22[1];  split_22 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:423 in forward, code: query_layer = query_layer.view(output_size[2], output_size[0] * output_size[1], -1)
            view_266: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_264, [6, 32, -1]);  view_264 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:424 in forward, code: key_layer = key_layer.view(output_size[3], output_size[0] * output_size[1], -1)
            view_267: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_44, [6, 32, -1]);  getitem_44 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:112 in forward, code: self.inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, device=x.device).float() / self.dim))
            arange_45: "i64[80]" = torch.ops.aten.arange.start_step(0, 160, 2, device = device(type='cpu'), pin_memory = False)
            _to_copy_201: "f32[80]" = torch.ops.aten._to_copy.default(arange_45, dtype = torch.float32);  arange_45 = None
            div_22: "f32[80]" = torch.ops.aten.div.Tensor(_to_copy_201, 160);  _to_copy_201 = None
            pow_68: "f32[80]" = torch.ops.aten.pow.Scalar(30420.108888514722, div_22);  div_22 = None
            reciprocal_22: "f32[80]" = torch.ops.aten.reciprocal.default(pow_68);  pow_68 = None
            mul_288: "f32[80]" = torch.ops.aten.mul.Tensor(reciprocal_22, 1.0);  reciprocal_22 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:114 in forward, code: t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)
            arange_46: "f32[8192]" = torch.ops.aten.arange.default(8192, dtype = torch.float32, device = device(type='cpu'), pin_memory = False)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:115 in forward, code: freqs = torch.einsum('i,j->ij', t, self.inv_freq)
            einsum_44: "f32[8192, 80]" = torch.ops.aten.einsum.default('i,j->ij', [arange_46, mul_288]);  arange_46 = mul_288 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:117 in forward, code: emb = torch.cat((freqs, freqs), dim=-1).to(x.device)
            cat_66: "f32[8192, 160]" = torch.ops.aten.cat.default([einsum_44, einsum_44], -1);  einsum_44 = None
            _to_copy_202: "f32[8192, 160]" = torch.ops.aten._to_copy.default(cat_66, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  cat_66 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:121 in forward, code: self.cos_cached = self.mscale * emb.cos()[:, None, :].to(dtype)
            cos_22: "f32[8192, 160]" = torch.ops.aten.cos.default(_to_copy_202)
            slice_234: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(cos_22, 0, 0, 9223372036854775807);  cos_22 = None
            unsqueeze_52: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_234, 1);  slice_234 = None
            slice_235: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_52, 2, 0, 9223372036854775807);  unsqueeze_52 = None
            _to_copy_203: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_235, dtype = torch.float32);  slice_235 = None
            mul_289: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_203, 1.0);  _to_copy_203 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:122 in forward, code: self.sin_cached = self.mscale * emb.sin()[:, None, :].to(dtype)
            sin_22: "f32[8192, 160]" = torch.ops.aten.sin.default(_to_copy_202);  _to_copy_202 = None
            slice_236: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(sin_22, 0, 0, 9223372036854775807);  sin_22 = None
            unsqueeze_53: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_236, 1);  slice_236 = None
            slice_237: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_53, 2, 0, 9223372036854775807);  unsqueeze_53 = None
            _to_copy_204: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_237, dtype = torch.float32);  slice_237 = None
            mul_290: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_204, 1.0);  _to_copy_204 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:123 in forward, code: return self.cos_cached[:seq_len, ...], self.sin_cached[:seq_len, ...]
            alias_44: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_289);  mul_289 = None
            alias_45: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_290);  mul_290 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:133 in apply_rotary_pos_emb_torch, code: cos, sin = cos[offset:q.shape[0] + offset, ...], sin[offset:q.shape[0] + offset, ...]
            slice_238: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_44, 0, 0, 6);  alias_44 = None
            slice_239: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_45, 0, 0, 6);  alias_45 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_291: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_266, slice_238)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_240: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_266, 2, 0, 80)
            slice_241: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_266, 2, 80, 9223372036854775807);  view_266 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_44: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_241);  slice_241 = None
            cat_67: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_44, slice_240], 2);  neg_44 = slice_240 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_292: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_67, slice_239);  cat_67 = None
            add_133: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_291, mul_292);  mul_291 = mul_292 = None
            mul_293: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_267, slice_238);  slice_238 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_242: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_267, 2, 0, 80)
            slice_243: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_267, 2, 80, 9223372036854775807);  view_267 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_45: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_243);  slice_243 = None
            cat_68: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_45, slice_242], 2);  neg_45 = slice_242 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_294: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_68, slice_239);  cat_68 = slice_239 = None
            add_134: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_293, mul_294);  mul_293 = mul_294 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:448 in forward, code: query_layer = query_layer.reshape((s_query, bz, head, dim))
            view_268: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_133, [6, 1, 32, 160]);  add_133 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:449 in forward, code: key_layer = key_layer.reshape((s_key, bz, head, dim))
            view_269: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_134, [6, 1, 32, 160]);  add_134 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:458 in forward, code: query_layer = query_layer.reshape(s_query, bz * self.num_heads, dim)
            view_270: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_268, [6, 32, 160]);  view_268 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:460 in forward, code: key_layer = key_layer.reshape(s_key, bz * self.num_heads, dim)
            view_271: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_269, [6, 32, 160]);  view_269 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            transpose_111: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_270, 0, 1);  view_270 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:462 in forward, code: key_layer.transpose(0, 1).transpose(1, 2))
            transpose_112: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_271, 0, 1);  view_271 = None
            transpose_113: "f32[32, 160, 6]" = torch.ops.aten.transpose.int(transpose_112, 1, 2);  transpose_112 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            einsum_45: "f32[32, 6, 6]" = torch.ops.aten.einsum.default('bik,bkj->bij', [transpose_111, transpose_113]);  transpose_111 = transpose_113 = None
            mul_295: "f32[32, 6, 6]" = torch.ops.aten.mul.Tensor(einsum_45, 0.07905694150420949);  einsum_45 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:464 in forward, code: attention_scores = matmul_result.view(bz, self.num_heads, s_query, s_key)
            view_272: "f32[1, 32, 6, 6]" = torch.ops.aten.view.default(mul_295, [1, 32, 6, 6]);  mul_295 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:469 in forward, code: attn_weights = torch.masked_fill(attention_scores, attention_mask, torch.finfo(attention_scores.dtype).min)
            masked_fill_22: "f32[1, 32, 6, 6]" = torch.ops.aten.masked_fill.Scalar(view_272, or_1, -3.4028234663852886e+38);  view_272 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:470 in forward, code: attention_probs = F.softmax(attn_weights, dim=-1).to(input_dtype)  ##dtype = torch.float32
            softmax_22: "f32[1, 32, 6, 6]" = torch.ops.aten.softmax.int(masked_fill_22, -1);  masked_fill_22 = None
            _to_copy_205: "f32[1, 32, 6, 6]" = torch.ops.aten._to_copy.default(softmax_22, dtype = torch.float32);  softmax_22 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:471 in forward, code: attention_probs = self.attention_dropout(attention_probs)
            dropout_66: "f32[1, 32, 6, 6]" = torch.ops.aten.dropout.default(_to_copy_205, 0.0, False);  _to_copy_205 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:472 in forward, code: attention_probs_reshaped = attention_probs.view(bz * self.num_heads, s_query, s_key)
            view_273: "f32[32, 6, 6]" = torch.ops.aten.view.default(dropout_66, [32, 6, 6]);  dropout_66 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:474 in forward, code: value_layer = value_layer.reshape(s_key, bz * self.num_heads, dim)
            view_274: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_45, [6, 32, 160]);  getitem_45 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:475 in forward, code: context_layer = torch.bmm(attention_probs_reshaped, value_layer.transpose(0, 1))
            transpose_114: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_274, 0, 1);  view_274 = None
            bmm_22: "f32[32, 6, 160]" = torch.ops.aten.bmm.default(view_273, transpose_114);  view_273 = transpose_114 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:391 in _merge_heads, code: x = x.view(batch_size, self.num_heads, seq_length, self.head_dim)
            view_275: "f32[1, 32, 6, 160]" = torch.ops.aten.view.default(bmm_22, [1, 32, 6, 160]);  bmm_22 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:392 in _merge_heads, code: x = x.permute(0, 2, 1, 3)
            permute_22: "f32[1, 6, 32, 160]" = torch.ops.aten.permute.default(view_275, [0, 2, 1, 3]);  view_275 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:393 in _merge_heads, code: return x.reshape(batch_size, seq_length, self.num_heads * self.head_dim)
            clone_22: "f32[1, 6, 32, 160]" = torch.ops.aten.clone.default(permute_22, memory_format = torch.contiguous_format);  permute_22 = None
            _unsafe_view_22: "f32[1, 6, 5120]" = torch.ops.aten._unsafe_view.default(clone_22, [1, 6, 5120]);  clone_22 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:478 in forward, code: output_tensor = self.dense(context_layer)
            linear_134: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(_unsafe_view_22, p_h_22_self_attention_dense_weight, p_h_22_self_attention_dense_bias);  _unsafe_view_22 = p_h_22_self_attention_dense_weight = p_h_22_self_attention_dense_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_67: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_134, 0.0, False);  linear_134 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_135: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_131, dropout_67);  add_131 = dropout_67 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_206: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_135, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_69: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_206, 2)
            mean_45: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_69, [-1], True);  pow_69 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_136: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_45, 1e-05);  mean_45 = None
            rsqrt_45: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_136);  add_136 = None
            mul_296: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_206, rsqrt_45);  _to_copy_206 = rsqrt_45 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_207: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_296, dtype = torch.float32);  mul_296 = None
            mul_297: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_22_post_attention_layernorm_weight, _to_copy_207);  p_h_22_post_attention_layernorm_weight = _to_copy_207 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:499 in forward, code: intermediate_output = self.down_proj(F.silu(self.gate_proj(hidden_states)) * self.up_proj(hidden_states))
            linear_135: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_297, p_h_22_mlp_gate_proj_weight);  p_h_22_mlp_gate_proj_weight = None
            silu_22: "f32[1, 6, 12288]" = torch.ops.aten.silu.default(linear_135);  linear_135 = None
            linear_136: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_297, p_h_22_mlp_up_proj_weight);  mul_297 = p_h_22_mlp_up_proj_weight = None
            mul_298: "f32[1, 6, 12288]" = torch.ops.aten.mul.Tensor(silu_22, linear_136);  silu_22 = linear_136 = None
            linear_137: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(mul_298, p_h_22_mlp_down_proj_weight, p_h_22_mlp_down_proj_bias);  mul_298 = p_h_22_mlp_down_proj_weight = p_h_22_mlp_down_proj_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_68: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_137, 0.0, False);  linear_137 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_137: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_135, dropout_68);  add_135 = dropout_68 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_208: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_137, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_70: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_208, 2)
            mean_46: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_70, [-1], True);  pow_70 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_138: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_46, 1e-05);  mean_46 = None
            rsqrt_46: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_138);  add_138 = None
            mul_299: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_208, rsqrt_46);  _to_copy_208 = rsqrt_46 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_209: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_299, dtype = torch.float32);  mul_299 = None
            mul_300: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_23_input_layernorm_weight, _to_copy_209);  p_h_23_input_layernorm_weight = _to_copy_209 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:404 in forward, code: hidden_states = hidden_states.transpose(1, 0)
            transpose_115: "f32[6, 1, 5120]" = torch.ops.aten.transpose.int(mul_300, 1, 0);  mul_300 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:405 in forward, code: query_layer = self.query(hidden_states)
            linear_138: "f32[6, 1, 5120]" = torch.ops.aten.linear.default(transpose_115, p_h_23_self_attention_query_weight);  p_h_23_self_attention_query_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:409 in forward, code: query_layer = query_layer.view(*new_tensor_shape)
            view_276: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(linear_138, [6, 1, 32, 160]);  linear_138 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:411 in forward, code: mixed_kv_layer = self.key_value(hidden_states)
            linear_139: "f32[6, 1, 10240]" = torch.ops.aten.linear.default(transpose_115, p_h_23_self_attention_key_value_weight);  transpose_115 = p_h_23_self_attention_key_value_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:415 in forward, code: mixed_kv_layer = mixed_kv_layer.view(*new_tensor_shape)
            view_277: "f32[6, 1, 32, 320]" = torch.ops.aten.view.default(linear_139, [6, 1, 32, 320]);  linear_139 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:381 in split_tensor_along_last_dim, code: tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)
            split_23 = torch.ops.aten.split.Tensor(view_277, 160, 3);  view_277 = None
            getitem_46: "f32[6, 1, 32, 160]" = split_23[0]
            getitem_47: "f32[6, 1, 32, 160]" = split_23[1];  split_23 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:423 in forward, code: query_layer = query_layer.view(output_size[2], output_size[0] * output_size[1], -1)
            view_278: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_276, [6, 32, -1]);  view_276 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:424 in forward, code: key_layer = key_layer.view(output_size[3], output_size[0] * output_size[1], -1)
            view_279: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_46, [6, 32, -1]);  getitem_46 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:112 in forward, code: self.inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, device=x.device).float() / self.dim))
            arange_47: "i64[80]" = torch.ops.aten.arange.start_step(0, 160, 2, device = device(type='cpu'), pin_memory = False)
            _to_copy_210: "f32[80]" = torch.ops.aten._to_copy.default(arange_47, dtype = torch.float32);  arange_47 = None
            div_23: "f32[80]" = torch.ops.aten.div.Tensor(_to_copy_210, 160);  _to_copy_210 = None
            pow_71: "f32[80]" = torch.ops.aten.pow.Scalar(30420.108888514722, div_23);  div_23 = None
            reciprocal_23: "f32[80]" = torch.ops.aten.reciprocal.default(pow_71);  pow_71 = None
            mul_301: "f32[80]" = torch.ops.aten.mul.Tensor(reciprocal_23, 1.0);  reciprocal_23 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:114 in forward, code: t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)
            arange_48: "f32[8192]" = torch.ops.aten.arange.default(8192, dtype = torch.float32, device = device(type='cpu'), pin_memory = False)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:115 in forward, code: freqs = torch.einsum('i,j->ij', t, self.inv_freq)
            einsum_46: "f32[8192, 80]" = torch.ops.aten.einsum.default('i,j->ij', [arange_48, mul_301]);  arange_48 = mul_301 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:117 in forward, code: emb = torch.cat((freqs, freqs), dim=-1).to(x.device)
            cat_69: "f32[8192, 160]" = torch.ops.aten.cat.default([einsum_46, einsum_46], -1);  einsum_46 = None
            _to_copy_211: "f32[8192, 160]" = torch.ops.aten._to_copy.default(cat_69, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  cat_69 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:121 in forward, code: self.cos_cached = self.mscale * emb.cos()[:, None, :].to(dtype)
            cos_23: "f32[8192, 160]" = torch.ops.aten.cos.default(_to_copy_211)
            slice_244: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(cos_23, 0, 0, 9223372036854775807);  cos_23 = None
            unsqueeze_54: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_244, 1);  slice_244 = None
            slice_245: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_54, 2, 0, 9223372036854775807);  unsqueeze_54 = None
            _to_copy_212: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_245, dtype = torch.float32);  slice_245 = None
            mul_302: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_212, 1.0);  _to_copy_212 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:122 in forward, code: self.sin_cached = self.mscale * emb.sin()[:, None, :].to(dtype)
            sin_23: "f32[8192, 160]" = torch.ops.aten.sin.default(_to_copy_211);  _to_copy_211 = None
            slice_246: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(sin_23, 0, 0, 9223372036854775807);  sin_23 = None
            unsqueeze_55: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_246, 1);  slice_246 = None
            slice_247: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_55, 2, 0, 9223372036854775807);  unsqueeze_55 = None
            _to_copy_213: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_247, dtype = torch.float32);  slice_247 = None
            mul_303: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_213, 1.0);  _to_copy_213 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:123 in forward, code: return self.cos_cached[:seq_len, ...], self.sin_cached[:seq_len, ...]
            alias_46: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_302);  mul_302 = None
            alias_47: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_303);  mul_303 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:133 in apply_rotary_pos_emb_torch, code: cos, sin = cos[offset:q.shape[0] + offset, ...], sin[offset:q.shape[0] + offset, ...]
            slice_248: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_46, 0, 0, 6);  alias_46 = None
            slice_249: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_47, 0, 0, 6);  alias_47 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_304: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_278, slice_248)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_250: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_278, 2, 0, 80)
            slice_251: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_278, 2, 80, 9223372036854775807);  view_278 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_46: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_251);  slice_251 = None
            cat_70: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_46, slice_250], 2);  neg_46 = slice_250 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_305: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_70, slice_249);  cat_70 = None
            add_139: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_304, mul_305);  mul_304 = mul_305 = None
            mul_306: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_279, slice_248);  slice_248 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_252: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_279, 2, 0, 80)
            slice_253: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_279, 2, 80, 9223372036854775807);  view_279 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_47: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_253);  slice_253 = None
            cat_71: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_47, slice_252], 2);  neg_47 = slice_252 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_307: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_71, slice_249);  cat_71 = slice_249 = None
            add_140: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_306, mul_307);  mul_306 = mul_307 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:448 in forward, code: query_layer = query_layer.reshape((s_query, bz, head, dim))
            view_280: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_139, [6, 1, 32, 160]);  add_139 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:449 in forward, code: key_layer = key_layer.reshape((s_key, bz, head, dim))
            view_281: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_140, [6, 1, 32, 160]);  add_140 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:458 in forward, code: query_layer = query_layer.reshape(s_query, bz * self.num_heads, dim)
            view_282: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_280, [6, 32, 160]);  view_280 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:460 in forward, code: key_layer = key_layer.reshape(s_key, bz * self.num_heads, dim)
            view_283: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_281, [6, 32, 160]);  view_281 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            transpose_116: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_282, 0, 1);  view_282 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:462 in forward, code: key_layer.transpose(0, 1).transpose(1, 2))
            transpose_117: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_283, 0, 1);  view_283 = None
            transpose_118: "f32[32, 160, 6]" = torch.ops.aten.transpose.int(transpose_117, 1, 2);  transpose_117 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            einsum_47: "f32[32, 6, 6]" = torch.ops.aten.einsum.default('bik,bkj->bij', [transpose_116, transpose_118]);  transpose_116 = transpose_118 = None
            mul_308: "f32[32, 6, 6]" = torch.ops.aten.mul.Tensor(einsum_47, 0.07905694150420949);  einsum_47 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:464 in forward, code: attention_scores = matmul_result.view(bz, self.num_heads, s_query, s_key)
            view_284: "f32[1, 32, 6, 6]" = torch.ops.aten.view.default(mul_308, [1, 32, 6, 6]);  mul_308 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:469 in forward, code: attn_weights = torch.masked_fill(attention_scores, attention_mask, torch.finfo(attention_scores.dtype).min)
            masked_fill_23: "f32[1, 32, 6, 6]" = torch.ops.aten.masked_fill.Scalar(view_284, or_1, -3.4028234663852886e+38);  view_284 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:470 in forward, code: attention_probs = F.softmax(attn_weights, dim=-1).to(input_dtype)  ##dtype = torch.float32
            softmax_23: "f32[1, 32, 6, 6]" = torch.ops.aten.softmax.int(masked_fill_23, -1);  masked_fill_23 = None
            _to_copy_214: "f32[1, 32, 6, 6]" = torch.ops.aten._to_copy.default(softmax_23, dtype = torch.float32);  softmax_23 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:471 in forward, code: attention_probs = self.attention_dropout(attention_probs)
            dropout_69: "f32[1, 32, 6, 6]" = torch.ops.aten.dropout.default(_to_copy_214, 0.0, False);  _to_copy_214 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:472 in forward, code: attention_probs_reshaped = attention_probs.view(bz * self.num_heads, s_query, s_key)
            view_285: "f32[32, 6, 6]" = torch.ops.aten.view.default(dropout_69, [32, 6, 6]);  dropout_69 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:474 in forward, code: value_layer = value_layer.reshape(s_key, bz * self.num_heads, dim)
            view_286: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_47, [6, 32, 160]);  getitem_47 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:475 in forward, code: context_layer = torch.bmm(attention_probs_reshaped, value_layer.transpose(0, 1))
            transpose_119: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_286, 0, 1);  view_286 = None
            bmm_23: "f32[32, 6, 160]" = torch.ops.aten.bmm.default(view_285, transpose_119);  view_285 = transpose_119 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:391 in _merge_heads, code: x = x.view(batch_size, self.num_heads, seq_length, self.head_dim)
            view_287: "f32[1, 32, 6, 160]" = torch.ops.aten.view.default(bmm_23, [1, 32, 6, 160]);  bmm_23 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:392 in _merge_heads, code: x = x.permute(0, 2, 1, 3)
            permute_23: "f32[1, 6, 32, 160]" = torch.ops.aten.permute.default(view_287, [0, 2, 1, 3]);  view_287 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:393 in _merge_heads, code: return x.reshape(batch_size, seq_length, self.num_heads * self.head_dim)
            clone_23: "f32[1, 6, 32, 160]" = torch.ops.aten.clone.default(permute_23, memory_format = torch.contiguous_format);  permute_23 = None
            _unsafe_view_23: "f32[1, 6, 5120]" = torch.ops.aten._unsafe_view.default(clone_23, [1, 6, 5120]);  clone_23 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:478 in forward, code: output_tensor = self.dense(context_layer)
            linear_140: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(_unsafe_view_23, p_h_23_self_attention_dense_weight, p_h_23_self_attention_dense_bias);  _unsafe_view_23 = p_h_23_self_attention_dense_weight = p_h_23_self_attention_dense_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_70: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_140, 0.0, False);  linear_140 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_141: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_137, dropout_70);  add_137 = dropout_70 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_215: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_141, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_72: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_215, 2)
            mean_47: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_72, [-1], True);  pow_72 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_142: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_47, 1e-05);  mean_47 = None
            rsqrt_47: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_142);  add_142 = None
            mul_309: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_215, rsqrt_47);  _to_copy_215 = rsqrt_47 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_216: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_309, dtype = torch.float32);  mul_309 = None
            mul_310: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_23_post_attention_layernorm_weight, _to_copy_216);  p_h_23_post_attention_layernorm_weight = _to_copy_216 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:499 in forward, code: intermediate_output = self.down_proj(F.silu(self.gate_proj(hidden_states)) * self.up_proj(hidden_states))
            linear_141: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_310, p_h_23_mlp_gate_proj_weight);  p_h_23_mlp_gate_proj_weight = None
            silu_23: "f32[1, 6, 12288]" = torch.ops.aten.silu.default(linear_141);  linear_141 = None
            linear_142: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_310, p_h_23_mlp_up_proj_weight);  mul_310 = p_h_23_mlp_up_proj_weight = None
            mul_311: "f32[1, 6, 12288]" = torch.ops.aten.mul.Tensor(silu_23, linear_142);  silu_23 = linear_142 = None
            linear_143: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(mul_311, p_h_23_mlp_down_proj_weight, p_h_23_mlp_down_proj_bias);  mul_311 = p_h_23_mlp_down_proj_weight = p_h_23_mlp_down_proj_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_71: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_143, 0.0, False);  linear_143 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_143: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_141, dropout_71);  add_141 = dropout_71 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_217: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_143, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_73: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_217, 2)
            mean_48: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_73, [-1], True);  pow_73 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_144: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_48, 1e-05);  mean_48 = None
            rsqrt_48: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_144);  add_144 = None
            mul_312: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_217, rsqrt_48);  _to_copy_217 = rsqrt_48 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_218: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_312, dtype = torch.float32);  mul_312 = None
            mul_313: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_24_input_layernorm_weight, _to_copy_218);  p_h_24_input_layernorm_weight = _to_copy_218 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:404 in forward, code: hidden_states = hidden_states.transpose(1, 0)
            transpose_120: "f32[6, 1, 5120]" = torch.ops.aten.transpose.int(mul_313, 1, 0);  mul_313 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:405 in forward, code: query_layer = self.query(hidden_states)
            linear_144: "f32[6, 1, 5120]" = torch.ops.aten.linear.default(transpose_120, p_h_24_self_attention_query_weight);  p_h_24_self_attention_query_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:409 in forward, code: query_layer = query_layer.view(*new_tensor_shape)
            view_288: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(linear_144, [6, 1, 32, 160]);  linear_144 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:411 in forward, code: mixed_kv_layer = self.key_value(hidden_states)
            linear_145: "f32[6, 1, 10240]" = torch.ops.aten.linear.default(transpose_120, p_h_24_self_attention_key_value_weight);  transpose_120 = p_h_24_self_attention_key_value_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:415 in forward, code: mixed_kv_layer = mixed_kv_layer.view(*new_tensor_shape)
            view_289: "f32[6, 1, 32, 320]" = torch.ops.aten.view.default(linear_145, [6, 1, 32, 320]);  linear_145 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:381 in split_tensor_along_last_dim, code: tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)
            split_24 = torch.ops.aten.split.Tensor(view_289, 160, 3);  view_289 = None
            getitem_48: "f32[6, 1, 32, 160]" = split_24[0]
            getitem_49: "f32[6, 1, 32, 160]" = split_24[1];  split_24 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:423 in forward, code: query_layer = query_layer.view(output_size[2], output_size[0] * output_size[1], -1)
            view_290: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_288, [6, 32, -1]);  view_288 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:424 in forward, code: key_layer = key_layer.view(output_size[3], output_size[0] * output_size[1], -1)
            view_291: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_48, [6, 32, -1]);  getitem_48 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:112 in forward, code: self.inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, device=x.device).float() / self.dim))
            arange_49: "i64[80]" = torch.ops.aten.arange.start_step(0, 160, 2, device = device(type='cpu'), pin_memory = False)
            _to_copy_219: "f32[80]" = torch.ops.aten._to_copy.default(arange_49, dtype = torch.float32);  arange_49 = None
            div_24: "f32[80]" = torch.ops.aten.div.Tensor(_to_copy_219, 160);  _to_copy_219 = None
            pow_74: "f32[80]" = torch.ops.aten.pow.Scalar(30420.108888514722, div_24);  div_24 = None
            reciprocal_24: "f32[80]" = torch.ops.aten.reciprocal.default(pow_74);  pow_74 = None
            mul_314: "f32[80]" = torch.ops.aten.mul.Tensor(reciprocal_24, 1.0);  reciprocal_24 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:114 in forward, code: t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)
            arange_50: "f32[8192]" = torch.ops.aten.arange.default(8192, dtype = torch.float32, device = device(type='cpu'), pin_memory = False)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:115 in forward, code: freqs = torch.einsum('i,j->ij', t, self.inv_freq)
            einsum_48: "f32[8192, 80]" = torch.ops.aten.einsum.default('i,j->ij', [arange_50, mul_314]);  arange_50 = mul_314 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:117 in forward, code: emb = torch.cat((freqs, freqs), dim=-1).to(x.device)
            cat_72: "f32[8192, 160]" = torch.ops.aten.cat.default([einsum_48, einsum_48], -1);  einsum_48 = None
            _to_copy_220: "f32[8192, 160]" = torch.ops.aten._to_copy.default(cat_72, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  cat_72 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:121 in forward, code: self.cos_cached = self.mscale * emb.cos()[:, None, :].to(dtype)
            cos_24: "f32[8192, 160]" = torch.ops.aten.cos.default(_to_copy_220)
            slice_254: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(cos_24, 0, 0, 9223372036854775807);  cos_24 = None
            unsqueeze_56: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_254, 1);  slice_254 = None
            slice_255: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_56, 2, 0, 9223372036854775807);  unsqueeze_56 = None
            _to_copy_221: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_255, dtype = torch.float32);  slice_255 = None
            mul_315: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_221, 1.0);  _to_copy_221 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:122 in forward, code: self.sin_cached = self.mscale * emb.sin()[:, None, :].to(dtype)
            sin_24: "f32[8192, 160]" = torch.ops.aten.sin.default(_to_copy_220);  _to_copy_220 = None
            slice_256: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(sin_24, 0, 0, 9223372036854775807);  sin_24 = None
            unsqueeze_57: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_256, 1);  slice_256 = None
            slice_257: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_57, 2, 0, 9223372036854775807);  unsqueeze_57 = None
            _to_copy_222: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_257, dtype = torch.float32);  slice_257 = None
            mul_316: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_222, 1.0);  _to_copy_222 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:123 in forward, code: return self.cos_cached[:seq_len, ...], self.sin_cached[:seq_len, ...]
            alias_48: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_315);  mul_315 = None
            alias_49: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_316);  mul_316 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:133 in apply_rotary_pos_emb_torch, code: cos, sin = cos[offset:q.shape[0] + offset, ...], sin[offset:q.shape[0] + offset, ...]
            slice_258: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_48, 0, 0, 6);  alias_48 = None
            slice_259: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_49, 0, 0, 6);  alias_49 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_317: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_290, slice_258)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_260: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_290, 2, 0, 80)
            slice_261: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_290, 2, 80, 9223372036854775807);  view_290 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_48: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_261);  slice_261 = None
            cat_73: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_48, slice_260], 2);  neg_48 = slice_260 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_318: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_73, slice_259);  cat_73 = None
            add_145: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_317, mul_318);  mul_317 = mul_318 = None
            mul_319: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_291, slice_258);  slice_258 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_262: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_291, 2, 0, 80)
            slice_263: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_291, 2, 80, 9223372036854775807);  view_291 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_49: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_263);  slice_263 = None
            cat_74: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_49, slice_262], 2);  neg_49 = slice_262 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_320: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_74, slice_259);  cat_74 = slice_259 = None
            add_146: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_319, mul_320);  mul_319 = mul_320 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:448 in forward, code: query_layer = query_layer.reshape((s_query, bz, head, dim))
            view_292: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_145, [6, 1, 32, 160]);  add_145 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:449 in forward, code: key_layer = key_layer.reshape((s_key, bz, head, dim))
            view_293: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_146, [6, 1, 32, 160]);  add_146 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:458 in forward, code: query_layer = query_layer.reshape(s_query, bz * self.num_heads, dim)
            view_294: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_292, [6, 32, 160]);  view_292 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:460 in forward, code: key_layer = key_layer.reshape(s_key, bz * self.num_heads, dim)
            view_295: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_293, [6, 32, 160]);  view_293 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            transpose_121: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_294, 0, 1);  view_294 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:462 in forward, code: key_layer.transpose(0, 1).transpose(1, 2))
            transpose_122: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_295, 0, 1);  view_295 = None
            transpose_123: "f32[32, 160, 6]" = torch.ops.aten.transpose.int(transpose_122, 1, 2);  transpose_122 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            einsum_49: "f32[32, 6, 6]" = torch.ops.aten.einsum.default('bik,bkj->bij', [transpose_121, transpose_123]);  transpose_121 = transpose_123 = None
            mul_321: "f32[32, 6, 6]" = torch.ops.aten.mul.Tensor(einsum_49, 0.07905694150420949);  einsum_49 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:464 in forward, code: attention_scores = matmul_result.view(bz, self.num_heads, s_query, s_key)
            view_296: "f32[1, 32, 6, 6]" = torch.ops.aten.view.default(mul_321, [1, 32, 6, 6]);  mul_321 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:469 in forward, code: attn_weights = torch.masked_fill(attention_scores, attention_mask, torch.finfo(attention_scores.dtype).min)
            masked_fill_24: "f32[1, 32, 6, 6]" = torch.ops.aten.masked_fill.Scalar(view_296, or_1, -3.4028234663852886e+38);  view_296 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:470 in forward, code: attention_probs = F.softmax(attn_weights, dim=-1).to(input_dtype)  ##dtype = torch.float32
            softmax_24: "f32[1, 32, 6, 6]" = torch.ops.aten.softmax.int(masked_fill_24, -1);  masked_fill_24 = None
            _to_copy_223: "f32[1, 32, 6, 6]" = torch.ops.aten._to_copy.default(softmax_24, dtype = torch.float32);  softmax_24 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:471 in forward, code: attention_probs = self.attention_dropout(attention_probs)
            dropout_72: "f32[1, 32, 6, 6]" = torch.ops.aten.dropout.default(_to_copy_223, 0.0, False);  _to_copy_223 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:472 in forward, code: attention_probs_reshaped = attention_probs.view(bz * self.num_heads, s_query, s_key)
            view_297: "f32[32, 6, 6]" = torch.ops.aten.view.default(dropout_72, [32, 6, 6]);  dropout_72 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:474 in forward, code: value_layer = value_layer.reshape(s_key, bz * self.num_heads, dim)
            view_298: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_49, [6, 32, 160]);  getitem_49 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:475 in forward, code: context_layer = torch.bmm(attention_probs_reshaped, value_layer.transpose(0, 1))
            transpose_124: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_298, 0, 1);  view_298 = None
            bmm_24: "f32[32, 6, 160]" = torch.ops.aten.bmm.default(view_297, transpose_124);  view_297 = transpose_124 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:391 in _merge_heads, code: x = x.view(batch_size, self.num_heads, seq_length, self.head_dim)
            view_299: "f32[1, 32, 6, 160]" = torch.ops.aten.view.default(bmm_24, [1, 32, 6, 160]);  bmm_24 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:392 in _merge_heads, code: x = x.permute(0, 2, 1, 3)
            permute_24: "f32[1, 6, 32, 160]" = torch.ops.aten.permute.default(view_299, [0, 2, 1, 3]);  view_299 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:393 in _merge_heads, code: return x.reshape(batch_size, seq_length, self.num_heads * self.head_dim)
            clone_24: "f32[1, 6, 32, 160]" = torch.ops.aten.clone.default(permute_24, memory_format = torch.contiguous_format);  permute_24 = None
            _unsafe_view_24: "f32[1, 6, 5120]" = torch.ops.aten._unsafe_view.default(clone_24, [1, 6, 5120]);  clone_24 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:478 in forward, code: output_tensor = self.dense(context_layer)
            linear_146: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(_unsafe_view_24, p_h_24_self_attention_dense_weight, p_h_24_self_attention_dense_bias);  _unsafe_view_24 = p_h_24_self_attention_dense_weight = p_h_24_self_attention_dense_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_73: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_146, 0.0, False);  linear_146 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_147: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_143, dropout_73);  add_143 = dropout_73 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_224: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_147, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_75: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_224, 2)
            mean_49: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_75, [-1], True);  pow_75 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_148: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_49, 1e-05);  mean_49 = None
            rsqrt_49: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_148);  add_148 = None
            mul_322: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_224, rsqrt_49);  _to_copy_224 = rsqrt_49 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_225: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_322, dtype = torch.float32);  mul_322 = None
            mul_323: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_24_post_attention_layernorm_weight, _to_copy_225);  p_h_24_post_attention_layernorm_weight = _to_copy_225 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:499 in forward, code: intermediate_output = self.down_proj(F.silu(self.gate_proj(hidden_states)) * self.up_proj(hidden_states))
            linear_147: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_323, p_h_24_mlp_gate_proj_weight);  p_h_24_mlp_gate_proj_weight = None
            silu_24: "f32[1, 6, 12288]" = torch.ops.aten.silu.default(linear_147);  linear_147 = None
            linear_148: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_323, p_h_24_mlp_up_proj_weight);  mul_323 = p_h_24_mlp_up_proj_weight = None
            mul_324: "f32[1, 6, 12288]" = torch.ops.aten.mul.Tensor(silu_24, linear_148);  silu_24 = linear_148 = None
            linear_149: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(mul_324, p_h_24_mlp_down_proj_weight, p_h_24_mlp_down_proj_bias);  mul_324 = p_h_24_mlp_down_proj_weight = p_h_24_mlp_down_proj_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_74: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_149, 0.0, False);  linear_149 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_149: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_147, dropout_74);  add_147 = dropout_74 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_226: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_149, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_76: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_226, 2)
            mean_50: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_76, [-1], True);  pow_76 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_150: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_50, 1e-05);  mean_50 = None
            rsqrt_50: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_150);  add_150 = None
            mul_325: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_226, rsqrt_50);  _to_copy_226 = rsqrt_50 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_227: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_325, dtype = torch.float32);  mul_325 = None
            mul_326: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_25_input_layernorm_weight, _to_copy_227);  p_h_25_input_layernorm_weight = _to_copy_227 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:404 in forward, code: hidden_states = hidden_states.transpose(1, 0)
            transpose_125: "f32[6, 1, 5120]" = torch.ops.aten.transpose.int(mul_326, 1, 0);  mul_326 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:405 in forward, code: query_layer = self.query(hidden_states)
            linear_150: "f32[6, 1, 5120]" = torch.ops.aten.linear.default(transpose_125, p_h_25_self_attention_query_weight);  p_h_25_self_attention_query_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:409 in forward, code: query_layer = query_layer.view(*new_tensor_shape)
            view_300: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(linear_150, [6, 1, 32, 160]);  linear_150 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:411 in forward, code: mixed_kv_layer = self.key_value(hidden_states)
            linear_151: "f32[6, 1, 10240]" = torch.ops.aten.linear.default(transpose_125, p_h_25_self_attention_key_value_weight);  transpose_125 = p_h_25_self_attention_key_value_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:415 in forward, code: mixed_kv_layer = mixed_kv_layer.view(*new_tensor_shape)
            view_301: "f32[6, 1, 32, 320]" = torch.ops.aten.view.default(linear_151, [6, 1, 32, 320]);  linear_151 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:381 in split_tensor_along_last_dim, code: tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)
            split_25 = torch.ops.aten.split.Tensor(view_301, 160, 3);  view_301 = None
            getitem_50: "f32[6, 1, 32, 160]" = split_25[0]
            getitem_51: "f32[6, 1, 32, 160]" = split_25[1];  split_25 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:423 in forward, code: query_layer = query_layer.view(output_size[2], output_size[0] * output_size[1], -1)
            view_302: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_300, [6, 32, -1]);  view_300 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:424 in forward, code: key_layer = key_layer.view(output_size[3], output_size[0] * output_size[1], -1)
            view_303: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_50, [6, 32, -1]);  getitem_50 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:112 in forward, code: self.inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, device=x.device).float() / self.dim))
            arange_51: "i64[80]" = torch.ops.aten.arange.start_step(0, 160, 2, device = device(type='cpu'), pin_memory = False)
            _to_copy_228: "f32[80]" = torch.ops.aten._to_copy.default(arange_51, dtype = torch.float32);  arange_51 = None
            div_25: "f32[80]" = torch.ops.aten.div.Tensor(_to_copy_228, 160);  _to_copy_228 = None
            pow_77: "f32[80]" = torch.ops.aten.pow.Scalar(30420.108888514722, div_25);  div_25 = None
            reciprocal_25: "f32[80]" = torch.ops.aten.reciprocal.default(pow_77);  pow_77 = None
            mul_327: "f32[80]" = torch.ops.aten.mul.Tensor(reciprocal_25, 1.0);  reciprocal_25 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:114 in forward, code: t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)
            arange_52: "f32[8192]" = torch.ops.aten.arange.default(8192, dtype = torch.float32, device = device(type='cpu'), pin_memory = False)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:115 in forward, code: freqs = torch.einsum('i,j->ij', t, self.inv_freq)
            einsum_50: "f32[8192, 80]" = torch.ops.aten.einsum.default('i,j->ij', [arange_52, mul_327]);  arange_52 = mul_327 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:117 in forward, code: emb = torch.cat((freqs, freqs), dim=-1).to(x.device)
            cat_75: "f32[8192, 160]" = torch.ops.aten.cat.default([einsum_50, einsum_50], -1);  einsum_50 = None
            _to_copy_229: "f32[8192, 160]" = torch.ops.aten._to_copy.default(cat_75, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  cat_75 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:121 in forward, code: self.cos_cached = self.mscale * emb.cos()[:, None, :].to(dtype)
            cos_25: "f32[8192, 160]" = torch.ops.aten.cos.default(_to_copy_229)
            slice_264: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(cos_25, 0, 0, 9223372036854775807);  cos_25 = None
            unsqueeze_58: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_264, 1);  slice_264 = None
            slice_265: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_58, 2, 0, 9223372036854775807);  unsqueeze_58 = None
            _to_copy_230: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_265, dtype = torch.float32);  slice_265 = None
            mul_328: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_230, 1.0);  _to_copy_230 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:122 in forward, code: self.sin_cached = self.mscale * emb.sin()[:, None, :].to(dtype)
            sin_25: "f32[8192, 160]" = torch.ops.aten.sin.default(_to_copy_229);  _to_copy_229 = None
            slice_266: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(sin_25, 0, 0, 9223372036854775807);  sin_25 = None
            unsqueeze_59: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_266, 1);  slice_266 = None
            slice_267: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_59, 2, 0, 9223372036854775807);  unsqueeze_59 = None
            _to_copy_231: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_267, dtype = torch.float32);  slice_267 = None
            mul_329: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_231, 1.0);  _to_copy_231 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:123 in forward, code: return self.cos_cached[:seq_len, ...], self.sin_cached[:seq_len, ...]
            alias_50: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_328);  mul_328 = None
            alias_51: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_329);  mul_329 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:133 in apply_rotary_pos_emb_torch, code: cos, sin = cos[offset:q.shape[0] + offset, ...], sin[offset:q.shape[0] + offset, ...]
            slice_268: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_50, 0, 0, 6);  alias_50 = None
            slice_269: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_51, 0, 0, 6);  alias_51 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_330: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_302, slice_268)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_270: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_302, 2, 0, 80)
            slice_271: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_302, 2, 80, 9223372036854775807);  view_302 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_50: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_271);  slice_271 = None
            cat_76: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_50, slice_270], 2);  neg_50 = slice_270 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_331: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_76, slice_269);  cat_76 = None
            add_151: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_330, mul_331);  mul_330 = mul_331 = None
            mul_332: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_303, slice_268);  slice_268 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_272: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_303, 2, 0, 80)
            slice_273: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_303, 2, 80, 9223372036854775807);  view_303 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_51: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_273);  slice_273 = None
            cat_77: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_51, slice_272], 2);  neg_51 = slice_272 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_333: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_77, slice_269);  cat_77 = slice_269 = None
            add_152: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_332, mul_333);  mul_332 = mul_333 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:448 in forward, code: query_layer = query_layer.reshape((s_query, bz, head, dim))
            view_304: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_151, [6, 1, 32, 160]);  add_151 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:449 in forward, code: key_layer = key_layer.reshape((s_key, bz, head, dim))
            view_305: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_152, [6, 1, 32, 160]);  add_152 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:458 in forward, code: query_layer = query_layer.reshape(s_query, bz * self.num_heads, dim)
            view_306: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_304, [6, 32, 160]);  view_304 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:460 in forward, code: key_layer = key_layer.reshape(s_key, bz * self.num_heads, dim)
            view_307: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_305, [6, 32, 160]);  view_305 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            transpose_126: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_306, 0, 1);  view_306 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:462 in forward, code: key_layer.transpose(0, 1).transpose(1, 2))
            transpose_127: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_307, 0, 1);  view_307 = None
            transpose_128: "f32[32, 160, 6]" = torch.ops.aten.transpose.int(transpose_127, 1, 2);  transpose_127 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            einsum_51: "f32[32, 6, 6]" = torch.ops.aten.einsum.default('bik,bkj->bij', [transpose_126, transpose_128]);  transpose_126 = transpose_128 = None
            mul_334: "f32[32, 6, 6]" = torch.ops.aten.mul.Tensor(einsum_51, 0.07905694150420949);  einsum_51 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:464 in forward, code: attention_scores = matmul_result.view(bz, self.num_heads, s_query, s_key)
            view_308: "f32[1, 32, 6, 6]" = torch.ops.aten.view.default(mul_334, [1, 32, 6, 6]);  mul_334 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:469 in forward, code: attn_weights = torch.masked_fill(attention_scores, attention_mask, torch.finfo(attention_scores.dtype).min)
            masked_fill_25: "f32[1, 32, 6, 6]" = torch.ops.aten.masked_fill.Scalar(view_308, or_1, -3.4028234663852886e+38);  view_308 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:470 in forward, code: attention_probs = F.softmax(attn_weights, dim=-1).to(input_dtype)  ##dtype = torch.float32
            softmax_25: "f32[1, 32, 6, 6]" = torch.ops.aten.softmax.int(masked_fill_25, -1);  masked_fill_25 = None
            _to_copy_232: "f32[1, 32, 6, 6]" = torch.ops.aten._to_copy.default(softmax_25, dtype = torch.float32);  softmax_25 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:471 in forward, code: attention_probs = self.attention_dropout(attention_probs)
            dropout_75: "f32[1, 32, 6, 6]" = torch.ops.aten.dropout.default(_to_copy_232, 0.0, False);  _to_copy_232 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:472 in forward, code: attention_probs_reshaped = attention_probs.view(bz * self.num_heads, s_query, s_key)
            view_309: "f32[32, 6, 6]" = torch.ops.aten.view.default(dropout_75, [32, 6, 6]);  dropout_75 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:474 in forward, code: value_layer = value_layer.reshape(s_key, bz * self.num_heads, dim)
            view_310: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_51, [6, 32, 160]);  getitem_51 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:475 in forward, code: context_layer = torch.bmm(attention_probs_reshaped, value_layer.transpose(0, 1))
            transpose_129: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_310, 0, 1);  view_310 = None
            bmm_25: "f32[32, 6, 160]" = torch.ops.aten.bmm.default(view_309, transpose_129);  view_309 = transpose_129 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:391 in _merge_heads, code: x = x.view(batch_size, self.num_heads, seq_length, self.head_dim)
            view_311: "f32[1, 32, 6, 160]" = torch.ops.aten.view.default(bmm_25, [1, 32, 6, 160]);  bmm_25 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:392 in _merge_heads, code: x = x.permute(0, 2, 1, 3)
            permute_25: "f32[1, 6, 32, 160]" = torch.ops.aten.permute.default(view_311, [0, 2, 1, 3]);  view_311 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:393 in _merge_heads, code: return x.reshape(batch_size, seq_length, self.num_heads * self.head_dim)
            clone_25: "f32[1, 6, 32, 160]" = torch.ops.aten.clone.default(permute_25, memory_format = torch.contiguous_format);  permute_25 = None
            _unsafe_view_25: "f32[1, 6, 5120]" = torch.ops.aten._unsafe_view.default(clone_25, [1, 6, 5120]);  clone_25 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:478 in forward, code: output_tensor = self.dense(context_layer)
            linear_152: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(_unsafe_view_25, p_h_25_self_attention_dense_weight, p_h_25_self_attention_dense_bias);  _unsafe_view_25 = p_h_25_self_attention_dense_weight = p_h_25_self_attention_dense_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_76: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_152, 0.0, False);  linear_152 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_153: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_149, dropout_76);  add_149 = dropout_76 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_233: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_153, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_78: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_233, 2)
            mean_51: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_78, [-1], True);  pow_78 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_154: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_51, 1e-05);  mean_51 = None
            rsqrt_51: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_154);  add_154 = None
            mul_335: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_233, rsqrt_51);  _to_copy_233 = rsqrt_51 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_234: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_335, dtype = torch.float32);  mul_335 = None
            mul_336: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_25_post_attention_layernorm_weight, _to_copy_234);  p_h_25_post_attention_layernorm_weight = _to_copy_234 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:499 in forward, code: intermediate_output = self.down_proj(F.silu(self.gate_proj(hidden_states)) * self.up_proj(hidden_states))
            linear_153: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_336, p_h_25_mlp_gate_proj_weight);  p_h_25_mlp_gate_proj_weight = None
            silu_25: "f32[1, 6, 12288]" = torch.ops.aten.silu.default(linear_153);  linear_153 = None
            linear_154: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_336, p_h_25_mlp_up_proj_weight);  mul_336 = p_h_25_mlp_up_proj_weight = None
            mul_337: "f32[1, 6, 12288]" = torch.ops.aten.mul.Tensor(silu_25, linear_154);  silu_25 = linear_154 = None
            linear_155: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(mul_337, p_h_25_mlp_down_proj_weight, p_h_25_mlp_down_proj_bias);  mul_337 = p_h_25_mlp_down_proj_weight = p_h_25_mlp_down_proj_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_77: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_155, 0.0, False);  linear_155 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_155: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_153, dropout_77);  add_153 = dropout_77 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_235: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_155, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_79: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_235, 2)
            mean_52: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_79, [-1], True);  pow_79 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_156: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_52, 1e-05);  mean_52 = None
            rsqrt_52: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_156);  add_156 = None
            mul_338: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_235, rsqrt_52);  _to_copy_235 = rsqrt_52 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_236: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_338, dtype = torch.float32);  mul_338 = None
            mul_339: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_26_input_layernorm_weight, _to_copy_236);  p_h_26_input_layernorm_weight = _to_copy_236 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:404 in forward, code: hidden_states = hidden_states.transpose(1, 0)
            transpose_130: "f32[6, 1, 5120]" = torch.ops.aten.transpose.int(mul_339, 1, 0);  mul_339 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:405 in forward, code: query_layer = self.query(hidden_states)
            linear_156: "f32[6, 1, 5120]" = torch.ops.aten.linear.default(transpose_130, p_h_26_self_attention_query_weight);  p_h_26_self_attention_query_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:409 in forward, code: query_layer = query_layer.view(*new_tensor_shape)
            view_312: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(linear_156, [6, 1, 32, 160]);  linear_156 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:411 in forward, code: mixed_kv_layer = self.key_value(hidden_states)
            linear_157: "f32[6, 1, 10240]" = torch.ops.aten.linear.default(transpose_130, p_h_26_self_attention_key_value_weight);  transpose_130 = p_h_26_self_attention_key_value_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:415 in forward, code: mixed_kv_layer = mixed_kv_layer.view(*new_tensor_shape)
            view_313: "f32[6, 1, 32, 320]" = torch.ops.aten.view.default(linear_157, [6, 1, 32, 320]);  linear_157 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:381 in split_tensor_along_last_dim, code: tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)
            split_26 = torch.ops.aten.split.Tensor(view_313, 160, 3);  view_313 = None
            getitem_52: "f32[6, 1, 32, 160]" = split_26[0]
            getitem_53: "f32[6, 1, 32, 160]" = split_26[1];  split_26 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:423 in forward, code: query_layer = query_layer.view(output_size[2], output_size[0] * output_size[1], -1)
            view_314: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_312, [6, 32, -1]);  view_312 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:424 in forward, code: key_layer = key_layer.view(output_size[3], output_size[0] * output_size[1], -1)
            view_315: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_52, [6, 32, -1]);  getitem_52 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:112 in forward, code: self.inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, device=x.device).float() / self.dim))
            arange_53: "i64[80]" = torch.ops.aten.arange.start_step(0, 160, 2, device = device(type='cpu'), pin_memory = False)
            _to_copy_237: "f32[80]" = torch.ops.aten._to_copy.default(arange_53, dtype = torch.float32);  arange_53 = None
            div_26: "f32[80]" = torch.ops.aten.div.Tensor(_to_copy_237, 160);  _to_copy_237 = None
            pow_80: "f32[80]" = torch.ops.aten.pow.Scalar(30420.108888514722, div_26);  div_26 = None
            reciprocal_26: "f32[80]" = torch.ops.aten.reciprocal.default(pow_80);  pow_80 = None
            mul_340: "f32[80]" = torch.ops.aten.mul.Tensor(reciprocal_26, 1.0);  reciprocal_26 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:114 in forward, code: t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)
            arange_54: "f32[8192]" = torch.ops.aten.arange.default(8192, dtype = torch.float32, device = device(type='cpu'), pin_memory = False)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:115 in forward, code: freqs = torch.einsum('i,j->ij', t, self.inv_freq)
            einsum_52: "f32[8192, 80]" = torch.ops.aten.einsum.default('i,j->ij', [arange_54, mul_340]);  arange_54 = mul_340 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:117 in forward, code: emb = torch.cat((freqs, freqs), dim=-1).to(x.device)
            cat_78: "f32[8192, 160]" = torch.ops.aten.cat.default([einsum_52, einsum_52], -1);  einsum_52 = None
            _to_copy_238: "f32[8192, 160]" = torch.ops.aten._to_copy.default(cat_78, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  cat_78 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:121 in forward, code: self.cos_cached = self.mscale * emb.cos()[:, None, :].to(dtype)
            cos_26: "f32[8192, 160]" = torch.ops.aten.cos.default(_to_copy_238)
            slice_274: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(cos_26, 0, 0, 9223372036854775807);  cos_26 = None
            unsqueeze_60: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_274, 1);  slice_274 = None
            slice_275: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_60, 2, 0, 9223372036854775807);  unsqueeze_60 = None
            _to_copy_239: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_275, dtype = torch.float32);  slice_275 = None
            mul_341: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_239, 1.0);  _to_copy_239 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:122 in forward, code: self.sin_cached = self.mscale * emb.sin()[:, None, :].to(dtype)
            sin_26: "f32[8192, 160]" = torch.ops.aten.sin.default(_to_copy_238);  _to_copy_238 = None
            slice_276: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(sin_26, 0, 0, 9223372036854775807);  sin_26 = None
            unsqueeze_61: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_276, 1);  slice_276 = None
            slice_277: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_61, 2, 0, 9223372036854775807);  unsqueeze_61 = None
            _to_copy_240: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_277, dtype = torch.float32);  slice_277 = None
            mul_342: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_240, 1.0);  _to_copy_240 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:123 in forward, code: return self.cos_cached[:seq_len, ...], self.sin_cached[:seq_len, ...]
            alias_52: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_341);  mul_341 = None
            alias_53: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_342);  mul_342 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:133 in apply_rotary_pos_emb_torch, code: cos, sin = cos[offset:q.shape[0] + offset, ...], sin[offset:q.shape[0] + offset, ...]
            slice_278: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_52, 0, 0, 6);  alias_52 = None
            slice_279: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_53, 0, 0, 6);  alias_53 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_343: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_314, slice_278)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_280: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_314, 2, 0, 80)
            slice_281: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_314, 2, 80, 9223372036854775807);  view_314 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_52: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_281);  slice_281 = None
            cat_79: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_52, slice_280], 2);  neg_52 = slice_280 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_344: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_79, slice_279);  cat_79 = None
            add_157: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_343, mul_344);  mul_343 = mul_344 = None
            mul_345: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_315, slice_278);  slice_278 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_282: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_315, 2, 0, 80)
            slice_283: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_315, 2, 80, 9223372036854775807);  view_315 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_53: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_283);  slice_283 = None
            cat_80: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_53, slice_282], 2);  neg_53 = slice_282 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_346: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_80, slice_279);  cat_80 = slice_279 = None
            add_158: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_345, mul_346);  mul_345 = mul_346 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:448 in forward, code: query_layer = query_layer.reshape((s_query, bz, head, dim))
            view_316: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_157, [6, 1, 32, 160]);  add_157 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:449 in forward, code: key_layer = key_layer.reshape((s_key, bz, head, dim))
            view_317: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_158, [6, 1, 32, 160]);  add_158 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:458 in forward, code: query_layer = query_layer.reshape(s_query, bz * self.num_heads, dim)
            view_318: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_316, [6, 32, 160]);  view_316 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:460 in forward, code: key_layer = key_layer.reshape(s_key, bz * self.num_heads, dim)
            view_319: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_317, [6, 32, 160]);  view_317 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            transpose_131: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_318, 0, 1);  view_318 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:462 in forward, code: key_layer.transpose(0, 1).transpose(1, 2))
            transpose_132: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_319, 0, 1);  view_319 = None
            transpose_133: "f32[32, 160, 6]" = torch.ops.aten.transpose.int(transpose_132, 1, 2);  transpose_132 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            einsum_53: "f32[32, 6, 6]" = torch.ops.aten.einsum.default('bik,bkj->bij', [transpose_131, transpose_133]);  transpose_131 = transpose_133 = None
            mul_347: "f32[32, 6, 6]" = torch.ops.aten.mul.Tensor(einsum_53, 0.07905694150420949);  einsum_53 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:464 in forward, code: attention_scores = matmul_result.view(bz, self.num_heads, s_query, s_key)
            view_320: "f32[1, 32, 6, 6]" = torch.ops.aten.view.default(mul_347, [1, 32, 6, 6]);  mul_347 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:469 in forward, code: attn_weights = torch.masked_fill(attention_scores, attention_mask, torch.finfo(attention_scores.dtype).min)
            masked_fill_26: "f32[1, 32, 6, 6]" = torch.ops.aten.masked_fill.Scalar(view_320, or_1, -3.4028234663852886e+38);  view_320 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:470 in forward, code: attention_probs = F.softmax(attn_weights, dim=-1).to(input_dtype)  ##dtype = torch.float32
            softmax_26: "f32[1, 32, 6, 6]" = torch.ops.aten.softmax.int(masked_fill_26, -1);  masked_fill_26 = None
            _to_copy_241: "f32[1, 32, 6, 6]" = torch.ops.aten._to_copy.default(softmax_26, dtype = torch.float32);  softmax_26 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:471 in forward, code: attention_probs = self.attention_dropout(attention_probs)
            dropout_78: "f32[1, 32, 6, 6]" = torch.ops.aten.dropout.default(_to_copy_241, 0.0, False);  _to_copy_241 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:472 in forward, code: attention_probs_reshaped = attention_probs.view(bz * self.num_heads, s_query, s_key)
            view_321: "f32[32, 6, 6]" = torch.ops.aten.view.default(dropout_78, [32, 6, 6]);  dropout_78 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:474 in forward, code: value_layer = value_layer.reshape(s_key, bz * self.num_heads, dim)
            view_322: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_53, [6, 32, 160]);  getitem_53 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:475 in forward, code: context_layer = torch.bmm(attention_probs_reshaped, value_layer.transpose(0, 1))
            transpose_134: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_322, 0, 1);  view_322 = None
            bmm_26: "f32[32, 6, 160]" = torch.ops.aten.bmm.default(view_321, transpose_134);  view_321 = transpose_134 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:391 in _merge_heads, code: x = x.view(batch_size, self.num_heads, seq_length, self.head_dim)
            view_323: "f32[1, 32, 6, 160]" = torch.ops.aten.view.default(bmm_26, [1, 32, 6, 160]);  bmm_26 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:392 in _merge_heads, code: x = x.permute(0, 2, 1, 3)
            permute_26: "f32[1, 6, 32, 160]" = torch.ops.aten.permute.default(view_323, [0, 2, 1, 3]);  view_323 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:393 in _merge_heads, code: return x.reshape(batch_size, seq_length, self.num_heads * self.head_dim)
            clone_26: "f32[1, 6, 32, 160]" = torch.ops.aten.clone.default(permute_26, memory_format = torch.contiguous_format);  permute_26 = None
            _unsafe_view_26: "f32[1, 6, 5120]" = torch.ops.aten._unsafe_view.default(clone_26, [1, 6, 5120]);  clone_26 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:478 in forward, code: output_tensor = self.dense(context_layer)
            linear_158: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(_unsafe_view_26, p_h_26_self_attention_dense_weight, p_h_26_self_attention_dense_bias);  _unsafe_view_26 = p_h_26_self_attention_dense_weight = p_h_26_self_attention_dense_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_79: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_158, 0.0, False);  linear_158 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_159: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_155, dropout_79);  add_155 = dropout_79 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_242: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_159, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_81: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_242, 2)
            mean_53: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_81, [-1], True);  pow_81 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_160: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_53, 1e-05);  mean_53 = None
            rsqrt_53: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_160);  add_160 = None
            mul_348: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_242, rsqrt_53);  _to_copy_242 = rsqrt_53 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_243: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_348, dtype = torch.float32);  mul_348 = None
            mul_349: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_26_post_attention_layernorm_weight, _to_copy_243);  p_h_26_post_attention_layernorm_weight = _to_copy_243 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:499 in forward, code: intermediate_output = self.down_proj(F.silu(self.gate_proj(hidden_states)) * self.up_proj(hidden_states))
            linear_159: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_349, p_h_26_mlp_gate_proj_weight);  p_h_26_mlp_gate_proj_weight = None
            silu_26: "f32[1, 6, 12288]" = torch.ops.aten.silu.default(linear_159);  linear_159 = None
            linear_160: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_349, p_h_26_mlp_up_proj_weight);  mul_349 = p_h_26_mlp_up_proj_weight = None
            mul_350: "f32[1, 6, 12288]" = torch.ops.aten.mul.Tensor(silu_26, linear_160);  silu_26 = linear_160 = None
            linear_161: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(mul_350, p_h_26_mlp_down_proj_weight, p_h_26_mlp_down_proj_bias);  mul_350 = p_h_26_mlp_down_proj_weight = p_h_26_mlp_down_proj_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_80: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_161, 0.0, False);  linear_161 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_161: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_159, dropout_80);  add_159 = dropout_80 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_244: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_161, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_82: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_244, 2)
            mean_54: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_82, [-1], True);  pow_82 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_162: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_54, 1e-05);  mean_54 = None
            rsqrt_54: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_162);  add_162 = None
            mul_351: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_244, rsqrt_54);  _to_copy_244 = rsqrt_54 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_245: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_351, dtype = torch.float32);  mul_351 = None
            mul_352: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_27_input_layernorm_weight, _to_copy_245);  p_h_27_input_layernorm_weight = _to_copy_245 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:404 in forward, code: hidden_states = hidden_states.transpose(1, 0)
            transpose_135: "f32[6, 1, 5120]" = torch.ops.aten.transpose.int(mul_352, 1, 0);  mul_352 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:405 in forward, code: query_layer = self.query(hidden_states)
            linear_162: "f32[6, 1, 5120]" = torch.ops.aten.linear.default(transpose_135, p_h_27_self_attention_query_weight);  p_h_27_self_attention_query_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:409 in forward, code: query_layer = query_layer.view(*new_tensor_shape)
            view_324: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(linear_162, [6, 1, 32, 160]);  linear_162 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:411 in forward, code: mixed_kv_layer = self.key_value(hidden_states)
            linear_163: "f32[6, 1, 10240]" = torch.ops.aten.linear.default(transpose_135, p_h_27_self_attention_key_value_weight);  transpose_135 = p_h_27_self_attention_key_value_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:415 in forward, code: mixed_kv_layer = mixed_kv_layer.view(*new_tensor_shape)
            view_325: "f32[6, 1, 32, 320]" = torch.ops.aten.view.default(linear_163, [6, 1, 32, 320]);  linear_163 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:381 in split_tensor_along_last_dim, code: tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)
            split_27 = torch.ops.aten.split.Tensor(view_325, 160, 3);  view_325 = None
            getitem_54: "f32[6, 1, 32, 160]" = split_27[0]
            getitem_55: "f32[6, 1, 32, 160]" = split_27[1];  split_27 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:423 in forward, code: query_layer = query_layer.view(output_size[2], output_size[0] * output_size[1], -1)
            view_326: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_324, [6, 32, -1]);  view_324 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:424 in forward, code: key_layer = key_layer.view(output_size[3], output_size[0] * output_size[1], -1)
            view_327: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_54, [6, 32, -1]);  getitem_54 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:112 in forward, code: self.inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, device=x.device).float() / self.dim))
            arange_55: "i64[80]" = torch.ops.aten.arange.start_step(0, 160, 2, device = device(type='cpu'), pin_memory = False)
            _to_copy_246: "f32[80]" = torch.ops.aten._to_copy.default(arange_55, dtype = torch.float32);  arange_55 = None
            div_27: "f32[80]" = torch.ops.aten.div.Tensor(_to_copy_246, 160);  _to_copy_246 = None
            pow_83: "f32[80]" = torch.ops.aten.pow.Scalar(30420.108888514722, div_27);  div_27 = None
            reciprocal_27: "f32[80]" = torch.ops.aten.reciprocal.default(pow_83);  pow_83 = None
            mul_353: "f32[80]" = torch.ops.aten.mul.Tensor(reciprocal_27, 1.0);  reciprocal_27 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:114 in forward, code: t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)
            arange_56: "f32[8192]" = torch.ops.aten.arange.default(8192, dtype = torch.float32, device = device(type='cpu'), pin_memory = False)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:115 in forward, code: freqs = torch.einsum('i,j->ij', t, self.inv_freq)
            einsum_54: "f32[8192, 80]" = torch.ops.aten.einsum.default('i,j->ij', [arange_56, mul_353]);  arange_56 = mul_353 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:117 in forward, code: emb = torch.cat((freqs, freqs), dim=-1).to(x.device)
            cat_81: "f32[8192, 160]" = torch.ops.aten.cat.default([einsum_54, einsum_54], -1);  einsum_54 = None
            _to_copy_247: "f32[8192, 160]" = torch.ops.aten._to_copy.default(cat_81, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  cat_81 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:121 in forward, code: self.cos_cached = self.mscale * emb.cos()[:, None, :].to(dtype)
            cos_27: "f32[8192, 160]" = torch.ops.aten.cos.default(_to_copy_247)
            slice_284: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(cos_27, 0, 0, 9223372036854775807);  cos_27 = None
            unsqueeze_62: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_284, 1);  slice_284 = None
            slice_285: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_62, 2, 0, 9223372036854775807);  unsqueeze_62 = None
            _to_copy_248: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_285, dtype = torch.float32);  slice_285 = None
            mul_354: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_248, 1.0);  _to_copy_248 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:122 in forward, code: self.sin_cached = self.mscale * emb.sin()[:, None, :].to(dtype)
            sin_27: "f32[8192, 160]" = torch.ops.aten.sin.default(_to_copy_247);  _to_copy_247 = None
            slice_286: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(sin_27, 0, 0, 9223372036854775807);  sin_27 = None
            unsqueeze_63: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_286, 1);  slice_286 = None
            slice_287: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_63, 2, 0, 9223372036854775807);  unsqueeze_63 = None
            _to_copy_249: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_287, dtype = torch.float32);  slice_287 = None
            mul_355: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_249, 1.0);  _to_copy_249 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:123 in forward, code: return self.cos_cached[:seq_len, ...], self.sin_cached[:seq_len, ...]
            alias_54: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_354);  mul_354 = None
            alias_55: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_355);  mul_355 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:133 in apply_rotary_pos_emb_torch, code: cos, sin = cos[offset:q.shape[0] + offset, ...], sin[offset:q.shape[0] + offset, ...]
            slice_288: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_54, 0, 0, 6);  alias_54 = None
            slice_289: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_55, 0, 0, 6);  alias_55 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_356: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_326, slice_288)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_290: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_326, 2, 0, 80)
            slice_291: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_326, 2, 80, 9223372036854775807);  view_326 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_54: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_291);  slice_291 = None
            cat_82: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_54, slice_290], 2);  neg_54 = slice_290 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_357: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_82, slice_289);  cat_82 = None
            add_163: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_356, mul_357);  mul_356 = mul_357 = None
            mul_358: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_327, slice_288);  slice_288 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_292: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_327, 2, 0, 80)
            slice_293: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_327, 2, 80, 9223372036854775807);  view_327 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_55: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_293);  slice_293 = None
            cat_83: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_55, slice_292], 2);  neg_55 = slice_292 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_359: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_83, slice_289);  cat_83 = slice_289 = None
            add_164: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_358, mul_359);  mul_358 = mul_359 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:448 in forward, code: query_layer = query_layer.reshape((s_query, bz, head, dim))
            view_328: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_163, [6, 1, 32, 160]);  add_163 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:449 in forward, code: key_layer = key_layer.reshape((s_key, bz, head, dim))
            view_329: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_164, [6, 1, 32, 160]);  add_164 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:458 in forward, code: query_layer = query_layer.reshape(s_query, bz * self.num_heads, dim)
            view_330: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_328, [6, 32, 160]);  view_328 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:460 in forward, code: key_layer = key_layer.reshape(s_key, bz * self.num_heads, dim)
            view_331: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_329, [6, 32, 160]);  view_329 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            transpose_136: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_330, 0, 1);  view_330 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:462 in forward, code: key_layer.transpose(0, 1).transpose(1, 2))
            transpose_137: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_331, 0, 1);  view_331 = None
            transpose_138: "f32[32, 160, 6]" = torch.ops.aten.transpose.int(transpose_137, 1, 2);  transpose_137 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            einsum_55: "f32[32, 6, 6]" = torch.ops.aten.einsum.default('bik,bkj->bij', [transpose_136, transpose_138]);  transpose_136 = transpose_138 = None
            mul_360: "f32[32, 6, 6]" = torch.ops.aten.mul.Tensor(einsum_55, 0.07905694150420949);  einsum_55 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:464 in forward, code: attention_scores = matmul_result.view(bz, self.num_heads, s_query, s_key)
            view_332: "f32[1, 32, 6, 6]" = torch.ops.aten.view.default(mul_360, [1, 32, 6, 6]);  mul_360 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:469 in forward, code: attn_weights = torch.masked_fill(attention_scores, attention_mask, torch.finfo(attention_scores.dtype).min)
            masked_fill_27: "f32[1, 32, 6, 6]" = torch.ops.aten.masked_fill.Scalar(view_332, or_1, -3.4028234663852886e+38);  view_332 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:470 in forward, code: attention_probs = F.softmax(attn_weights, dim=-1).to(input_dtype)  ##dtype = torch.float32
            softmax_27: "f32[1, 32, 6, 6]" = torch.ops.aten.softmax.int(masked_fill_27, -1);  masked_fill_27 = None
            _to_copy_250: "f32[1, 32, 6, 6]" = torch.ops.aten._to_copy.default(softmax_27, dtype = torch.float32);  softmax_27 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:471 in forward, code: attention_probs = self.attention_dropout(attention_probs)
            dropout_81: "f32[1, 32, 6, 6]" = torch.ops.aten.dropout.default(_to_copy_250, 0.0, False);  _to_copy_250 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:472 in forward, code: attention_probs_reshaped = attention_probs.view(bz * self.num_heads, s_query, s_key)
            view_333: "f32[32, 6, 6]" = torch.ops.aten.view.default(dropout_81, [32, 6, 6]);  dropout_81 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:474 in forward, code: value_layer = value_layer.reshape(s_key, bz * self.num_heads, dim)
            view_334: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_55, [6, 32, 160]);  getitem_55 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:475 in forward, code: context_layer = torch.bmm(attention_probs_reshaped, value_layer.transpose(0, 1))
            transpose_139: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_334, 0, 1);  view_334 = None
            bmm_27: "f32[32, 6, 160]" = torch.ops.aten.bmm.default(view_333, transpose_139);  view_333 = transpose_139 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:391 in _merge_heads, code: x = x.view(batch_size, self.num_heads, seq_length, self.head_dim)
            view_335: "f32[1, 32, 6, 160]" = torch.ops.aten.view.default(bmm_27, [1, 32, 6, 160]);  bmm_27 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:392 in _merge_heads, code: x = x.permute(0, 2, 1, 3)
            permute_27: "f32[1, 6, 32, 160]" = torch.ops.aten.permute.default(view_335, [0, 2, 1, 3]);  view_335 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:393 in _merge_heads, code: return x.reshape(batch_size, seq_length, self.num_heads * self.head_dim)
            clone_27: "f32[1, 6, 32, 160]" = torch.ops.aten.clone.default(permute_27, memory_format = torch.contiguous_format);  permute_27 = None
            _unsafe_view_27: "f32[1, 6, 5120]" = torch.ops.aten._unsafe_view.default(clone_27, [1, 6, 5120]);  clone_27 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:478 in forward, code: output_tensor = self.dense(context_layer)
            linear_164: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(_unsafe_view_27, p_h_27_self_attention_dense_weight, p_h_27_self_attention_dense_bias);  _unsafe_view_27 = p_h_27_self_attention_dense_weight = p_h_27_self_attention_dense_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_82: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_164, 0.0, False);  linear_164 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_165: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_161, dropout_82);  add_161 = dropout_82 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_251: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_165, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_84: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_251, 2)
            mean_55: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_84, [-1], True);  pow_84 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_166: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_55, 1e-05);  mean_55 = None
            rsqrt_55: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_166);  add_166 = None
            mul_361: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_251, rsqrt_55);  _to_copy_251 = rsqrt_55 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_252: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_361, dtype = torch.float32);  mul_361 = None
            mul_362: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_27_post_attention_layernorm_weight, _to_copy_252);  p_h_27_post_attention_layernorm_weight = _to_copy_252 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:499 in forward, code: intermediate_output = self.down_proj(F.silu(self.gate_proj(hidden_states)) * self.up_proj(hidden_states))
            linear_165: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_362, p_h_27_mlp_gate_proj_weight);  p_h_27_mlp_gate_proj_weight = None
            silu_27: "f32[1, 6, 12288]" = torch.ops.aten.silu.default(linear_165);  linear_165 = None
            linear_166: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_362, p_h_27_mlp_up_proj_weight);  mul_362 = p_h_27_mlp_up_proj_weight = None
            mul_363: "f32[1, 6, 12288]" = torch.ops.aten.mul.Tensor(silu_27, linear_166);  silu_27 = linear_166 = None
            linear_167: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(mul_363, p_h_27_mlp_down_proj_weight, p_h_27_mlp_down_proj_bias);  mul_363 = p_h_27_mlp_down_proj_weight = p_h_27_mlp_down_proj_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_83: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_167, 0.0, False);  linear_167 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_167: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_165, dropout_83);  add_165 = dropout_83 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_253: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_167, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_85: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_253, 2)
            mean_56: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_85, [-1], True);  pow_85 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_168: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_56, 1e-05);  mean_56 = None
            rsqrt_56: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_168);  add_168 = None
            mul_364: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_253, rsqrt_56);  _to_copy_253 = rsqrt_56 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_254: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_364, dtype = torch.float32);  mul_364 = None
            mul_365: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_28_input_layernorm_weight, _to_copy_254);  p_h_28_input_layernorm_weight = _to_copy_254 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:404 in forward, code: hidden_states = hidden_states.transpose(1, 0)
            transpose_140: "f32[6, 1, 5120]" = torch.ops.aten.transpose.int(mul_365, 1, 0);  mul_365 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:405 in forward, code: query_layer = self.query(hidden_states)
            linear_168: "f32[6, 1, 5120]" = torch.ops.aten.linear.default(transpose_140, p_h_28_self_attention_query_weight);  p_h_28_self_attention_query_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:409 in forward, code: query_layer = query_layer.view(*new_tensor_shape)
            view_336: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(linear_168, [6, 1, 32, 160]);  linear_168 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:411 in forward, code: mixed_kv_layer = self.key_value(hidden_states)
            linear_169: "f32[6, 1, 10240]" = torch.ops.aten.linear.default(transpose_140, p_h_28_self_attention_key_value_weight);  transpose_140 = p_h_28_self_attention_key_value_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:415 in forward, code: mixed_kv_layer = mixed_kv_layer.view(*new_tensor_shape)
            view_337: "f32[6, 1, 32, 320]" = torch.ops.aten.view.default(linear_169, [6, 1, 32, 320]);  linear_169 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:381 in split_tensor_along_last_dim, code: tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)
            split_28 = torch.ops.aten.split.Tensor(view_337, 160, 3);  view_337 = None
            getitem_56: "f32[6, 1, 32, 160]" = split_28[0]
            getitem_57: "f32[6, 1, 32, 160]" = split_28[1];  split_28 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:423 in forward, code: query_layer = query_layer.view(output_size[2], output_size[0] * output_size[1], -1)
            view_338: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_336, [6, 32, -1]);  view_336 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:424 in forward, code: key_layer = key_layer.view(output_size[3], output_size[0] * output_size[1], -1)
            view_339: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_56, [6, 32, -1]);  getitem_56 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:112 in forward, code: self.inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, device=x.device).float() / self.dim))
            arange_57: "i64[80]" = torch.ops.aten.arange.start_step(0, 160, 2, device = device(type='cpu'), pin_memory = False)
            _to_copy_255: "f32[80]" = torch.ops.aten._to_copy.default(arange_57, dtype = torch.float32);  arange_57 = None
            div_28: "f32[80]" = torch.ops.aten.div.Tensor(_to_copy_255, 160);  _to_copy_255 = None
            pow_86: "f32[80]" = torch.ops.aten.pow.Scalar(30420.108888514722, div_28);  div_28 = None
            reciprocal_28: "f32[80]" = torch.ops.aten.reciprocal.default(pow_86);  pow_86 = None
            mul_366: "f32[80]" = torch.ops.aten.mul.Tensor(reciprocal_28, 1.0);  reciprocal_28 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:114 in forward, code: t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)
            arange_58: "f32[8192]" = torch.ops.aten.arange.default(8192, dtype = torch.float32, device = device(type='cpu'), pin_memory = False)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:115 in forward, code: freqs = torch.einsum('i,j->ij', t, self.inv_freq)
            einsum_56: "f32[8192, 80]" = torch.ops.aten.einsum.default('i,j->ij', [arange_58, mul_366]);  arange_58 = mul_366 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:117 in forward, code: emb = torch.cat((freqs, freqs), dim=-1).to(x.device)
            cat_84: "f32[8192, 160]" = torch.ops.aten.cat.default([einsum_56, einsum_56], -1);  einsum_56 = None
            _to_copy_256: "f32[8192, 160]" = torch.ops.aten._to_copy.default(cat_84, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  cat_84 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:121 in forward, code: self.cos_cached = self.mscale * emb.cos()[:, None, :].to(dtype)
            cos_28: "f32[8192, 160]" = torch.ops.aten.cos.default(_to_copy_256)
            slice_294: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(cos_28, 0, 0, 9223372036854775807);  cos_28 = None
            unsqueeze_64: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_294, 1);  slice_294 = None
            slice_295: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_64, 2, 0, 9223372036854775807);  unsqueeze_64 = None
            _to_copy_257: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_295, dtype = torch.float32);  slice_295 = None
            mul_367: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_257, 1.0);  _to_copy_257 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:122 in forward, code: self.sin_cached = self.mscale * emb.sin()[:, None, :].to(dtype)
            sin_28: "f32[8192, 160]" = torch.ops.aten.sin.default(_to_copy_256);  _to_copy_256 = None
            slice_296: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(sin_28, 0, 0, 9223372036854775807);  sin_28 = None
            unsqueeze_65: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_296, 1);  slice_296 = None
            slice_297: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_65, 2, 0, 9223372036854775807);  unsqueeze_65 = None
            _to_copy_258: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_297, dtype = torch.float32);  slice_297 = None
            mul_368: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_258, 1.0);  _to_copy_258 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:123 in forward, code: return self.cos_cached[:seq_len, ...], self.sin_cached[:seq_len, ...]
            alias_56: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_367);  mul_367 = None
            alias_57: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_368);  mul_368 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:133 in apply_rotary_pos_emb_torch, code: cos, sin = cos[offset:q.shape[0] + offset, ...], sin[offset:q.shape[0] + offset, ...]
            slice_298: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_56, 0, 0, 6);  alias_56 = None
            slice_299: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_57, 0, 0, 6);  alias_57 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_369: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_338, slice_298)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_300: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_338, 2, 0, 80)
            slice_301: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_338, 2, 80, 9223372036854775807);  view_338 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_56: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_301);  slice_301 = None
            cat_85: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_56, slice_300], 2);  neg_56 = slice_300 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_370: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_85, slice_299);  cat_85 = None
            add_169: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_369, mul_370);  mul_369 = mul_370 = None
            mul_371: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_339, slice_298);  slice_298 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_302: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_339, 2, 0, 80)
            slice_303: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_339, 2, 80, 9223372036854775807);  view_339 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_57: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_303);  slice_303 = None
            cat_86: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_57, slice_302], 2);  neg_57 = slice_302 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_372: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_86, slice_299);  cat_86 = slice_299 = None
            add_170: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_371, mul_372);  mul_371 = mul_372 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:448 in forward, code: query_layer = query_layer.reshape((s_query, bz, head, dim))
            view_340: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_169, [6, 1, 32, 160]);  add_169 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:449 in forward, code: key_layer = key_layer.reshape((s_key, bz, head, dim))
            view_341: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_170, [6, 1, 32, 160]);  add_170 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:458 in forward, code: query_layer = query_layer.reshape(s_query, bz * self.num_heads, dim)
            view_342: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_340, [6, 32, 160]);  view_340 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:460 in forward, code: key_layer = key_layer.reshape(s_key, bz * self.num_heads, dim)
            view_343: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_341, [6, 32, 160]);  view_341 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            transpose_141: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_342, 0, 1);  view_342 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:462 in forward, code: key_layer.transpose(0, 1).transpose(1, 2))
            transpose_142: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_343, 0, 1);  view_343 = None
            transpose_143: "f32[32, 160, 6]" = torch.ops.aten.transpose.int(transpose_142, 1, 2);  transpose_142 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            einsum_57: "f32[32, 6, 6]" = torch.ops.aten.einsum.default('bik,bkj->bij', [transpose_141, transpose_143]);  transpose_141 = transpose_143 = None
            mul_373: "f32[32, 6, 6]" = torch.ops.aten.mul.Tensor(einsum_57, 0.07905694150420949);  einsum_57 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:464 in forward, code: attention_scores = matmul_result.view(bz, self.num_heads, s_query, s_key)
            view_344: "f32[1, 32, 6, 6]" = torch.ops.aten.view.default(mul_373, [1, 32, 6, 6]);  mul_373 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:469 in forward, code: attn_weights = torch.masked_fill(attention_scores, attention_mask, torch.finfo(attention_scores.dtype).min)
            masked_fill_28: "f32[1, 32, 6, 6]" = torch.ops.aten.masked_fill.Scalar(view_344, or_1, -3.4028234663852886e+38);  view_344 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:470 in forward, code: attention_probs = F.softmax(attn_weights, dim=-1).to(input_dtype)  ##dtype = torch.float32
            softmax_28: "f32[1, 32, 6, 6]" = torch.ops.aten.softmax.int(masked_fill_28, -1);  masked_fill_28 = None
            _to_copy_259: "f32[1, 32, 6, 6]" = torch.ops.aten._to_copy.default(softmax_28, dtype = torch.float32);  softmax_28 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:471 in forward, code: attention_probs = self.attention_dropout(attention_probs)
            dropout_84: "f32[1, 32, 6, 6]" = torch.ops.aten.dropout.default(_to_copy_259, 0.0, False);  _to_copy_259 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:472 in forward, code: attention_probs_reshaped = attention_probs.view(bz * self.num_heads, s_query, s_key)
            view_345: "f32[32, 6, 6]" = torch.ops.aten.view.default(dropout_84, [32, 6, 6]);  dropout_84 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:474 in forward, code: value_layer = value_layer.reshape(s_key, bz * self.num_heads, dim)
            view_346: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_57, [6, 32, 160]);  getitem_57 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:475 in forward, code: context_layer = torch.bmm(attention_probs_reshaped, value_layer.transpose(0, 1))
            transpose_144: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_346, 0, 1);  view_346 = None
            bmm_28: "f32[32, 6, 160]" = torch.ops.aten.bmm.default(view_345, transpose_144);  view_345 = transpose_144 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:391 in _merge_heads, code: x = x.view(batch_size, self.num_heads, seq_length, self.head_dim)
            view_347: "f32[1, 32, 6, 160]" = torch.ops.aten.view.default(bmm_28, [1, 32, 6, 160]);  bmm_28 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:392 in _merge_heads, code: x = x.permute(0, 2, 1, 3)
            permute_28: "f32[1, 6, 32, 160]" = torch.ops.aten.permute.default(view_347, [0, 2, 1, 3]);  view_347 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:393 in _merge_heads, code: return x.reshape(batch_size, seq_length, self.num_heads * self.head_dim)
            clone_28: "f32[1, 6, 32, 160]" = torch.ops.aten.clone.default(permute_28, memory_format = torch.contiguous_format);  permute_28 = None
            _unsafe_view_28: "f32[1, 6, 5120]" = torch.ops.aten._unsafe_view.default(clone_28, [1, 6, 5120]);  clone_28 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:478 in forward, code: output_tensor = self.dense(context_layer)
            linear_170: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(_unsafe_view_28, p_h_28_self_attention_dense_weight, p_h_28_self_attention_dense_bias);  _unsafe_view_28 = p_h_28_self_attention_dense_weight = p_h_28_self_attention_dense_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_85: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_170, 0.0, False);  linear_170 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_171: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_167, dropout_85);  add_167 = dropout_85 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_260: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_171, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_87: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_260, 2)
            mean_57: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_87, [-1], True);  pow_87 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_172: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_57, 1e-05);  mean_57 = None
            rsqrt_57: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_172);  add_172 = None
            mul_374: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_260, rsqrt_57);  _to_copy_260 = rsqrt_57 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_261: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_374, dtype = torch.float32);  mul_374 = None
            mul_375: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_28_post_attention_layernorm_weight, _to_copy_261);  p_h_28_post_attention_layernorm_weight = _to_copy_261 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:499 in forward, code: intermediate_output = self.down_proj(F.silu(self.gate_proj(hidden_states)) * self.up_proj(hidden_states))
            linear_171: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_375, p_h_28_mlp_gate_proj_weight);  p_h_28_mlp_gate_proj_weight = None
            silu_28: "f32[1, 6, 12288]" = torch.ops.aten.silu.default(linear_171);  linear_171 = None
            linear_172: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_375, p_h_28_mlp_up_proj_weight);  mul_375 = p_h_28_mlp_up_proj_weight = None
            mul_376: "f32[1, 6, 12288]" = torch.ops.aten.mul.Tensor(silu_28, linear_172);  silu_28 = linear_172 = None
            linear_173: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(mul_376, p_h_28_mlp_down_proj_weight, p_h_28_mlp_down_proj_bias);  mul_376 = p_h_28_mlp_down_proj_weight = p_h_28_mlp_down_proj_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_86: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_173, 0.0, False);  linear_173 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_173: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_171, dropout_86);  add_171 = dropout_86 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_262: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_173, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_88: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_262, 2)
            mean_58: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_88, [-1], True);  pow_88 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_174: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_58, 1e-05);  mean_58 = None
            rsqrt_58: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_174);  add_174 = None
            mul_377: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_262, rsqrt_58);  _to_copy_262 = rsqrt_58 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_263: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_377, dtype = torch.float32);  mul_377 = None
            mul_378: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_29_input_layernorm_weight, _to_copy_263);  p_h_29_input_layernorm_weight = _to_copy_263 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:404 in forward, code: hidden_states = hidden_states.transpose(1, 0)
            transpose_145: "f32[6, 1, 5120]" = torch.ops.aten.transpose.int(mul_378, 1, 0);  mul_378 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:405 in forward, code: query_layer = self.query(hidden_states)
            linear_174: "f32[6, 1, 5120]" = torch.ops.aten.linear.default(transpose_145, p_h_29_self_attention_query_weight);  p_h_29_self_attention_query_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:409 in forward, code: query_layer = query_layer.view(*new_tensor_shape)
            view_348: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(linear_174, [6, 1, 32, 160]);  linear_174 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:411 in forward, code: mixed_kv_layer = self.key_value(hidden_states)
            linear_175: "f32[6, 1, 10240]" = torch.ops.aten.linear.default(transpose_145, p_h_29_self_attention_key_value_weight);  transpose_145 = p_h_29_self_attention_key_value_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:415 in forward, code: mixed_kv_layer = mixed_kv_layer.view(*new_tensor_shape)
            view_349: "f32[6, 1, 32, 320]" = torch.ops.aten.view.default(linear_175, [6, 1, 32, 320]);  linear_175 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:381 in split_tensor_along_last_dim, code: tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)
            split_29 = torch.ops.aten.split.Tensor(view_349, 160, 3);  view_349 = None
            getitem_58: "f32[6, 1, 32, 160]" = split_29[0]
            getitem_59: "f32[6, 1, 32, 160]" = split_29[1];  split_29 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:423 in forward, code: query_layer = query_layer.view(output_size[2], output_size[0] * output_size[1], -1)
            view_350: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_348, [6, 32, -1]);  view_348 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:424 in forward, code: key_layer = key_layer.view(output_size[3], output_size[0] * output_size[1], -1)
            view_351: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_58, [6, 32, -1]);  getitem_58 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:112 in forward, code: self.inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, device=x.device).float() / self.dim))
            arange_59: "i64[80]" = torch.ops.aten.arange.start_step(0, 160, 2, device = device(type='cpu'), pin_memory = False)
            _to_copy_264: "f32[80]" = torch.ops.aten._to_copy.default(arange_59, dtype = torch.float32);  arange_59 = None
            div_29: "f32[80]" = torch.ops.aten.div.Tensor(_to_copy_264, 160);  _to_copy_264 = None
            pow_89: "f32[80]" = torch.ops.aten.pow.Scalar(30420.108888514722, div_29);  div_29 = None
            reciprocal_29: "f32[80]" = torch.ops.aten.reciprocal.default(pow_89);  pow_89 = None
            mul_379: "f32[80]" = torch.ops.aten.mul.Tensor(reciprocal_29, 1.0);  reciprocal_29 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:114 in forward, code: t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)
            arange_60: "f32[8192]" = torch.ops.aten.arange.default(8192, dtype = torch.float32, device = device(type='cpu'), pin_memory = False)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:115 in forward, code: freqs = torch.einsum('i,j->ij', t, self.inv_freq)
            einsum_58: "f32[8192, 80]" = torch.ops.aten.einsum.default('i,j->ij', [arange_60, mul_379]);  arange_60 = mul_379 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:117 in forward, code: emb = torch.cat((freqs, freqs), dim=-1).to(x.device)
            cat_87: "f32[8192, 160]" = torch.ops.aten.cat.default([einsum_58, einsum_58], -1);  einsum_58 = None
            _to_copy_265: "f32[8192, 160]" = torch.ops.aten._to_copy.default(cat_87, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  cat_87 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:121 in forward, code: self.cos_cached = self.mscale * emb.cos()[:, None, :].to(dtype)
            cos_29: "f32[8192, 160]" = torch.ops.aten.cos.default(_to_copy_265)
            slice_304: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(cos_29, 0, 0, 9223372036854775807);  cos_29 = None
            unsqueeze_66: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_304, 1);  slice_304 = None
            slice_305: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_66, 2, 0, 9223372036854775807);  unsqueeze_66 = None
            _to_copy_266: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_305, dtype = torch.float32);  slice_305 = None
            mul_380: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_266, 1.0);  _to_copy_266 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:122 in forward, code: self.sin_cached = self.mscale * emb.sin()[:, None, :].to(dtype)
            sin_29: "f32[8192, 160]" = torch.ops.aten.sin.default(_to_copy_265);  _to_copy_265 = None
            slice_306: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(sin_29, 0, 0, 9223372036854775807);  sin_29 = None
            unsqueeze_67: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_306, 1);  slice_306 = None
            slice_307: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_67, 2, 0, 9223372036854775807);  unsqueeze_67 = None
            _to_copy_267: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_307, dtype = torch.float32);  slice_307 = None
            mul_381: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_267, 1.0);  _to_copy_267 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:123 in forward, code: return self.cos_cached[:seq_len, ...], self.sin_cached[:seq_len, ...]
            alias_58: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_380);  mul_380 = None
            alias_59: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_381);  mul_381 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:133 in apply_rotary_pos_emb_torch, code: cos, sin = cos[offset:q.shape[0] + offset, ...], sin[offset:q.shape[0] + offset, ...]
            slice_308: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_58, 0, 0, 6);  alias_58 = None
            slice_309: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_59, 0, 0, 6);  alias_59 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_382: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_350, slice_308)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_310: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_350, 2, 0, 80)
            slice_311: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_350, 2, 80, 9223372036854775807);  view_350 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_58: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_311);  slice_311 = None
            cat_88: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_58, slice_310], 2);  neg_58 = slice_310 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_383: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_88, slice_309);  cat_88 = None
            add_175: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_382, mul_383);  mul_382 = mul_383 = None
            mul_384: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_351, slice_308);  slice_308 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_312: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_351, 2, 0, 80)
            slice_313: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_351, 2, 80, 9223372036854775807);  view_351 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_59: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_313);  slice_313 = None
            cat_89: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_59, slice_312], 2);  neg_59 = slice_312 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_385: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_89, slice_309);  cat_89 = slice_309 = None
            add_176: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_384, mul_385);  mul_384 = mul_385 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:448 in forward, code: query_layer = query_layer.reshape((s_query, bz, head, dim))
            view_352: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_175, [6, 1, 32, 160]);  add_175 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:449 in forward, code: key_layer = key_layer.reshape((s_key, bz, head, dim))
            view_353: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_176, [6, 1, 32, 160]);  add_176 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:458 in forward, code: query_layer = query_layer.reshape(s_query, bz * self.num_heads, dim)
            view_354: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_352, [6, 32, 160]);  view_352 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:460 in forward, code: key_layer = key_layer.reshape(s_key, bz * self.num_heads, dim)
            view_355: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_353, [6, 32, 160]);  view_353 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            transpose_146: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_354, 0, 1);  view_354 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:462 in forward, code: key_layer.transpose(0, 1).transpose(1, 2))
            transpose_147: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_355, 0, 1);  view_355 = None
            transpose_148: "f32[32, 160, 6]" = torch.ops.aten.transpose.int(transpose_147, 1, 2);  transpose_147 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            einsum_59: "f32[32, 6, 6]" = torch.ops.aten.einsum.default('bik,bkj->bij', [transpose_146, transpose_148]);  transpose_146 = transpose_148 = None
            mul_386: "f32[32, 6, 6]" = torch.ops.aten.mul.Tensor(einsum_59, 0.07905694150420949);  einsum_59 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:464 in forward, code: attention_scores = matmul_result.view(bz, self.num_heads, s_query, s_key)
            view_356: "f32[1, 32, 6, 6]" = torch.ops.aten.view.default(mul_386, [1, 32, 6, 6]);  mul_386 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:469 in forward, code: attn_weights = torch.masked_fill(attention_scores, attention_mask, torch.finfo(attention_scores.dtype).min)
            masked_fill_29: "f32[1, 32, 6, 6]" = torch.ops.aten.masked_fill.Scalar(view_356, or_1, -3.4028234663852886e+38);  view_356 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:470 in forward, code: attention_probs = F.softmax(attn_weights, dim=-1).to(input_dtype)  ##dtype = torch.float32
            softmax_29: "f32[1, 32, 6, 6]" = torch.ops.aten.softmax.int(masked_fill_29, -1);  masked_fill_29 = None
            _to_copy_268: "f32[1, 32, 6, 6]" = torch.ops.aten._to_copy.default(softmax_29, dtype = torch.float32);  softmax_29 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:471 in forward, code: attention_probs = self.attention_dropout(attention_probs)
            dropout_87: "f32[1, 32, 6, 6]" = torch.ops.aten.dropout.default(_to_copy_268, 0.0, False);  _to_copy_268 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:472 in forward, code: attention_probs_reshaped = attention_probs.view(bz * self.num_heads, s_query, s_key)
            view_357: "f32[32, 6, 6]" = torch.ops.aten.view.default(dropout_87, [32, 6, 6]);  dropout_87 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:474 in forward, code: value_layer = value_layer.reshape(s_key, bz * self.num_heads, dim)
            view_358: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_59, [6, 32, 160]);  getitem_59 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:475 in forward, code: context_layer = torch.bmm(attention_probs_reshaped, value_layer.transpose(0, 1))
            transpose_149: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_358, 0, 1);  view_358 = None
            bmm_29: "f32[32, 6, 160]" = torch.ops.aten.bmm.default(view_357, transpose_149);  view_357 = transpose_149 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:391 in _merge_heads, code: x = x.view(batch_size, self.num_heads, seq_length, self.head_dim)
            view_359: "f32[1, 32, 6, 160]" = torch.ops.aten.view.default(bmm_29, [1, 32, 6, 160]);  bmm_29 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:392 in _merge_heads, code: x = x.permute(0, 2, 1, 3)
            permute_29: "f32[1, 6, 32, 160]" = torch.ops.aten.permute.default(view_359, [0, 2, 1, 3]);  view_359 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:393 in _merge_heads, code: return x.reshape(batch_size, seq_length, self.num_heads * self.head_dim)
            clone_29: "f32[1, 6, 32, 160]" = torch.ops.aten.clone.default(permute_29, memory_format = torch.contiguous_format);  permute_29 = None
            _unsafe_view_29: "f32[1, 6, 5120]" = torch.ops.aten._unsafe_view.default(clone_29, [1, 6, 5120]);  clone_29 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:478 in forward, code: output_tensor = self.dense(context_layer)
            linear_176: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(_unsafe_view_29, p_h_29_self_attention_dense_weight, p_h_29_self_attention_dense_bias);  _unsafe_view_29 = p_h_29_self_attention_dense_weight = p_h_29_self_attention_dense_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_88: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_176, 0.0, False);  linear_176 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_177: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_173, dropout_88);  add_173 = dropout_88 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_269: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_177, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_90: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_269, 2)
            mean_59: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_90, [-1], True);  pow_90 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_178: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_59, 1e-05);  mean_59 = None
            rsqrt_59: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_178);  add_178 = None
            mul_387: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_269, rsqrt_59);  _to_copy_269 = rsqrt_59 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_270: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_387, dtype = torch.float32);  mul_387 = None
            mul_388: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_29_post_attention_layernorm_weight, _to_copy_270);  p_h_29_post_attention_layernorm_weight = _to_copy_270 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:499 in forward, code: intermediate_output = self.down_proj(F.silu(self.gate_proj(hidden_states)) * self.up_proj(hidden_states))
            linear_177: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_388, p_h_29_mlp_gate_proj_weight);  p_h_29_mlp_gate_proj_weight = None
            silu_29: "f32[1, 6, 12288]" = torch.ops.aten.silu.default(linear_177);  linear_177 = None
            linear_178: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_388, p_h_29_mlp_up_proj_weight);  mul_388 = p_h_29_mlp_up_proj_weight = None
            mul_389: "f32[1, 6, 12288]" = torch.ops.aten.mul.Tensor(silu_29, linear_178);  silu_29 = linear_178 = None
            linear_179: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(mul_389, p_h_29_mlp_down_proj_weight, p_h_29_mlp_down_proj_bias);  mul_389 = p_h_29_mlp_down_proj_weight = p_h_29_mlp_down_proj_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_89: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_179, 0.0, False);  linear_179 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_179: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_177, dropout_89);  add_177 = dropout_89 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_271: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_179, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_91: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_271, 2)
            mean_60: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_91, [-1], True);  pow_91 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_180: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_60, 1e-05);  mean_60 = None
            rsqrt_60: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_180);  add_180 = None
            mul_390: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_271, rsqrt_60);  _to_copy_271 = rsqrt_60 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_272: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_390, dtype = torch.float32);  mul_390 = None
            mul_391: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_30_input_layernorm_weight, _to_copy_272);  p_h_30_input_layernorm_weight = _to_copy_272 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:404 in forward, code: hidden_states = hidden_states.transpose(1, 0)
            transpose_150: "f32[6, 1, 5120]" = torch.ops.aten.transpose.int(mul_391, 1, 0);  mul_391 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:405 in forward, code: query_layer = self.query(hidden_states)
            linear_180: "f32[6, 1, 5120]" = torch.ops.aten.linear.default(transpose_150, p_h_30_self_attention_query_weight);  p_h_30_self_attention_query_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:409 in forward, code: query_layer = query_layer.view(*new_tensor_shape)
            view_360: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(linear_180, [6, 1, 32, 160]);  linear_180 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:411 in forward, code: mixed_kv_layer = self.key_value(hidden_states)
            linear_181: "f32[6, 1, 10240]" = torch.ops.aten.linear.default(transpose_150, p_h_30_self_attention_key_value_weight);  transpose_150 = p_h_30_self_attention_key_value_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:415 in forward, code: mixed_kv_layer = mixed_kv_layer.view(*new_tensor_shape)
            view_361: "f32[6, 1, 32, 320]" = torch.ops.aten.view.default(linear_181, [6, 1, 32, 320]);  linear_181 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:381 in split_tensor_along_last_dim, code: tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)
            split_30 = torch.ops.aten.split.Tensor(view_361, 160, 3);  view_361 = None
            getitem_60: "f32[6, 1, 32, 160]" = split_30[0]
            getitem_61: "f32[6, 1, 32, 160]" = split_30[1];  split_30 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:423 in forward, code: query_layer = query_layer.view(output_size[2], output_size[0] * output_size[1], -1)
            view_362: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_360, [6, 32, -1]);  view_360 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:424 in forward, code: key_layer = key_layer.view(output_size[3], output_size[0] * output_size[1], -1)
            view_363: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_60, [6, 32, -1]);  getitem_60 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:112 in forward, code: self.inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, device=x.device).float() / self.dim))
            arange_61: "i64[80]" = torch.ops.aten.arange.start_step(0, 160, 2, device = device(type='cpu'), pin_memory = False)
            _to_copy_273: "f32[80]" = torch.ops.aten._to_copy.default(arange_61, dtype = torch.float32);  arange_61 = None
            div_30: "f32[80]" = torch.ops.aten.div.Tensor(_to_copy_273, 160);  _to_copy_273 = None
            pow_92: "f32[80]" = torch.ops.aten.pow.Scalar(30420.108888514722, div_30);  div_30 = None
            reciprocal_30: "f32[80]" = torch.ops.aten.reciprocal.default(pow_92);  pow_92 = None
            mul_392: "f32[80]" = torch.ops.aten.mul.Tensor(reciprocal_30, 1.0);  reciprocal_30 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:114 in forward, code: t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)
            arange_62: "f32[8192]" = torch.ops.aten.arange.default(8192, dtype = torch.float32, device = device(type='cpu'), pin_memory = False)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:115 in forward, code: freqs = torch.einsum('i,j->ij', t, self.inv_freq)
            einsum_60: "f32[8192, 80]" = torch.ops.aten.einsum.default('i,j->ij', [arange_62, mul_392]);  arange_62 = mul_392 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:117 in forward, code: emb = torch.cat((freqs, freqs), dim=-1).to(x.device)
            cat_90: "f32[8192, 160]" = torch.ops.aten.cat.default([einsum_60, einsum_60], -1);  einsum_60 = None
            _to_copy_274: "f32[8192, 160]" = torch.ops.aten._to_copy.default(cat_90, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  cat_90 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:121 in forward, code: self.cos_cached = self.mscale * emb.cos()[:, None, :].to(dtype)
            cos_30: "f32[8192, 160]" = torch.ops.aten.cos.default(_to_copy_274)
            slice_314: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(cos_30, 0, 0, 9223372036854775807);  cos_30 = None
            unsqueeze_68: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_314, 1);  slice_314 = None
            slice_315: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_68, 2, 0, 9223372036854775807);  unsqueeze_68 = None
            _to_copy_275: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_315, dtype = torch.float32);  slice_315 = None
            mul_393: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_275, 1.0);  _to_copy_275 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:122 in forward, code: self.sin_cached = self.mscale * emb.sin()[:, None, :].to(dtype)
            sin_30: "f32[8192, 160]" = torch.ops.aten.sin.default(_to_copy_274);  _to_copy_274 = None
            slice_316: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(sin_30, 0, 0, 9223372036854775807);  sin_30 = None
            unsqueeze_69: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_316, 1);  slice_316 = None
            slice_317: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_69, 2, 0, 9223372036854775807);  unsqueeze_69 = None
            _to_copy_276: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_317, dtype = torch.float32);  slice_317 = None
            mul_394: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_276, 1.0);  _to_copy_276 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:123 in forward, code: return self.cos_cached[:seq_len, ...], self.sin_cached[:seq_len, ...]
            alias_60: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_393);  mul_393 = None
            alias_61: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_394);  mul_394 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:133 in apply_rotary_pos_emb_torch, code: cos, sin = cos[offset:q.shape[0] + offset, ...], sin[offset:q.shape[0] + offset, ...]
            slice_318: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_60, 0, 0, 6);  alias_60 = None
            slice_319: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_61, 0, 0, 6);  alias_61 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_395: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_362, slice_318)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_320: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_362, 2, 0, 80)
            slice_321: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_362, 2, 80, 9223372036854775807);  view_362 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_60: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_321);  slice_321 = None
            cat_91: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_60, slice_320], 2);  neg_60 = slice_320 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_396: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_91, slice_319);  cat_91 = None
            add_181: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_395, mul_396);  mul_395 = mul_396 = None
            mul_397: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_363, slice_318);  slice_318 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_322: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_363, 2, 0, 80)
            slice_323: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_363, 2, 80, 9223372036854775807);  view_363 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_61: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_323);  slice_323 = None
            cat_92: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_61, slice_322], 2);  neg_61 = slice_322 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_398: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_92, slice_319);  cat_92 = slice_319 = None
            add_182: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_397, mul_398);  mul_397 = mul_398 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:448 in forward, code: query_layer = query_layer.reshape((s_query, bz, head, dim))
            view_364: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_181, [6, 1, 32, 160]);  add_181 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:449 in forward, code: key_layer = key_layer.reshape((s_key, bz, head, dim))
            view_365: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_182, [6, 1, 32, 160]);  add_182 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:458 in forward, code: query_layer = query_layer.reshape(s_query, bz * self.num_heads, dim)
            view_366: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_364, [6, 32, 160]);  view_364 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:460 in forward, code: key_layer = key_layer.reshape(s_key, bz * self.num_heads, dim)
            view_367: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_365, [6, 32, 160]);  view_365 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            transpose_151: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_366, 0, 1);  view_366 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:462 in forward, code: key_layer.transpose(0, 1).transpose(1, 2))
            transpose_152: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_367, 0, 1);  view_367 = None
            transpose_153: "f32[32, 160, 6]" = torch.ops.aten.transpose.int(transpose_152, 1, 2);  transpose_152 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            einsum_61: "f32[32, 6, 6]" = torch.ops.aten.einsum.default('bik,bkj->bij', [transpose_151, transpose_153]);  transpose_151 = transpose_153 = None
            mul_399: "f32[32, 6, 6]" = torch.ops.aten.mul.Tensor(einsum_61, 0.07905694150420949);  einsum_61 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:464 in forward, code: attention_scores = matmul_result.view(bz, self.num_heads, s_query, s_key)
            view_368: "f32[1, 32, 6, 6]" = torch.ops.aten.view.default(mul_399, [1, 32, 6, 6]);  mul_399 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:469 in forward, code: attn_weights = torch.masked_fill(attention_scores, attention_mask, torch.finfo(attention_scores.dtype).min)
            masked_fill_30: "f32[1, 32, 6, 6]" = torch.ops.aten.masked_fill.Scalar(view_368, or_1, -3.4028234663852886e+38);  view_368 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:470 in forward, code: attention_probs = F.softmax(attn_weights, dim=-1).to(input_dtype)  ##dtype = torch.float32
            softmax_30: "f32[1, 32, 6, 6]" = torch.ops.aten.softmax.int(masked_fill_30, -1);  masked_fill_30 = None
            _to_copy_277: "f32[1, 32, 6, 6]" = torch.ops.aten._to_copy.default(softmax_30, dtype = torch.float32);  softmax_30 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:471 in forward, code: attention_probs = self.attention_dropout(attention_probs)
            dropout_90: "f32[1, 32, 6, 6]" = torch.ops.aten.dropout.default(_to_copy_277, 0.0, False);  _to_copy_277 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:472 in forward, code: attention_probs_reshaped = attention_probs.view(bz * self.num_heads, s_query, s_key)
            view_369: "f32[32, 6, 6]" = torch.ops.aten.view.default(dropout_90, [32, 6, 6]);  dropout_90 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:474 in forward, code: value_layer = value_layer.reshape(s_key, bz * self.num_heads, dim)
            view_370: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_61, [6, 32, 160]);  getitem_61 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:475 in forward, code: context_layer = torch.bmm(attention_probs_reshaped, value_layer.transpose(0, 1))
            transpose_154: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_370, 0, 1);  view_370 = None
            bmm_30: "f32[32, 6, 160]" = torch.ops.aten.bmm.default(view_369, transpose_154);  view_369 = transpose_154 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:391 in _merge_heads, code: x = x.view(batch_size, self.num_heads, seq_length, self.head_dim)
            view_371: "f32[1, 32, 6, 160]" = torch.ops.aten.view.default(bmm_30, [1, 32, 6, 160]);  bmm_30 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:392 in _merge_heads, code: x = x.permute(0, 2, 1, 3)
            permute_30: "f32[1, 6, 32, 160]" = torch.ops.aten.permute.default(view_371, [0, 2, 1, 3]);  view_371 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:393 in _merge_heads, code: return x.reshape(batch_size, seq_length, self.num_heads * self.head_dim)
            clone_30: "f32[1, 6, 32, 160]" = torch.ops.aten.clone.default(permute_30, memory_format = torch.contiguous_format);  permute_30 = None
            _unsafe_view_30: "f32[1, 6, 5120]" = torch.ops.aten._unsafe_view.default(clone_30, [1, 6, 5120]);  clone_30 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:478 in forward, code: output_tensor = self.dense(context_layer)
            linear_182: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(_unsafe_view_30, p_h_30_self_attention_dense_weight, p_h_30_self_attention_dense_bias);  _unsafe_view_30 = p_h_30_self_attention_dense_weight = p_h_30_self_attention_dense_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_91: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_182, 0.0, False);  linear_182 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_183: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_179, dropout_91);  add_179 = dropout_91 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_278: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_183, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_93: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_278, 2)
            mean_61: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_93, [-1], True);  pow_93 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_184: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_61, 1e-05);  mean_61 = None
            rsqrt_61: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_184);  add_184 = None
            mul_400: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_278, rsqrt_61);  _to_copy_278 = rsqrt_61 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_279: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_400, dtype = torch.float32);  mul_400 = None
            mul_401: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_30_post_attention_layernorm_weight, _to_copy_279);  p_h_30_post_attention_layernorm_weight = _to_copy_279 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:499 in forward, code: intermediate_output = self.down_proj(F.silu(self.gate_proj(hidden_states)) * self.up_proj(hidden_states))
            linear_183: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_401, p_h_30_mlp_gate_proj_weight);  p_h_30_mlp_gate_proj_weight = None
            silu_30: "f32[1, 6, 12288]" = torch.ops.aten.silu.default(linear_183);  linear_183 = None
            linear_184: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_401, p_h_30_mlp_up_proj_weight);  mul_401 = p_h_30_mlp_up_proj_weight = None
            mul_402: "f32[1, 6, 12288]" = torch.ops.aten.mul.Tensor(silu_30, linear_184);  silu_30 = linear_184 = None
            linear_185: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(mul_402, p_h_30_mlp_down_proj_weight, p_h_30_mlp_down_proj_bias);  mul_402 = p_h_30_mlp_down_proj_weight = p_h_30_mlp_down_proj_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_92: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_185, 0.0, False);  linear_185 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_185: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_183, dropout_92);  add_183 = dropout_92 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_280: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_185, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_94: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_280, 2)
            mean_62: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_94, [-1], True);  pow_94 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_186: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_62, 1e-05);  mean_62 = None
            rsqrt_62: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_186);  add_186 = None
            mul_403: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_280, rsqrt_62);  _to_copy_280 = rsqrt_62 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_281: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_403, dtype = torch.float32);  mul_403 = None
            mul_404: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_31_input_layernorm_weight, _to_copy_281);  p_h_31_input_layernorm_weight = _to_copy_281 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:404 in forward, code: hidden_states = hidden_states.transpose(1, 0)
            transpose_155: "f32[6, 1, 5120]" = torch.ops.aten.transpose.int(mul_404, 1, 0);  mul_404 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:405 in forward, code: query_layer = self.query(hidden_states)
            linear_186: "f32[6, 1, 5120]" = torch.ops.aten.linear.default(transpose_155, p_h_31_self_attention_query_weight);  p_h_31_self_attention_query_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:409 in forward, code: query_layer = query_layer.view(*new_tensor_shape)
            view_372: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(linear_186, [6, 1, 32, 160]);  linear_186 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:411 in forward, code: mixed_kv_layer = self.key_value(hidden_states)
            linear_187: "f32[6, 1, 10240]" = torch.ops.aten.linear.default(transpose_155, p_h_31_self_attention_key_value_weight);  transpose_155 = p_h_31_self_attention_key_value_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:415 in forward, code: mixed_kv_layer = mixed_kv_layer.view(*new_tensor_shape)
            view_373: "f32[6, 1, 32, 320]" = torch.ops.aten.view.default(linear_187, [6, 1, 32, 320]);  linear_187 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:381 in split_tensor_along_last_dim, code: tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)
            split_31 = torch.ops.aten.split.Tensor(view_373, 160, 3);  view_373 = None
            getitem_62: "f32[6, 1, 32, 160]" = split_31[0]
            getitem_63: "f32[6, 1, 32, 160]" = split_31[1];  split_31 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:423 in forward, code: query_layer = query_layer.view(output_size[2], output_size[0] * output_size[1], -1)
            view_374: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_372, [6, 32, -1]);  view_372 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:424 in forward, code: key_layer = key_layer.view(output_size[3], output_size[0] * output_size[1], -1)
            view_375: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_62, [6, 32, -1]);  getitem_62 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:112 in forward, code: self.inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, device=x.device).float() / self.dim))
            arange_63: "i64[80]" = torch.ops.aten.arange.start_step(0, 160, 2, device = device(type='cpu'), pin_memory = False)
            _to_copy_282: "f32[80]" = torch.ops.aten._to_copy.default(arange_63, dtype = torch.float32);  arange_63 = None
            div_31: "f32[80]" = torch.ops.aten.div.Tensor(_to_copy_282, 160);  _to_copy_282 = None
            pow_95: "f32[80]" = torch.ops.aten.pow.Scalar(30420.108888514722, div_31);  div_31 = None
            reciprocal_31: "f32[80]" = torch.ops.aten.reciprocal.default(pow_95);  pow_95 = None
            mul_405: "f32[80]" = torch.ops.aten.mul.Tensor(reciprocal_31, 1.0);  reciprocal_31 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:114 in forward, code: t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)
            arange_64: "f32[8192]" = torch.ops.aten.arange.default(8192, dtype = torch.float32, device = device(type='cpu'), pin_memory = False)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:115 in forward, code: freqs = torch.einsum('i,j->ij', t, self.inv_freq)
            einsum_62: "f32[8192, 80]" = torch.ops.aten.einsum.default('i,j->ij', [arange_64, mul_405]);  arange_64 = mul_405 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:117 in forward, code: emb = torch.cat((freqs, freqs), dim=-1).to(x.device)
            cat_93: "f32[8192, 160]" = torch.ops.aten.cat.default([einsum_62, einsum_62], -1);  einsum_62 = None
            _to_copy_283: "f32[8192, 160]" = torch.ops.aten._to_copy.default(cat_93, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  cat_93 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:121 in forward, code: self.cos_cached = self.mscale * emb.cos()[:, None, :].to(dtype)
            cos_31: "f32[8192, 160]" = torch.ops.aten.cos.default(_to_copy_283)
            slice_324: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(cos_31, 0, 0, 9223372036854775807);  cos_31 = None
            unsqueeze_70: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_324, 1);  slice_324 = None
            slice_325: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_70, 2, 0, 9223372036854775807);  unsqueeze_70 = None
            _to_copy_284: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_325, dtype = torch.float32);  slice_325 = None
            mul_406: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_284, 1.0);  _to_copy_284 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:122 in forward, code: self.sin_cached = self.mscale * emb.sin()[:, None, :].to(dtype)
            sin_31: "f32[8192, 160]" = torch.ops.aten.sin.default(_to_copy_283);  _to_copy_283 = None
            slice_326: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(sin_31, 0, 0, 9223372036854775807);  sin_31 = None
            unsqueeze_71: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_326, 1);  slice_326 = None
            slice_327: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_71, 2, 0, 9223372036854775807);  unsqueeze_71 = None
            _to_copy_285: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_327, dtype = torch.float32);  slice_327 = None
            mul_407: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_285, 1.0);  _to_copy_285 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:123 in forward, code: return self.cos_cached[:seq_len, ...], self.sin_cached[:seq_len, ...]
            alias_62: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_406);  mul_406 = None
            alias_63: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_407);  mul_407 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:133 in apply_rotary_pos_emb_torch, code: cos, sin = cos[offset:q.shape[0] + offset, ...], sin[offset:q.shape[0] + offset, ...]
            slice_328: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_62, 0, 0, 6);  alias_62 = None
            slice_329: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_63, 0, 0, 6);  alias_63 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_408: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_374, slice_328)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_330: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_374, 2, 0, 80)
            slice_331: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_374, 2, 80, 9223372036854775807);  view_374 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_62: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_331);  slice_331 = None
            cat_94: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_62, slice_330], 2);  neg_62 = slice_330 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_409: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_94, slice_329);  cat_94 = None
            add_187: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_408, mul_409);  mul_408 = mul_409 = None
            mul_410: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_375, slice_328);  slice_328 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_332: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_375, 2, 0, 80)
            slice_333: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_375, 2, 80, 9223372036854775807);  view_375 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_63: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_333);  slice_333 = None
            cat_95: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_63, slice_332], 2);  neg_63 = slice_332 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_411: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_95, slice_329);  cat_95 = slice_329 = None
            add_188: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_410, mul_411);  mul_410 = mul_411 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:448 in forward, code: query_layer = query_layer.reshape((s_query, bz, head, dim))
            view_376: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_187, [6, 1, 32, 160]);  add_187 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:449 in forward, code: key_layer = key_layer.reshape((s_key, bz, head, dim))
            view_377: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_188, [6, 1, 32, 160]);  add_188 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:458 in forward, code: query_layer = query_layer.reshape(s_query, bz * self.num_heads, dim)
            view_378: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_376, [6, 32, 160]);  view_376 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:460 in forward, code: key_layer = key_layer.reshape(s_key, bz * self.num_heads, dim)
            view_379: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_377, [6, 32, 160]);  view_377 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            transpose_156: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_378, 0, 1);  view_378 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:462 in forward, code: key_layer.transpose(0, 1).transpose(1, 2))
            transpose_157: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_379, 0, 1);  view_379 = None
            transpose_158: "f32[32, 160, 6]" = torch.ops.aten.transpose.int(transpose_157, 1, 2);  transpose_157 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            einsum_63: "f32[32, 6, 6]" = torch.ops.aten.einsum.default('bik,bkj->bij', [transpose_156, transpose_158]);  transpose_156 = transpose_158 = None
            mul_412: "f32[32, 6, 6]" = torch.ops.aten.mul.Tensor(einsum_63, 0.07905694150420949);  einsum_63 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:464 in forward, code: attention_scores = matmul_result.view(bz, self.num_heads, s_query, s_key)
            view_380: "f32[1, 32, 6, 6]" = torch.ops.aten.view.default(mul_412, [1, 32, 6, 6]);  mul_412 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:469 in forward, code: attn_weights = torch.masked_fill(attention_scores, attention_mask, torch.finfo(attention_scores.dtype).min)
            masked_fill_31: "f32[1, 32, 6, 6]" = torch.ops.aten.masked_fill.Scalar(view_380, or_1, -3.4028234663852886e+38);  view_380 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:470 in forward, code: attention_probs = F.softmax(attn_weights, dim=-1).to(input_dtype)  ##dtype = torch.float32
            softmax_31: "f32[1, 32, 6, 6]" = torch.ops.aten.softmax.int(masked_fill_31, -1);  masked_fill_31 = None
            _to_copy_286: "f32[1, 32, 6, 6]" = torch.ops.aten._to_copy.default(softmax_31, dtype = torch.float32);  softmax_31 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:471 in forward, code: attention_probs = self.attention_dropout(attention_probs)
            dropout_93: "f32[1, 32, 6, 6]" = torch.ops.aten.dropout.default(_to_copy_286, 0.0, False);  _to_copy_286 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:472 in forward, code: attention_probs_reshaped = attention_probs.view(bz * self.num_heads, s_query, s_key)
            view_381: "f32[32, 6, 6]" = torch.ops.aten.view.default(dropout_93, [32, 6, 6]);  dropout_93 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:474 in forward, code: value_layer = value_layer.reshape(s_key, bz * self.num_heads, dim)
            view_382: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_63, [6, 32, 160]);  getitem_63 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:475 in forward, code: context_layer = torch.bmm(attention_probs_reshaped, value_layer.transpose(0, 1))
            transpose_159: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_382, 0, 1);  view_382 = None
            bmm_31: "f32[32, 6, 160]" = torch.ops.aten.bmm.default(view_381, transpose_159);  view_381 = transpose_159 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:391 in _merge_heads, code: x = x.view(batch_size, self.num_heads, seq_length, self.head_dim)
            view_383: "f32[1, 32, 6, 160]" = torch.ops.aten.view.default(bmm_31, [1, 32, 6, 160]);  bmm_31 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:392 in _merge_heads, code: x = x.permute(0, 2, 1, 3)
            permute_31: "f32[1, 6, 32, 160]" = torch.ops.aten.permute.default(view_383, [0, 2, 1, 3]);  view_383 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:393 in _merge_heads, code: return x.reshape(batch_size, seq_length, self.num_heads * self.head_dim)
            clone_31: "f32[1, 6, 32, 160]" = torch.ops.aten.clone.default(permute_31, memory_format = torch.contiguous_format);  permute_31 = None
            _unsafe_view_31: "f32[1, 6, 5120]" = torch.ops.aten._unsafe_view.default(clone_31, [1, 6, 5120]);  clone_31 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:478 in forward, code: output_tensor = self.dense(context_layer)
            linear_188: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(_unsafe_view_31, p_h_31_self_attention_dense_weight, p_h_31_self_attention_dense_bias);  _unsafe_view_31 = p_h_31_self_attention_dense_weight = p_h_31_self_attention_dense_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_94: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_188, 0.0, False);  linear_188 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_189: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_185, dropout_94);  add_185 = dropout_94 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_287: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_189, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_96: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_287, 2)
            mean_63: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_96, [-1], True);  pow_96 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_190: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_63, 1e-05);  mean_63 = None
            rsqrt_63: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_190);  add_190 = None
            mul_413: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_287, rsqrt_63);  _to_copy_287 = rsqrt_63 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_288: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_413, dtype = torch.float32);  mul_413 = None
            mul_414: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_31_post_attention_layernorm_weight, _to_copy_288);  p_h_31_post_attention_layernorm_weight = _to_copy_288 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:499 in forward, code: intermediate_output = self.down_proj(F.silu(self.gate_proj(hidden_states)) * self.up_proj(hidden_states))
            linear_189: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_414, p_h_31_mlp_gate_proj_weight);  p_h_31_mlp_gate_proj_weight = None
            silu_31: "f32[1, 6, 12288]" = torch.ops.aten.silu.default(linear_189);  linear_189 = None
            linear_190: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_414, p_h_31_mlp_up_proj_weight);  mul_414 = p_h_31_mlp_up_proj_weight = None
            mul_415: "f32[1, 6, 12288]" = torch.ops.aten.mul.Tensor(silu_31, linear_190);  silu_31 = linear_190 = None
            linear_191: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(mul_415, p_h_31_mlp_down_proj_weight, p_h_31_mlp_down_proj_bias);  mul_415 = p_h_31_mlp_down_proj_weight = p_h_31_mlp_down_proj_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_95: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_191, 0.0, False);  linear_191 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_191: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_189, dropout_95);  add_189 = dropout_95 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_289: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_191, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_97: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_289, 2)
            mean_64: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_97, [-1], True);  pow_97 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_192: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_64, 1e-05);  mean_64 = None
            rsqrt_64: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_192);  add_192 = None
            mul_416: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_289, rsqrt_64);  _to_copy_289 = rsqrt_64 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_290: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_416, dtype = torch.float32);  mul_416 = None
            mul_417: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_32_input_layernorm_weight, _to_copy_290);  p_h_32_input_layernorm_weight = _to_copy_290 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:404 in forward, code: hidden_states = hidden_states.transpose(1, 0)
            transpose_160: "f32[6, 1, 5120]" = torch.ops.aten.transpose.int(mul_417, 1, 0);  mul_417 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:405 in forward, code: query_layer = self.query(hidden_states)
            linear_192: "f32[6, 1, 5120]" = torch.ops.aten.linear.default(transpose_160, p_h_32_self_attention_query_weight);  p_h_32_self_attention_query_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:409 in forward, code: query_layer = query_layer.view(*new_tensor_shape)
            view_384: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(linear_192, [6, 1, 32, 160]);  linear_192 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:411 in forward, code: mixed_kv_layer = self.key_value(hidden_states)
            linear_193: "f32[6, 1, 10240]" = torch.ops.aten.linear.default(transpose_160, p_h_32_self_attention_key_value_weight);  transpose_160 = p_h_32_self_attention_key_value_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:415 in forward, code: mixed_kv_layer = mixed_kv_layer.view(*new_tensor_shape)
            view_385: "f32[6, 1, 32, 320]" = torch.ops.aten.view.default(linear_193, [6, 1, 32, 320]);  linear_193 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:381 in split_tensor_along_last_dim, code: tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)
            split_32 = torch.ops.aten.split.Tensor(view_385, 160, 3);  view_385 = None
            getitem_64: "f32[6, 1, 32, 160]" = split_32[0]
            getitem_65: "f32[6, 1, 32, 160]" = split_32[1];  split_32 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:423 in forward, code: query_layer = query_layer.view(output_size[2], output_size[0] * output_size[1], -1)
            view_386: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_384, [6, 32, -1]);  view_384 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:424 in forward, code: key_layer = key_layer.view(output_size[3], output_size[0] * output_size[1], -1)
            view_387: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_64, [6, 32, -1]);  getitem_64 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:112 in forward, code: self.inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, device=x.device).float() / self.dim))
            arange_65: "i64[80]" = torch.ops.aten.arange.start_step(0, 160, 2, device = device(type='cpu'), pin_memory = False)
            _to_copy_291: "f32[80]" = torch.ops.aten._to_copy.default(arange_65, dtype = torch.float32);  arange_65 = None
            div_32: "f32[80]" = torch.ops.aten.div.Tensor(_to_copy_291, 160);  _to_copy_291 = None
            pow_98: "f32[80]" = torch.ops.aten.pow.Scalar(30420.108888514722, div_32);  div_32 = None
            reciprocal_32: "f32[80]" = torch.ops.aten.reciprocal.default(pow_98);  pow_98 = None
            mul_418: "f32[80]" = torch.ops.aten.mul.Tensor(reciprocal_32, 1.0);  reciprocal_32 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:114 in forward, code: t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)
            arange_66: "f32[8192]" = torch.ops.aten.arange.default(8192, dtype = torch.float32, device = device(type='cpu'), pin_memory = False)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:115 in forward, code: freqs = torch.einsum('i,j->ij', t, self.inv_freq)
            einsum_64: "f32[8192, 80]" = torch.ops.aten.einsum.default('i,j->ij', [arange_66, mul_418]);  arange_66 = mul_418 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:117 in forward, code: emb = torch.cat((freqs, freqs), dim=-1).to(x.device)
            cat_96: "f32[8192, 160]" = torch.ops.aten.cat.default([einsum_64, einsum_64], -1);  einsum_64 = None
            _to_copy_292: "f32[8192, 160]" = torch.ops.aten._to_copy.default(cat_96, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  cat_96 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:121 in forward, code: self.cos_cached = self.mscale * emb.cos()[:, None, :].to(dtype)
            cos_32: "f32[8192, 160]" = torch.ops.aten.cos.default(_to_copy_292)
            slice_334: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(cos_32, 0, 0, 9223372036854775807);  cos_32 = None
            unsqueeze_72: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_334, 1);  slice_334 = None
            slice_335: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_72, 2, 0, 9223372036854775807);  unsqueeze_72 = None
            _to_copy_293: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_335, dtype = torch.float32);  slice_335 = None
            mul_419: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_293, 1.0);  _to_copy_293 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:122 in forward, code: self.sin_cached = self.mscale * emb.sin()[:, None, :].to(dtype)
            sin_32: "f32[8192, 160]" = torch.ops.aten.sin.default(_to_copy_292);  _to_copy_292 = None
            slice_336: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(sin_32, 0, 0, 9223372036854775807);  sin_32 = None
            unsqueeze_73: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_336, 1);  slice_336 = None
            slice_337: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_73, 2, 0, 9223372036854775807);  unsqueeze_73 = None
            _to_copy_294: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_337, dtype = torch.float32);  slice_337 = None
            mul_420: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_294, 1.0);  _to_copy_294 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:123 in forward, code: return self.cos_cached[:seq_len, ...], self.sin_cached[:seq_len, ...]
            alias_64: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_419);  mul_419 = None
            alias_65: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_420);  mul_420 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:133 in apply_rotary_pos_emb_torch, code: cos, sin = cos[offset:q.shape[0] + offset, ...], sin[offset:q.shape[0] + offset, ...]
            slice_338: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_64, 0, 0, 6);  alias_64 = None
            slice_339: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_65, 0, 0, 6);  alias_65 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_421: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_386, slice_338)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_340: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_386, 2, 0, 80)
            slice_341: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_386, 2, 80, 9223372036854775807);  view_386 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_64: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_341);  slice_341 = None
            cat_97: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_64, slice_340], 2);  neg_64 = slice_340 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_422: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_97, slice_339);  cat_97 = None
            add_193: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_421, mul_422);  mul_421 = mul_422 = None
            mul_423: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_387, slice_338);  slice_338 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_342: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_387, 2, 0, 80)
            slice_343: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_387, 2, 80, 9223372036854775807);  view_387 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_65: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_343);  slice_343 = None
            cat_98: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_65, slice_342], 2);  neg_65 = slice_342 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_424: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_98, slice_339);  cat_98 = slice_339 = None
            add_194: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_423, mul_424);  mul_423 = mul_424 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:448 in forward, code: query_layer = query_layer.reshape((s_query, bz, head, dim))
            view_388: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_193, [6, 1, 32, 160]);  add_193 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:449 in forward, code: key_layer = key_layer.reshape((s_key, bz, head, dim))
            view_389: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_194, [6, 1, 32, 160]);  add_194 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:458 in forward, code: query_layer = query_layer.reshape(s_query, bz * self.num_heads, dim)
            view_390: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_388, [6, 32, 160]);  view_388 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:460 in forward, code: key_layer = key_layer.reshape(s_key, bz * self.num_heads, dim)
            view_391: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_389, [6, 32, 160]);  view_389 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            transpose_161: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_390, 0, 1);  view_390 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:462 in forward, code: key_layer.transpose(0, 1).transpose(1, 2))
            transpose_162: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_391, 0, 1);  view_391 = None
            transpose_163: "f32[32, 160, 6]" = torch.ops.aten.transpose.int(transpose_162, 1, 2);  transpose_162 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            einsum_65: "f32[32, 6, 6]" = torch.ops.aten.einsum.default('bik,bkj->bij', [transpose_161, transpose_163]);  transpose_161 = transpose_163 = None
            mul_425: "f32[32, 6, 6]" = torch.ops.aten.mul.Tensor(einsum_65, 0.07905694150420949);  einsum_65 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:464 in forward, code: attention_scores = matmul_result.view(bz, self.num_heads, s_query, s_key)
            view_392: "f32[1, 32, 6, 6]" = torch.ops.aten.view.default(mul_425, [1, 32, 6, 6]);  mul_425 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:469 in forward, code: attn_weights = torch.masked_fill(attention_scores, attention_mask, torch.finfo(attention_scores.dtype).min)
            masked_fill_32: "f32[1, 32, 6, 6]" = torch.ops.aten.masked_fill.Scalar(view_392, or_1, -3.4028234663852886e+38);  view_392 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:470 in forward, code: attention_probs = F.softmax(attn_weights, dim=-1).to(input_dtype)  ##dtype = torch.float32
            softmax_32: "f32[1, 32, 6, 6]" = torch.ops.aten.softmax.int(masked_fill_32, -1);  masked_fill_32 = None
            _to_copy_295: "f32[1, 32, 6, 6]" = torch.ops.aten._to_copy.default(softmax_32, dtype = torch.float32);  softmax_32 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:471 in forward, code: attention_probs = self.attention_dropout(attention_probs)
            dropout_96: "f32[1, 32, 6, 6]" = torch.ops.aten.dropout.default(_to_copy_295, 0.0, False);  _to_copy_295 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:472 in forward, code: attention_probs_reshaped = attention_probs.view(bz * self.num_heads, s_query, s_key)
            view_393: "f32[32, 6, 6]" = torch.ops.aten.view.default(dropout_96, [32, 6, 6]);  dropout_96 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:474 in forward, code: value_layer = value_layer.reshape(s_key, bz * self.num_heads, dim)
            view_394: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_65, [6, 32, 160]);  getitem_65 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:475 in forward, code: context_layer = torch.bmm(attention_probs_reshaped, value_layer.transpose(0, 1))
            transpose_164: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_394, 0, 1);  view_394 = None
            bmm_32: "f32[32, 6, 160]" = torch.ops.aten.bmm.default(view_393, transpose_164);  view_393 = transpose_164 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:391 in _merge_heads, code: x = x.view(batch_size, self.num_heads, seq_length, self.head_dim)
            view_395: "f32[1, 32, 6, 160]" = torch.ops.aten.view.default(bmm_32, [1, 32, 6, 160]);  bmm_32 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:392 in _merge_heads, code: x = x.permute(0, 2, 1, 3)
            permute_32: "f32[1, 6, 32, 160]" = torch.ops.aten.permute.default(view_395, [0, 2, 1, 3]);  view_395 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:393 in _merge_heads, code: return x.reshape(batch_size, seq_length, self.num_heads * self.head_dim)
            clone_32: "f32[1, 6, 32, 160]" = torch.ops.aten.clone.default(permute_32, memory_format = torch.contiguous_format);  permute_32 = None
            _unsafe_view_32: "f32[1, 6, 5120]" = torch.ops.aten._unsafe_view.default(clone_32, [1, 6, 5120]);  clone_32 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:478 in forward, code: output_tensor = self.dense(context_layer)
            linear_194: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(_unsafe_view_32, p_h_32_self_attention_dense_weight, p_h_32_self_attention_dense_bias);  _unsafe_view_32 = p_h_32_self_attention_dense_weight = p_h_32_self_attention_dense_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_97: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_194, 0.0, False);  linear_194 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_195: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_191, dropout_97);  add_191 = dropout_97 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_296: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_195, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_99: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_296, 2)
            mean_65: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_99, [-1], True);  pow_99 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_196: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_65, 1e-05);  mean_65 = None
            rsqrt_65: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_196);  add_196 = None
            mul_426: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_296, rsqrt_65);  _to_copy_296 = rsqrt_65 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_297: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_426, dtype = torch.float32);  mul_426 = None
            mul_427: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_32_post_attention_layernorm_weight, _to_copy_297);  p_h_32_post_attention_layernorm_weight = _to_copy_297 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:499 in forward, code: intermediate_output = self.down_proj(F.silu(self.gate_proj(hidden_states)) * self.up_proj(hidden_states))
            linear_195: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_427, p_h_32_mlp_gate_proj_weight);  p_h_32_mlp_gate_proj_weight = None
            silu_32: "f32[1, 6, 12288]" = torch.ops.aten.silu.default(linear_195);  linear_195 = None
            linear_196: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_427, p_h_32_mlp_up_proj_weight);  mul_427 = p_h_32_mlp_up_proj_weight = None
            mul_428: "f32[1, 6, 12288]" = torch.ops.aten.mul.Tensor(silu_32, linear_196);  silu_32 = linear_196 = None
            linear_197: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(mul_428, p_h_32_mlp_down_proj_weight, p_h_32_mlp_down_proj_bias);  mul_428 = p_h_32_mlp_down_proj_weight = p_h_32_mlp_down_proj_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_98: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_197, 0.0, False);  linear_197 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_197: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_195, dropout_98);  add_195 = dropout_98 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_298: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_197, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_100: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_298, 2)
            mean_66: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_100, [-1], True);  pow_100 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_198: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_66, 1e-05);  mean_66 = None
            rsqrt_66: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_198);  add_198 = None
            mul_429: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_298, rsqrt_66);  _to_copy_298 = rsqrt_66 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_299: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_429, dtype = torch.float32);  mul_429 = None
            mul_430: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_33_input_layernorm_weight, _to_copy_299);  p_h_33_input_layernorm_weight = _to_copy_299 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:404 in forward, code: hidden_states = hidden_states.transpose(1, 0)
            transpose_165: "f32[6, 1, 5120]" = torch.ops.aten.transpose.int(mul_430, 1, 0);  mul_430 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:405 in forward, code: query_layer = self.query(hidden_states)
            linear_198: "f32[6, 1, 5120]" = torch.ops.aten.linear.default(transpose_165, p_h_33_self_attention_query_weight);  p_h_33_self_attention_query_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:409 in forward, code: query_layer = query_layer.view(*new_tensor_shape)
            view_396: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(linear_198, [6, 1, 32, 160]);  linear_198 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:411 in forward, code: mixed_kv_layer = self.key_value(hidden_states)
            linear_199: "f32[6, 1, 10240]" = torch.ops.aten.linear.default(transpose_165, p_h_33_self_attention_key_value_weight);  transpose_165 = p_h_33_self_attention_key_value_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:415 in forward, code: mixed_kv_layer = mixed_kv_layer.view(*new_tensor_shape)
            view_397: "f32[6, 1, 32, 320]" = torch.ops.aten.view.default(linear_199, [6, 1, 32, 320]);  linear_199 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:381 in split_tensor_along_last_dim, code: tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)
            split_33 = torch.ops.aten.split.Tensor(view_397, 160, 3);  view_397 = None
            getitem_66: "f32[6, 1, 32, 160]" = split_33[0]
            getitem_67: "f32[6, 1, 32, 160]" = split_33[1];  split_33 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:423 in forward, code: query_layer = query_layer.view(output_size[2], output_size[0] * output_size[1], -1)
            view_398: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_396, [6, 32, -1]);  view_396 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:424 in forward, code: key_layer = key_layer.view(output_size[3], output_size[0] * output_size[1], -1)
            view_399: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_66, [6, 32, -1]);  getitem_66 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:112 in forward, code: self.inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, device=x.device).float() / self.dim))
            arange_67: "i64[80]" = torch.ops.aten.arange.start_step(0, 160, 2, device = device(type='cpu'), pin_memory = False)
            _to_copy_300: "f32[80]" = torch.ops.aten._to_copy.default(arange_67, dtype = torch.float32);  arange_67 = None
            div_33: "f32[80]" = torch.ops.aten.div.Tensor(_to_copy_300, 160);  _to_copy_300 = None
            pow_101: "f32[80]" = torch.ops.aten.pow.Scalar(30420.108888514722, div_33);  div_33 = None
            reciprocal_33: "f32[80]" = torch.ops.aten.reciprocal.default(pow_101);  pow_101 = None
            mul_431: "f32[80]" = torch.ops.aten.mul.Tensor(reciprocal_33, 1.0);  reciprocal_33 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:114 in forward, code: t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)
            arange_68: "f32[8192]" = torch.ops.aten.arange.default(8192, dtype = torch.float32, device = device(type='cpu'), pin_memory = False)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:115 in forward, code: freqs = torch.einsum('i,j->ij', t, self.inv_freq)
            einsum_66: "f32[8192, 80]" = torch.ops.aten.einsum.default('i,j->ij', [arange_68, mul_431]);  arange_68 = mul_431 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:117 in forward, code: emb = torch.cat((freqs, freqs), dim=-1).to(x.device)
            cat_99: "f32[8192, 160]" = torch.ops.aten.cat.default([einsum_66, einsum_66], -1);  einsum_66 = None
            _to_copy_301: "f32[8192, 160]" = torch.ops.aten._to_copy.default(cat_99, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  cat_99 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:121 in forward, code: self.cos_cached = self.mscale * emb.cos()[:, None, :].to(dtype)
            cos_33: "f32[8192, 160]" = torch.ops.aten.cos.default(_to_copy_301)
            slice_344: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(cos_33, 0, 0, 9223372036854775807);  cos_33 = None
            unsqueeze_74: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_344, 1);  slice_344 = None
            slice_345: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_74, 2, 0, 9223372036854775807);  unsqueeze_74 = None
            _to_copy_302: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_345, dtype = torch.float32);  slice_345 = None
            mul_432: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_302, 1.0);  _to_copy_302 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:122 in forward, code: self.sin_cached = self.mscale * emb.sin()[:, None, :].to(dtype)
            sin_33: "f32[8192, 160]" = torch.ops.aten.sin.default(_to_copy_301);  _to_copy_301 = None
            slice_346: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(sin_33, 0, 0, 9223372036854775807);  sin_33 = None
            unsqueeze_75: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_346, 1);  slice_346 = None
            slice_347: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_75, 2, 0, 9223372036854775807);  unsqueeze_75 = None
            _to_copy_303: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_347, dtype = torch.float32);  slice_347 = None
            mul_433: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_303, 1.0);  _to_copy_303 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:123 in forward, code: return self.cos_cached[:seq_len, ...], self.sin_cached[:seq_len, ...]
            alias_66: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_432);  mul_432 = None
            alias_67: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_433);  mul_433 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:133 in apply_rotary_pos_emb_torch, code: cos, sin = cos[offset:q.shape[0] + offset, ...], sin[offset:q.shape[0] + offset, ...]
            slice_348: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_66, 0, 0, 6);  alias_66 = None
            slice_349: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_67, 0, 0, 6);  alias_67 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_434: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_398, slice_348)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_350: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_398, 2, 0, 80)
            slice_351: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_398, 2, 80, 9223372036854775807);  view_398 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_66: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_351);  slice_351 = None
            cat_100: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_66, slice_350], 2);  neg_66 = slice_350 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_435: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_100, slice_349);  cat_100 = None
            add_199: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_434, mul_435);  mul_434 = mul_435 = None
            mul_436: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_399, slice_348);  slice_348 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_352: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_399, 2, 0, 80)
            slice_353: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_399, 2, 80, 9223372036854775807);  view_399 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_67: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_353);  slice_353 = None
            cat_101: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_67, slice_352], 2);  neg_67 = slice_352 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_437: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_101, slice_349);  cat_101 = slice_349 = None
            add_200: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_436, mul_437);  mul_436 = mul_437 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:448 in forward, code: query_layer = query_layer.reshape((s_query, bz, head, dim))
            view_400: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_199, [6, 1, 32, 160]);  add_199 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:449 in forward, code: key_layer = key_layer.reshape((s_key, bz, head, dim))
            view_401: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_200, [6, 1, 32, 160]);  add_200 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:458 in forward, code: query_layer = query_layer.reshape(s_query, bz * self.num_heads, dim)
            view_402: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_400, [6, 32, 160]);  view_400 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:460 in forward, code: key_layer = key_layer.reshape(s_key, bz * self.num_heads, dim)
            view_403: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_401, [6, 32, 160]);  view_401 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            transpose_166: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_402, 0, 1);  view_402 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:462 in forward, code: key_layer.transpose(0, 1).transpose(1, 2))
            transpose_167: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_403, 0, 1);  view_403 = None
            transpose_168: "f32[32, 160, 6]" = torch.ops.aten.transpose.int(transpose_167, 1, 2);  transpose_167 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            einsum_67: "f32[32, 6, 6]" = torch.ops.aten.einsum.default('bik,bkj->bij', [transpose_166, transpose_168]);  transpose_166 = transpose_168 = None
            mul_438: "f32[32, 6, 6]" = torch.ops.aten.mul.Tensor(einsum_67, 0.07905694150420949);  einsum_67 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:464 in forward, code: attention_scores = matmul_result.view(bz, self.num_heads, s_query, s_key)
            view_404: "f32[1, 32, 6, 6]" = torch.ops.aten.view.default(mul_438, [1, 32, 6, 6]);  mul_438 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:469 in forward, code: attn_weights = torch.masked_fill(attention_scores, attention_mask, torch.finfo(attention_scores.dtype).min)
            masked_fill_33: "f32[1, 32, 6, 6]" = torch.ops.aten.masked_fill.Scalar(view_404, or_1, -3.4028234663852886e+38);  view_404 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:470 in forward, code: attention_probs = F.softmax(attn_weights, dim=-1).to(input_dtype)  ##dtype = torch.float32
            softmax_33: "f32[1, 32, 6, 6]" = torch.ops.aten.softmax.int(masked_fill_33, -1);  masked_fill_33 = None
            _to_copy_304: "f32[1, 32, 6, 6]" = torch.ops.aten._to_copy.default(softmax_33, dtype = torch.float32);  softmax_33 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:471 in forward, code: attention_probs = self.attention_dropout(attention_probs)
            dropout_99: "f32[1, 32, 6, 6]" = torch.ops.aten.dropout.default(_to_copy_304, 0.0, False);  _to_copy_304 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:472 in forward, code: attention_probs_reshaped = attention_probs.view(bz * self.num_heads, s_query, s_key)
            view_405: "f32[32, 6, 6]" = torch.ops.aten.view.default(dropout_99, [32, 6, 6]);  dropout_99 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:474 in forward, code: value_layer = value_layer.reshape(s_key, bz * self.num_heads, dim)
            view_406: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_67, [6, 32, 160]);  getitem_67 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:475 in forward, code: context_layer = torch.bmm(attention_probs_reshaped, value_layer.transpose(0, 1))
            transpose_169: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_406, 0, 1);  view_406 = None
            bmm_33: "f32[32, 6, 160]" = torch.ops.aten.bmm.default(view_405, transpose_169);  view_405 = transpose_169 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:391 in _merge_heads, code: x = x.view(batch_size, self.num_heads, seq_length, self.head_dim)
            view_407: "f32[1, 32, 6, 160]" = torch.ops.aten.view.default(bmm_33, [1, 32, 6, 160]);  bmm_33 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:392 in _merge_heads, code: x = x.permute(0, 2, 1, 3)
            permute_33: "f32[1, 6, 32, 160]" = torch.ops.aten.permute.default(view_407, [0, 2, 1, 3]);  view_407 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:393 in _merge_heads, code: return x.reshape(batch_size, seq_length, self.num_heads * self.head_dim)
            clone_33: "f32[1, 6, 32, 160]" = torch.ops.aten.clone.default(permute_33, memory_format = torch.contiguous_format);  permute_33 = None
            _unsafe_view_33: "f32[1, 6, 5120]" = torch.ops.aten._unsafe_view.default(clone_33, [1, 6, 5120]);  clone_33 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:478 in forward, code: output_tensor = self.dense(context_layer)
            linear_200: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(_unsafe_view_33, p_h_33_self_attention_dense_weight, p_h_33_self_attention_dense_bias);  _unsafe_view_33 = p_h_33_self_attention_dense_weight = p_h_33_self_attention_dense_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_100: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_200, 0.0, False);  linear_200 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_201: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_197, dropout_100);  add_197 = dropout_100 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_305: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_201, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_102: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_305, 2)
            mean_67: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_102, [-1], True);  pow_102 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_202: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_67, 1e-05);  mean_67 = None
            rsqrt_67: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_202);  add_202 = None
            mul_439: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_305, rsqrt_67);  _to_copy_305 = rsqrt_67 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_306: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_439, dtype = torch.float32);  mul_439 = None
            mul_440: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_33_post_attention_layernorm_weight, _to_copy_306);  p_h_33_post_attention_layernorm_weight = _to_copy_306 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:499 in forward, code: intermediate_output = self.down_proj(F.silu(self.gate_proj(hidden_states)) * self.up_proj(hidden_states))
            linear_201: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_440, p_h_33_mlp_gate_proj_weight);  p_h_33_mlp_gate_proj_weight = None
            silu_33: "f32[1, 6, 12288]" = torch.ops.aten.silu.default(linear_201);  linear_201 = None
            linear_202: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_440, p_h_33_mlp_up_proj_weight);  mul_440 = p_h_33_mlp_up_proj_weight = None
            mul_441: "f32[1, 6, 12288]" = torch.ops.aten.mul.Tensor(silu_33, linear_202);  silu_33 = linear_202 = None
            linear_203: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(mul_441, p_h_33_mlp_down_proj_weight, p_h_33_mlp_down_proj_bias);  mul_441 = p_h_33_mlp_down_proj_weight = p_h_33_mlp_down_proj_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_101: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_203, 0.0, False);  linear_203 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_203: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_201, dropout_101);  add_201 = dropout_101 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_307: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_203, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_103: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_307, 2)
            mean_68: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_103, [-1], True);  pow_103 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_204: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_68, 1e-05);  mean_68 = None
            rsqrt_68: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_204);  add_204 = None
            mul_442: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_307, rsqrt_68);  _to_copy_307 = rsqrt_68 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_308: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_442, dtype = torch.float32);  mul_442 = None
            mul_443: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_34_input_layernorm_weight, _to_copy_308);  p_h_34_input_layernorm_weight = _to_copy_308 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:404 in forward, code: hidden_states = hidden_states.transpose(1, 0)
            transpose_170: "f32[6, 1, 5120]" = torch.ops.aten.transpose.int(mul_443, 1, 0);  mul_443 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:405 in forward, code: query_layer = self.query(hidden_states)
            linear_204: "f32[6, 1, 5120]" = torch.ops.aten.linear.default(transpose_170, p_h_34_self_attention_query_weight);  p_h_34_self_attention_query_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:409 in forward, code: query_layer = query_layer.view(*new_tensor_shape)
            view_408: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(linear_204, [6, 1, 32, 160]);  linear_204 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:411 in forward, code: mixed_kv_layer = self.key_value(hidden_states)
            linear_205: "f32[6, 1, 10240]" = torch.ops.aten.linear.default(transpose_170, p_h_34_self_attention_key_value_weight);  transpose_170 = p_h_34_self_attention_key_value_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:415 in forward, code: mixed_kv_layer = mixed_kv_layer.view(*new_tensor_shape)
            view_409: "f32[6, 1, 32, 320]" = torch.ops.aten.view.default(linear_205, [6, 1, 32, 320]);  linear_205 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:381 in split_tensor_along_last_dim, code: tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)
            split_34 = torch.ops.aten.split.Tensor(view_409, 160, 3);  view_409 = None
            getitem_68: "f32[6, 1, 32, 160]" = split_34[0]
            getitem_69: "f32[6, 1, 32, 160]" = split_34[1];  split_34 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:423 in forward, code: query_layer = query_layer.view(output_size[2], output_size[0] * output_size[1], -1)
            view_410: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_408, [6, 32, -1]);  view_408 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:424 in forward, code: key_layer = key_layer.view(output_size[3], output_size[0] * output_size[1], -1)
            view_411: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_68, [6, 32, -1]);  getitem_68 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:112 in forward, code: self.inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, device=x.device).float() / self.dim))
            arange_69: "i64[80]" = torch.ops.aten.arange.start_step(0, 160, 2, device = device(type='cpu'), pin_memory = False)
            _to_copy_309: "f32[80]" = torch.ops.aten._to_copy.default(arange_69, dtype = torch.float32);  arange_69 = None
            div_34: "f32[80]" = torch.ops.aten.div.Tensor(_to_copy_309, 160);  _to_copy_309 = None
            pow_104: "f32[80]" = torch.ops.aten.pow.Scalar(30420.108888514722, div_34);  div_34 = None
            reciprocal_34: "f32[80]" = torch.ops.aten.reciprocal.default(pow_104);  pow_104 = None
            mul_444: "f32[80]" = torch.ops.aten.mul.Tensor(reciprocal_34, 1.0);  reciprocal_34 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:114 in forward, code: t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)
            arange_70: "f32[8192]" = torch.ops.aten.arange.default(8192, dtype = torch.float32, device = device(type='cpu'), pin_memory = False)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:115 in forward, code: freqs = torch.einsum('i,j->ij', t, self.inv_freq)
            einsum_68: "f32[8192, 80]" = torch.ops.aten.einsum.default('i,j->ij', [arange_70, mul_444]);  arange_70 = mul_444 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:117 in forward, code: emb = torch.cat((freqs, freqs), dim=-1).to(x.device)
            cat_102: "f32[8192, 160]" = torch.ops.aten.cat.default([einsum_68, einsum_68], -1);  einsum_68 = None
            _to_copy_310: "f32[8192, 160]" = torch.ops.aten._to_copy.default(cat_102, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  cat_102 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:121 in forward, code: self.cos_cached = self.mscale * emb.cos()[:, None, :].to(dtype)
            cos_34: "f32[8192, 160]" = torch.ops.aten.cos.default(_to_copy_310)
            slice_354: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(cos_34, 0, 0, 9223372036854775807);  cos_34 = None
            unsqueeze_76: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_354, 1);  slice_354 = None
            slice_355: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_76, 2, 0, 9223372036854775807);  unsqueeze_76 = None
            _to_copy_311: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_355, dtype = torch.float32);  slice_355 = None
            mul_445: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_311, 1.0);  _to_copy_311 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:122 in forward, code: self.sin_cached = self.mscale * emb.sin()[:, None, :].to(dtype)
            sin_34: "f32[8192, 160]" = torch.ops.aten.sin.default(_to_copy_310);  _to_copy_310 = None
            slice_356: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(sin_34, 0, 0, 9223372036854775807);  sin_34 = None
            unsqueeze_77: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_356, 1);  slice_356 = None
            slice_357: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_77, 2, 0, 9223372036854775807);  unsqueeze_77 = None
            _to_copy_312: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_357, dtype = torch.float32);  slice_357 = None
            mul_446: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_312, 1.0);  _to_copy_312 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:123 in forward, code: return self.cos_cached[:seq_len, ...], self.sin_cached[:seq_len, ...]
            alias_68: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_445);  mul_445 = None
            alias_69: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_446);  mul_446 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:133 in apply_rotary_pos_emb_torch, code: cos, sin = cos[offset:q.shape[0] + offset, ...], sin[offset:q.shape[0] + offset, ...]
            slice_358: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_68, 0, 0, 6);  alias_68 = None
            slice_359: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_69, 0, 0, 6);  alias_69 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_447: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_410, slice_358)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_360: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_410, 2, 0, 80)
            slice_361: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_410, 2, 80, 9223372036854775807);  view_410 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_68: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_361);  slice_361 = None
            cat_103: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_68, slice_360], 2);  neg_68 = slice_360 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_448: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_103, slice_359);  cat_103 = None
            add_205: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_447, mul_448);  mul_447 = mul_448 = None
            mul_449: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_411, slice_358);  slice_358 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_362: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_411, 2, 0, 80)
            slice_363: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_411, 2, 80, 9223372036854775807);  view_411 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_69: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_363);  slice_363 = None
            cat_104: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_69, slice_362], 2);  neg_69 = slice_362 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_450: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_104, slice_359);  cat_104 = slice_359 = None
            add_206: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_449, mul_450);  mul_449 = mul_450 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:448 in forward, code: query_layer = query_layer.reshape((s_query, bz, head, dim))
            view_412: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_205, [6, 1, 32, 160]);  add_205 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:449 in forward, code: key_layer = key_layer.reshape((s_key, bz, head, dim))
            view_413: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_206, [6, 1, 32, 160]);  add_206 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:458 in forward, code: query_layer = query_layer.reshape(s_query, bz * self.num_heads, dim)
            view_414: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_412, [6, 32, 160]);  view_412 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:460 in forward, code: key_layer = key_layer.reshape(s_key, bz * self.num_heads, dim)
            view_415: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_413, [6, 32, 160]);  view_413 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            transpose_171: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_414, 0, 1);  view_414 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:462 in forward, code: key_layer.transpose(0, 1).transpose(1, 2))
            transpose_172: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_415, 0, 1);  view_415 = None
            transpose_173: "f32[32, 160, 6]" = torch.ops.aten.transpose.int(transpose_172, 1, 2);  transpose_172 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            einsum_69: "f32[32, 6, 6]" = torch.ops.aten.einsum.default('bik,bkj->bij', [transpose_171, transpose_173]);  transpose_171 = transpose_173 = None
            mul_451: "f32[32, 6, 6]" = torch.ops.aten.mul.Tensor(einsum_69, 0.07905694150420949);  einsum_69 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:464 in forward, code: attention_scores = matmul_result.view(bz, self.num_heads, s_query, s_key)
            view_416: "f32[1, 32, 6, 6]" = torch.ops.aten.view.default(mul_451, [1, 32, 6, 6]);  mul_451 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:469 in forward, code: attn_weights = torch.masked_fill(attention_scores, attention_mask, torch.finfo(attention_scores.dtype).min)
            masked_fill_34: "f32[1, 32, 6, 6]" = torch.ops.aten.masked_fill.Scalar(view_416, or_1, -3.4028234663852886e+38);  view_416 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:470 in forward, code: attention_probs = F.softmax(attn_weights, dim=-1).to(input_dtype)  ##dtype = torch.float32
            softmax_34: "f32[1, 32, 6, 6]" = torch.ops.aten.softmax.int(masked_fill_34, -1);  masked_fill_34 = None
            _to_copy_313: "f32[1, 32, 6, 6]" = torch.ops.aten._to_copy.default(softmax_34, dtype = torch.float32);  softmax_34 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:471 in forward, code: attention_probs = self.attention_dropout(attention_probs)
            dropout_102: "f32[1, 32, 6, 6]" = torch.ops.aten.dropout.default(_to_copy_313, 0.0, False);  _to_copy_313 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:472 in forward, code: attention_probs_reshaped = attention_probs.view(bz * self.num_heads, s_query, s_key)
            view_417: "f32[32, 6, 6]" = torch.ops.aten.view.default(dropout_102, [32, 6, 6]);  dropout_102 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:474 in forward, code: value_layer = value_layer.reshape(s_key, bz * self.num_heads, dim)
            view_418: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_69, [6, 32, 160]);  getitem_69 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:475 in forward, code: context_layer = torch.bmm(attention_probs_reshaped, value_layer.transpose(0, 1))
            transpose_174: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_418, 0, 1);  view_418 = None
            bmm_34: "f32[32, 6, 160]" = torch.ops.aten.bmm.default(view_417, transpose_174);  view_417 = transpose_174 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:391 in _merge_heads, code: x = x.view(batch_size, self.num_heads, seq_length, self.head_dim)
            view_419: "f32[1, 32, 6, 160]" = torch.ops.aten.view.default(bmm_34, [1, 32, 6, 160]);  bmm_34 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:392 in _merge_heads, code: x = x.permute(0, 2, 1, 3)
            permute_34: "f32[1, 6, 32, 160]" = torch.ops.aten.permute.default(view_419, [0, 2, 1, 3]);  view_419 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:393 in _merge_heads, code: return x.reshape(batch_size, seq_length, self.num_heads * self.head_dim)
            clone_34: "f32[1, 6, 32, 160]" = torch.ops.aten.clone.default(permute_34, memory_format = torch.contiguous_format);  permute_34 = None
            _unsafe_view_34: "f32[1, 6, 5120]" = torch.ops.aten._unsafe_view.default(clone_34, [1, 6, 5120]);  clone_34 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:478 in forward, code: output_tensor = self.dense(context_layer)
            linear_206: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(_unsafe_view_34, p_h_34_self_attention_dense_weight, p_h_34_self_attention_dense_bias);  _unsafe_view_34 = p_h_34_self_attention_dense_weight = p_h_34_self_attention_dense_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_103: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_206, 0.0, False);  linear_206 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_207: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_203, dropout_103);  add_203 = dropout_103 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_314: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_207, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_105: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_314, 2)
            mean_69: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_105, [-1], True);  pow_105 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_208: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_69, 1e-05);  mean_69 = None
            rsqrt_69: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_208);  add_208 = None
            mul_452: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_314, rsqrt_69);  _to_copy_314 = rsqrt_69 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_315: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_452, dtype = torch.float32);  mul_452 = None
            mul_453: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_34_post_attention_layernorm_weight, _to_copy_315);  p_h_34_post_attention_layernorm_weight = _to_copy_315 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:499 in forward, code: intermediate_output = self.down_proj(F.silu(self.gate_proj(hidden_states)) * self.up_proj(hidden_states))
            linear_207: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_453, p_h_34_mlp_gate_proj_weight);  p_h_34_mlp_gate_proj_weight = None
            silu_34: "f32[1, 6, 12288]" = torch.ops.aten.silu.default(linear_207);  linear_207 = None
            linear_208: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_453, p_h_34_mlp_up_proj_weight);  mul_453 = p_h_34_mlp_up_proj_weight = None
            mul_454: "f32[1, 6, 12288]" = torch.ops.aten.mul.Tensor(silu_34, linear_208);  silu_34 = linear_208 = None
            linear_209: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(mul_454, p_h_34_mlp_down_proj_weight, p_h_34_mlp_down_proj_bias);  mul_454 = p_h_34_mlp_down_proj_weight = p_h_34_mlp_down_proj_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_104: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_209, 0.0, False);  linear_209 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_209: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_207, dropout_104);  add_207 = dropout_104 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_316: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_209, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_106: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_316, 2)
            mean_70: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_106, [-1], True);  pow_106 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_210: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_70, 1e-05);  mean_70 = None
            rsqrt_70: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_210);  add_210 = None
            mul_455: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_316, rsqrt_70);  _to_copy_316 = rsqrt_70 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_317: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_455, dtype = torch.float32);  mul_455 = None
            mul_456: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_35_input_layernorm_weight, _to_copy_317);  p_h_35_input_layernorm_weight = _to_copy_317 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:404 in forward, code: hidden_states = hidden_states.transpose(1, 0)
            transpose_175: "f32[6, 1, 5120]" = torch.ops.aten.transpose.int(mul_456, 1, 0);  mul_456 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:405 in forward, code: query_layer = self.query(hidden_states)
            linear_210: "f32[6, 1, 5120]" = torch.ops.aten.linear.default(transpose_175, p_h_35_self_attention_query_weight);  p_h_35_self_attention_query_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:409 in forward, code: query_layer = query_layer.view(*new_tensor_shape)
            view_420: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(linear_210, [6, 1, 32, 160]);  linear_210 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:411 in forward, code: mixed_kv_layer = self.key_value(hidden_states)
            linear_211: "f32[6, 1, 10240]" = torch.ops.aten.linear.default(transpose_175, p_h_35_self_attention_key_value_weight);  transpose_175 = p_h_35_self_attention_key_value_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:415 in forward, code: mixed_kv_layer = mixed_kv_layer.view(*new_tensor_shape)
            view_421: "f32[6, 1, 32, 320]" = torch.ops.aten.view.default(linear_211, [6, 1, 32, 320]);  linear_211 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:381 in split_tensor_along_last_dim, code: tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)
            split_35 = torch.ops.aten.split.Tensor(view_421, 160, 3);  view_421 = None
            getitem_70: "f32[6, 1, 32, 160]" = split_35[0]
            getitem_71: "f32[6, 1, 32, 160]" = split_35[1];  split_35 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:423 in forward, code: query_layer = query_layer.view(output_size[2], output_size[0] * output_size[1], -1)
            view_422: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_420, [6, 32, -1]);  view_420 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:424 in forward, code: key_layer = key_layer.view(output_size[3], output_size[0] * output_size[1], -1)
            view_423: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_70, [6, 32, -1]);  getitem_70 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:112 in forward, code: self.inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, device=x.device).float() / self.dim))
            arange_71: "i64[80]" = torch.ops.aten.arange.start_step(0, 160, 2, device = device(type='cpu'), pin_memory = False)
            _to_copy_318: "f32[80]" = torch.ops.aten._to_copy.default(arange_71, dtype = torch.float32);  arange_71 = None
            div_35: "f32[80]" = torch.ops.aten.div.Tensor(_to_copy_318, 160);  _to_copy_318 = None
            pow_107: "f32[80]" = torch.ops.aten.pow.Scalar(30420.108888514722, div_35);  div_35 = None
            reciprocal_35: "f32[80]" = torch.ops.aten.reciprocal.default(pow_107);  pow_107 = None
            mul_457: "f32[80]" = torch.ops.aten.mul.Tensor(reciprocal_35, 1.0);  reciprocal_35 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:114 in forward, code: t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)
            arange_72: "f32[8192]" = torch.ops.aten.arange.default(8192, dtype = torch.float32, device = device(type='cpu'), pin_memory = False)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:115 in forward, code: freqs = torch.einsum('i,j->ij', t, self.inv_freq)
            einsum_70: "f32[8192, 80]" = torch.ops.aten.einsum.default('i,j->ij', [arange_72, mul_457]);  arange_72 = mul_457 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:117 in forward, code: emb = torch.cat((freqs, freqs), dim=-1).to(x.device)
            cat_105: "f32[8192, 160]" = torch.ops.aten.cat.default([einsum_70, einsum_70], -1);  einsum_70 = None
            _to_copy_319: "f32[8192, 160]" = torch.ops.aten._to_copy.default(cat_105, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  cat_105 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:121 in forward, code: self.cos_cached = self.mscale * emb.cos()[:, None, :].to(dtype)
            cos_35: "f32[8192, 160]" = torch.ops.aten.cos.default(_to_copy_319)
            slice_364: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(cos_35, 0, 0, 9223372036854775807);  cos_35 = None
            unsqueeze_78: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_364, 1);  slice_364 = None
            slice_365: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_78, 2, 0, 9223372036854775807);  unsqueeze_78 = None
            _to_copy_320: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_365, dtype = torch.float32);  slice_365 = None
            mul_458: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_320, 1.0);  _to_copy_320 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:122 in forward, code: self.sin_cached = self.mscale * emb.sin()[:, None, :].to(dtype)
            sin_35: "f32[8192, 160]" = torch.ops.aten.sin.default(_to_copy_319);  _to_copy_319 = None
            slice_366: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(sin_35, 0, 0, 9223372036854775807);  sin_35 = None
            unsqueeze_79: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_366, 1);  slice_366 = None
            slice_367: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_79, 2, 0, 9223372036854775807);  unsqueeze_79 = None
            _to_copy_321: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_367, dtype = torch.float32);  slice_367 = None
            mul_459: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_321, 1.0);  _to_copy_321 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:123 in forward, code: return self.cos_cached[:seq_len, ...], self.sin_cached[:seq_len, ...]
            alias_70: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_458);  mul_458 = None
            alias_71: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_459);  mul_459 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:133 in apply_rotary_pos_emb_torch, code: cos, sin = cos[offset:q.shape[0] + offset, ...], sin[offset:q.shape[0] + offset, ...]
            slice_368: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_70, 0, 0, 6);  alias_70 = None
            slice_369: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_71, 0, 0, 6);  alias_71 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_460: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_422, slice_368)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_370: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_422, 2, 0, 80)
            slice_371: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_422, 2, 80, 9223372036854775807);  view_422 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_70: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_371);  slice_371 = None
            cat_106: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_70, slice_370], 2);  neg_70 = slice_370 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_461: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_106, slice_369);  cat_106 = None
            add_211: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_460, mul_461);  mul_460 = mul_461 = None
            mul_462: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_423, slice_368);  slice_368 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_372: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_423, 2, 0, 80)
            slice_373: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_423, 2, 80, 9223372036854775807);  view_423 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_71: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_373);  slice_373 = None
            cat_107: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_71, slice_372], 2);  neg_71 = slice_372 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_463: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_107, slice_369);  cat_107 = slice_369 = None
            add_212: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_462, mul_463);  mul_462 = mul_463 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:448 in forward, code: query_layer = query_layer.reshape((s_query, bz, head, dim))
            view_424: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_211, [6, 1, 32, 160]);  add_211 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:449 in forward, code: key_layer = key_layer.reshape((s_key, bz, head, dim))
            view_425: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_212, [6, 1, 32, 160]);  add_212 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:458 in forward, code: query_layer = query_layer.reshape(s_query, bz * self.num_heads, dim)
            view_426: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_424, [6, 32, 160]);  view_424 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:460 in forward, code: key_layer = key_layer.reshape(s_key, bz * self.num_heads, dim)
            view_427: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_425, [6, 32, 160]);  view_425 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            transpose_176: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_426, 0, 1);  view_426 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:462 in forward, code: key_layer.transpose(0, 1).transpose(1, 2))
            transpose_177: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_427, 0, 1);  view_427 = None
            transpose_178: "f32[32, 160, 6]" = torch.ops.aten.transpose.int(transpose_177, 1, 2);  transpose_177 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            einsum_71: "f32[32, 6, 6]" = torch.ops.aten.einsum.default('bik,bkj->bij', [transpose_176, transpose_178]);  transpose_176 = transpose_178 = None
            mul_464: "f32[32, 6, 6]" = torch.ops.aten.mul.Tensor(einsum_71, 0.07905694150420949);  einsum_71 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:464 in forward, code: attention_scores = matmul_result.view(bz, self.num_heads, s_query, s_key)
            view_428: "f32[1, 32, 6, 6]" = torch.ops.aten.view.default(mul_464, [1, 32, 6, 6]);  mul_464 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:469 in forward, code: attn_weights = torch.masked_fill(attention_scores, attention_mask, torch.finfo(attention_scores.dtype).min)
            masked_fill_35: "f32[1, 32, 6, 6]" = torch.ops.aten.masked_fill.Scalar(view_428, or_1, -3.4028234663852886e+38);  view_428 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:470 in forward, code: attention_probs = F.softmax(attn_weights, dim=-1).to(input_dtype)  ##dtype = torch.float32
            softmax_35: "f32[1, 32, 6, 6]" = torch.ops.aten.softmax.int(masked_fill_35, -1);  masked_fill_35 = None
            _to_copy_322: "f32[1, 32, 6, 6]" = torch.ops.aten._to_copy.default(softmax_35, dtype = torch.float32);  softmax_35 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:471 in forward, code: attention_probs = self.attention_dropout(attention_probs)
            dropout_105: "f32[1, 32, 6, 6]" = torch.ops.aten.dropout.default(_to_copy_322, 0.0, False);  _to_copy_322 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:472 in forward, code: attention_probs_reshaped = attention_probs.view(bz * self.num_heads, s_query, s_key)
            view_429: "f32[32, 6, 6]" = torch.ops.aten.view.default(dropout_105, [32, 6, 6]);  dropout_105 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:474 in forward, code: value_layer = value_layer.reshape(s_key, bz * self.num_heads, dim)
            view_430: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_71, [6, 32, 160]);  getitem_71 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:475 in forward, code: context_layer = torch.bmm(attention_probs_reshaped, value_layer.transpose(0, 1))
            transpose_179: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_430, 0, 1);  view_430 = None
            bmm_35: "f32[32, 6, 160]" = torch.ops.aten.bmm.default(view_429, transpose_179);  view_429 = transpose_179 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:391 in _merge_heads, code: x = x.view(batch_size, self.num_heads, seq_length, self.head_dim)
            view_431: "f32[1, 32, 6, 160]" = torch.ops.aten.view.default(bmm_35, [1, 32, 6, 160]);  bmm_35 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:392 in _merge_heads, code: x = x.permute(0, 2, 1, 3)
            permute_35: "f32[1, 6, 32, 160]" = torch.ops.aten.permute.default(view_431, [0, 2, 1, 3]);  view_431 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:393 in _merge_heads, code: return x.reshape(batch_size, seq_length, self.num_heads * self.head_dim)
            clone_35: "f32[1, 6, 32, 160]" = torch.ops.aten.clone.default(permute_35, memory_format = torch.contiguous_format);  permute_35 = None
            _unsafe_view_35: "f32[1, 6, 5120]" = torch.ops.aten._unsafe_view.default(clone_35, [1, 6, 5120]);  clone_35 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:478 in forward, code: output_tensor = self.dense(context_layer)
            linear_212: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(_unsafe_view_35, p_h_35_self_attention_dense_weight, p_h_35_self_attention_dense_bias);  _unsafe_view_35 = p_h_35_self_attention_dense_weight = p_h_35_self_attention_dense_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_106: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_212, 0.0, False);  linear_212 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_213: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_209, dropout_106);  add_209 = dropout_106 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_323: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_213, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_108: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_323, 2)
            mean_71: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_108, [-1], True);  pow_108 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_214: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_71, 1e-05);  mean_71 = None
            rsqrt_71: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_214);  add_214 = None
            mul_465: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_323, rsqrt_71);  _to_copy_323 = rsqrt_71 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_324: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_465, dtype = torch.float32);  mul_465 = None
            mul_466: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_35_post_attention_layernorm_weight, _to_copy_324);  p_h_35_post_attention_layernorm_weight = _to_copy_324 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:499 in forward, code: intermediate_output = self.down_proj(F.silu(self.gate_proj(hidden_states)) * self.up_proj(hidden_states))
            linear_213: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_466, p_h_35_mlp_gate_proj_weight);  p_h_35_mlp_gate_proj_weight = None
            silu_35: "f32[1, 6, 12288]" = torch.ops.aten.silu.default(linear_213);  linear_213 = None
            linear_214: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_466, p_h_35_mlp_up_proj_weight);  mul_466 = p_h_35_mlp_up_proj_weight = None
            mul_467: "f32[1, 6, 12288]" = torch.ops.aten.mul.Tensor(silu_35, linear_214);  silu_35 = linear_214 = None
            linear_215: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(mul_467, p_h_35_mlp_down_proj_weight, p_h_35_mlp_down_proj_bias);  mul_467 = p_h_35_mlp_down_proj_weight = p_h_35_mlp_down_proj_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_107: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_215, 0.0, False);  linear_215 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_215: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_213, dropout_107);  add_213 = dropout_107 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_325: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_215, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_109: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_325, 2)
            mean_72: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_109, [-1], True);  pow_109 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_216: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_72, 1e-05);  mean_72 = None
            rsqrt_72: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_216);  add_216 = None
            mul_468: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_325, rsqrt_72);  _to_copy_325 = rsqrt_72 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_326: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_468, dtype = torch.float32);  mul_468 = None
            mul_469: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_36_input_layernorm_weight, _to_copy_326);  p_h_36_input_layernorm_weight = _to_copy_326 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:404 in forward, code: hidden_states = hidden_states.transpose(1, 0)
            transpose_180: "f32[6, 1, 5120]" = torch.ops.aten.transpose.int(mul_469, 1, 0);  mul_469 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:405 in forward, code: query_layer = self.query(hidden_states)
            linear_216: "f32[6, 1, 5120]" = torch.ops.aten.linear.default(transpose_180, p_h_36_self_attention_query_weight);  p_h_36_self_attention_query_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:409 in forward, code: query_layer = query_layer.view(*new_tensor_shape)
            view_432: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(linear_216, [6, 1, 32, 160]);  linear_216 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:411 in forward, code: mixed_kv_layer = self.key_value(hidden_states)
            linear_217: "f32[6, 1, 10240]" = torch.ops.aten.linear.default(transpose_180, p_h_36_self_attention_key_value_weight);  transpose_180 = p_h_36_self_attention_key_value_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:415 in forward, code: mixed_kv_layer = mixed_kv_layer.view(*new_tensor_shape)
            view_433: "f32[6, 1, 32, 320]" = torch.ops.aten.view.default(linear_217, [6, 1, 32, 320]);  linear_217 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:381 in split_tensor_along_last_dim, code: tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)
            split_36 = torch.ops.aten.split.Tensor(view_433, 160, 3);  view_433 = None
            getitem_72: "f32[6, 1, 32, 160]" = split_36[0]
            getitem_73: "f32[6, 1, 32, 160]" = split_36[1];  split_36 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:423 in forward, code: query_layer = query_layer.view(output_size[2], output_size[0] * output_size[1], -1)
            view_434: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_432, [6, 32, -1]);  view_432 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:424 in forward, code: key_layer = key_layer.view(output_size[3], output_size[0] * output_size[1], -1)
            view_435: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_72, [6, 32, -1]);  getitem_72 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:112 in forward, code: self.inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, device=x.device).float() / self.dim))
            arange_73: "i64[80]" = torch.ops.aten.arange.start_step(0, 160, 2, device = device(type='cpu'), pin_memory = False)
            _to_copy_327: "f32[80]" = torch.ops.aten._to_copy.default(arange_73, dtype = torch.float32);  arange_73 = None
            div_36: "f32[80]" = torch.ops.aten.div.Tensor(_to_copy_327, 160);  _to_copy_327 = None
            pow_110: "f32[80]" = torch.ops.aten.pow.Scalar(30420.108888514722, div_36);  div_36 = None
            reciprocal_36: "f32[80]" = torch.ops.aten.reciprocal.default(pow_110);  pow_110 = None
            mul_470: "f32[80]" = torch.ops.aten.mul.Tensor(reciprocal_36, 1.0);  reciprocal_36 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:114 in forward, code: t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)
            arange_74: "f32[8192]" = torch.ops.aten.arange.default(8192, dtype = torch.float32, device = device(type='cpu'), pin_memory = False)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:115 in forward, code: freqs = torch.einsum('i,j->ij', t, self.inv_freq)
            einsum_72: "f32[8192, 80]" = torch.ops.aten.einsum.default('i,j->ij', [arange_74, mul_470]);  arange_74 = mul_470 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:117 in forward, code: emb = torch.cat((freqs, freqs), dim=-1).to(x.device)
            cat_108: "f32[8192, 160]" = torch.ops.aten.cat.default([einsum_72, einsum_72], -1);  einsum_72 = None
            _to_copy_328: "f32[8192, 160]" = torch.ops.aten._to_copy.default(cat_108, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  cat_108 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:121 in forward, code: self.cos_cached = self.mscale * emb.cos()[:, None, :].to(dtype)
            cos_36: "f32[8192, 160]" = torch.ops.aten.cos.default(_to_copy_328)
            slice_374: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(cos_36, 0, 0, 9223372036854775807);  cos_36 = None
            unsqueeze_80: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_374, 1);  slice_374 = None
            slice_375: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_80, 2, 0, 9223372036854775807);  unsqueeze_80 = None
            _to_copy_329: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_375, dtype = torch.float32);  slice_375 = None
            mul_471: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_329, 1.0);  _to_copy_329 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:122 in forward, code: self.sin_cached = self.mscale * emb.sin()[:, None, :].to(dtype)
            sin_36: "f32[8192, 160]" = torch.ops.aten.sin.default(_to_copy_328);  _to_copy_328 = None
            slice_376: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(sin_36, 0, 0, 9223372036854775807);  sin_36 = None
            unsqueeze_81: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_376, 1);  slice_376 = None
            slice_377: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_81, 2, 0, 9223372036854775807);  unsqueeze_81 = None
            _to_copy_330: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_377, dtype = torch.float32);  slice_377 = None
            mul_472: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_330, 1.0);  _to_copy_330 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:123 in forward, code: return self.cos_cached[:seq_len, ...], self.sin_cached[:seq_len, ...]
            alias_72: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_471);  mul_471 = None
            alias_73: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_472);  mul_472 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:133 in apply_rotary_pos_emb_torch, code: cos, sin = cos[offset:q.shape[0] + offset, ...], sin[offset:q.shape[0] + offset, ...]
            slice_378: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_72, 0, 0, 6);  alias_72 = None
            slice_379: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_73, 0, 0, 6);  alias_73 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_473: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_434, slice_378)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_380: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_434, 2, 0, 80)
            slice_381: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_434, 2, 80, 9223372036854775807);  view_434 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_72: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_381);  slice_381 = None
            cat_109: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_72, slice_380], 2);  neg_72 = slice_380 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_474: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_109, slice_379);  cat_109 = None
            add_217: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_473, mul_474);  mul_473 = mul_474 = None
            mul_475: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_435, slice_378);  slice_378 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_382: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_435, 2, 0, 80)
            slice_383: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_435, 2, 80, 9223372036854775807);  view_435 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_73: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_383);  slice_383 = None
            cat_110: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_73, slice_382], 2);  neg_73 = slice_382 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_476: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_110, slice_379);  cat_110 = slice_379 = None
            add_218: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_475, mul_476);  mul_475 = mul_476 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:448 in forward, code: query_layer = query_layer.reshape((s_query, bz, head, dim))
            view_436: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_217, [6, 1, 32, 160]);  add_217 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:449 in forward, code: key_layer = key_layer.reshape((s_key, bz, head, dim))
            view_437: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_218, [6, 1, 32, 160]);  add_218 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:458 in forward, code: query_layer = query_layer.reshape(s_query, bz * self.num_heads, dim)
            view_438: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_436, [6, 32, 160]);  view_436 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:460 in forward, code: key_layer = key_layer.reshape(s_key, bz * self.num_heads, dim)
            view_439: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_437, [6, 32, 160]);  view_437 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            transpose_181: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_438, 0, 1);  view_438 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:462 in forward, code: key_layer.transpose(0, 1).transpose(1, 2))
            transpose_182: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_439, 0, 1);  view_439 = None
            transpose_183: "f32[32, 160, 6]" = torch.ops.aten.transpose.int(transpose_182, 1, 2);  transpose_182 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            einsum_73: "f32[32, 6, 6]" = torch.ops.aten.einsum.default('bik,bkj->bij', [transpose_181, transpose_183]);  transpose_181 = transpose_183 = None
            mul_477: "f32[32, 6, 6]" = torch.ops.aten.mul.Tensor(einsum_73, 0.07905694150420949);  einsum_73 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:464 in forward, code: attention_scores = matmul_result.view(bz, self.num_heads, s_query, s_key)
            view_440: "f32[1, 32, 6, 6]" = torch.ops.aten.view.default(mul_477, [1, 32, 6, 6]);  mul_477 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:469 in forward, code: attn_weights = torch.masked_fill(attention_scores, attention_mask, torch.finfo(attention_scores.dtype).min)
            masked_fill_36: "f32[1, 32, 6, 6]" = torch.ops.aten.masked_fill.Scalar(view_440, or_1, -3.4028234663852886e+38);  view_440 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:470 in forward, code: attention_probs = F.softmax(attn_weights, dim=-1).to(input_dtype)  ##dtype = torch.float32
            softmax_36: "f32[1, 32, 6, 6]" = torch.ops.aten.softmax.int(masked_fill_36, -1);  masked_fill_36 = None
            _to_copy_331: "f32[1, 32, 6, 6]" = torch.ops.aten._to_copy.default(softmax_36, dtype = torch.float32);  softmax_36 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:471 in forward, code: attention_probs = self.attention_dropout(attention_probs)
            dropout_108: "f32[1, 32, 6, 6]" = torch.ops.aten.dropout.default(_to_copy_331, 0.0, False);  _to_copy_331 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:472 in forward, code: attention_probs_reshaped = attention_probs.view(bz * self.num_heads, s_query, s_key)
            view_441: "f32[32, 6, 6]" = torch.ops.aten.view.default(dropout_108, [32, 6, 6]);  dropout_108 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:474 in forward, code: value_layer = value_layer.reshape(s_key, bz * self.num_heads, dim)
            view_442: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_73, [6, 32, 160]);  getitem_73 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:475 in forward, code: context_layer = torch.bmm(attention_probs_reshaped, value_layer.transpose(0, 1))
            transpose_184: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_442, 0, 1);  view_442 = None
            bmm_36: "f32[32, 6, 160]" = torch.ops.aten.bmm.default(view_441, transpose_184);  view_441 = transpose_184 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:391 in _merge_heads, code: x = x.view(batch_size, self.num_heads, seq_length, self.head_dim)
            view_443: "f32[1, 32, 6, 160]" = torch.ops.aten.view.default(bmm_36, [1, 32, 6, 160]);  bmm_36 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:392 in _merge_heads, code: x = x.permute(0, 2, 1, 3)
            permute_36: "f32[1, 6, 32, 160]" = torch.ops.aten.permute.default(view_443, [0, 2, 1, 3]);  view_443 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:393 in _merge_heads, code: return x.reshape(batch_size, seq_length, self.num_heads * self.head_dim)
            clone_36: "f32[1, 6, 32, 160]" = torch.ops.aten.clone.default(permute_36, memory_format = torch.contiguous_format);  permute_36 = None
            _unsafe_view_36: "f32[1, 6, 5120]" = torch.ops.aten._unsafe_view.default(clone_36, [1, 6, 5120]);  clone_36 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:478 in forward, code: output_tensor = self.dense(context_layer)
            linear_218: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(_unsafe_view_36, p_h_36_self_attention_dense_weight, p_h_36_self_attention_dense_bias);  _unsafe_view_36 = p_h_36_self_attention_dense_weight = p_h_36_self_attention_dense_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_109: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_218, 0.0, False);  linear_218 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_219: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_215, dropout_109);  add_215 = dropout_109 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_332: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_219, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_111: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_332, 2)
            mean_73: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_111, [-1], True);  pow_111 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_220: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_73, 1e-05);  mean_73 = None
            rsqrt_73: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_220);  add_220 = None
            mul_478: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_332, rsqrt_73);  _to_copy_332 = rsqrt_73 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_333: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_478, dtype = torch.float32);  mul_478 = None
            mul_479: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_36_post_attention_layernorm_weight, _to_copy_333);  p_h_36_post_attention_layernorm_weight = _to_copy_333 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:499 in forward, code: intermediate_output = self.down_proj(F.silu(self.gate_proj(hidden_states)) * self.up_proj(hidden_states))
            linear_219: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_479, p_h_36_mlp_gate_proj_weight);  p_h_36_mlp_gate_proj_weight = None
            silu_36: "f32[1, 6, 12288]" = torch.ops.aten.silu.default(linear_219);  linear_219 = None
            linear_220: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_479, p_h_36_mlp_up_proj_weight);  mul_479 = p_h_36_mlp_up_proj_weight = None
            mul_480: "f32[1, 6, 12288]" = torch.ops.aten.mul.Tensor(silu_36, linear_220);  silu_36 = linear_220 = None
            linear_221: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(mul_480, p_h_36_mlp_down_proj_weight, p_h_36_mlp_down_proj_bias);  mul_480 = p_h_36_mlp_down_proj_weight = p_h_36_mlp_down_proj_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_110: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_221, 0.0, False);  linear_221 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_221: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_219, dropout_110);  add_219 = dropout_110 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_334: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_221, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_112: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_334, 2)
            mean_74: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_112, [-1], True);  pow_112 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_222: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_74, 1e-05);  mean_74 = None
            rsqrt_74: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_222);  add_222 = None
            mul_481: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_334, rsqrt_74);  _to_copy_334 = rsqrt_74 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_335: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_481, dtype = torch.float32);  mul_481 = None
            mul_482: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_37_input_layernorm_weight, _to_copy_335);  p_h_37_input_layernorm_weight = _to_copy_335 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:404 in forward, code: hidden_states = hidden_states.transpose(1, 0)
            transpose_185: "f32[6, 1, 5120]" = torch.ops.aten.transpose.int(mul_482, 1, 0);  mul_482 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:405 in forward, code: query_layer = self.query(hidden_states)
            linear_222: "f32[6, 1, 5120]" = torch.ops.aten.linear.default(transpose_185, p_h_37_self_attention_query_weight);  p_h_37_self_attention_query_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:409 in forward, code: query_layer = query_layer.view(*new_tensor_shape)
            view_444: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(linear_222, [6, 1, 32, 160]);  linear_222 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:411 in forward, code: mixed_kv_layer = self.key_value(hidden_states)
            linear_223: "f32[6, 1, 10240]" = torch.ops.aten.linear.default(transpose_185, p_h_37_self_attention_key_value_weight);  transpose_185 = p_h_37_self_attention_key_value_weight = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:415 in forward, code: mixed_kv_layer = mixed_kv_layer.view(*new_tensor_shape)
            view_445: "f32[6, 1, 32, 320]" = torch.ops.aten.view.default(linear_223, [6, 1, 32, 320]);  linear_223 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:381 in split_tensor_along_last_dim, code: tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)
            split_37 = torch.ops.aten.split.Tensor(view_445, 160, 3);  view_445 = None
            getitem_74: "f32[6, 1, 32, 160]" = split_37[0]
            getitem_75: "f32[6, 1, 32, 160]" = split_37[1];  split_37 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:423 in forward, code: query_layer = query_layer.view(output_size[2], output_size[0] * output_size[1], -1)
            view_446: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_444, [6, 32, -1]);  view_444 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:424 in forward, code: key_layer = key_layer.view(output_size[3], output_size[0] * output_size[1], -1)
            view_447: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_74, [6, 32, -1]);  getitem_74 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:112 in forward, code: self.inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, device=x.device).float() / self.dim))
            arange_75: "i64[80]" = torch.ops.aten.arange.start_step(0, 160, 2, device = device(type='cpu'), pin_memory = False)
            _to_copy_336: "f32[80]" = torch.ops.aten._to_copy.default(arange_75, dtype = torch.float32);  arange_75 = None
            div_37: "f32[80]" = torch.ops.aten.div.Tensor(_to_copy_336, 160);  _to_copy_336 = None
            pow_113: "f32[80]" = torch.ops.aten.pow.Scalar(30420.108888514722, div_37);  div_37 = None
            reciprocal_37: "f32[80]" = torch.ops.aten.reciprocal.default(pow_113);  pow_113 = None
            mul_483: "f32[80]" = torch.ops.aten.mul.Tensor(reciprocal_37, 1.0);  reciprocal_37 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:114 in forward, code: t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)
            arange_76: "f32[8192]" = torch.ops.aten.arange.default(8192, dtype = torch.float32, device = device(type='cpu'), pin_memory = False)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:115 in forward, code: freqs = torch.einsum('i,j->ij', t, self.inv_freq)
            einsum_74: "f32[8192, 80]" = torch.ops.aten.einsum.default('i,j->ij', [arange_76, mul_483]);  arange_76 = mul_483 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:117 in forward, code: emb = torch.cat((freqs, freqs), dim=-1).to(x.device)
            cat_111: "f32[8192, 160]" = torch.ops.aten.cat.default([einsum_74, einsum_74], -1);  einsum_74 = None
            _to_copy_337: "f32[8192, 160]" = torch.ops.aten._to_copy.default(cat_111, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  cat_111 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:121 in forward, code: self.cos_cached = self.mscale * emb.cos()[:, None, :].to(dtype)
            cos_37: "f32[8192, 160]" = torch.ops.aten.cos.default(_to_copy_337)
            slice_384: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(cos_37, 0, 0, 9223372036854775807);  cos_37 = None
            unsqueeze_82: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_384, 1);  slice_384 = None
            slice_385: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_82, 2, 0, 9223372036854775807);  unsqueeze_82 = None
            _to_copy_338: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_385, dtype = torch.float32);  slice_385 = None
            mul_484: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_338, 1.0);  _to_copy_338 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:122 in forward, code: self.sin_cached = self.mscale * emb.sin()[:, None, :].to(dtype)
            sin_37: "f32[8192, 160]" = torch.ops.aten.sin.default(_to_copy_337);  _to_copy_337 = None
            slice_386: "f32[8192, 160]" = torch.ops.aten.slice.Tensor(sin_37, 0, 0, 9223372036854775807);  sin_37 = None
            unsqueeze_83: "f32[8192, 1, 160]" = torch.ops.aten.unsqueeze.default(slice_386, 1);  slice_386 = None
            slice_387: "f32[8192, 1, 160]" = torch.ops.aten.slice.Tensor(unsqueeze_83, 2, 0, 9223372036854775807);  unsqueeze_83 = None
            _to_copy_339: "f32[8192, 1, 160]" = torch.ops.aten._to_copy.default(slice_387, dtype = torch.float32);  slice_387 = None
            mul_485: "f32[8192, 1, 160]" = torch.ops.aten.mul.Tensor(_to_copy_339, 1.0);  _to_copy_339 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:123 in forward, code: return self.cos_cached[:seq_len, ...], self.sin_cached[:seq_len, ...]
            alias_74: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_484);  mul_484 = None
            alias_75: "f32[8192, 1, 160]" = torch.ops.aten.alias.default(mul_485);  mul_485 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:133 in apply_rotary_pos_emb_torch, code: cos, sin = cos[offset:q.shape[0] + offset, ...], sin[offset:q.shape[0] + offset, ...]
            slice_388: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_74, 0, 0, 6);  alias_74 = None
            slice_389: "f32[6, 1, 160]" = torch.ops.aten.slice.Tensor(alias_75, 0, 0, 6);  alias_75 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_486: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_446, slice_388)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_390: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_446, 2, 0, 80)
            slice_391: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_446, 2, 80, 9223372036854775807);  view_446 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_74: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_391);  slice_391 = None
            cat_112: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_74, slice_390], 2);  neg_74 = slice_390 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_487: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_112, slice_389);  cat_112 = None
            add_223: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_486, mul_487);  mul_486 = mul_487 = None
            mul_488: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(view_447, slice_388);  slice_388 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:128 in rotate_half, code: x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
            slice_392: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_447, 2, 0, 80)
            slice_393: "f32[6, 32, 80]" = torch.ops.aten.slice.Tensor(view_447, 2, 80, 9223372036854775807);  view_447 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:129 in rotate_half, code: return torch.cat((-x2, x1), dim=x1.ndim - 1)  # dim=-1 triggers a bug in earlier torch versions
            neg_75: "f32[6, 32, 80]" = torch.ops.aten.neg.default(slice_393);  slice_393 = None
            cat_113: "f32[6, 32, 160]" = torch.ops.aten.cat.default([neg_75, slice_392], 2);  neg_75 = slice_392 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:134 in apply_rotary_pos_emb_torch, code: return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)
            mul_489: "f32[6, 32, 160]" = torch.ops.aten.mul.Tensor(cat_113, slice_389);  cat_113 = slice_389 = None
            add_224: "f32[6, 32, 160]" = torch.ops.aten.add.Tensor(mul_488, mul_489);  mul_488 = mul_489 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:448 in forward, code: query_layer = query_layer.reshape((s_query, bz, head, dim))
            view_448: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_223, [6, 1, 32, 160]);  add_223 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:449 in forward, code: key_layer = key_layer.reshape((s_key, bz, head, dim))
            view_449: "f32[6, 1, 32, 160]" = torch.ops.aten.view.default(add_224, [6, 1, 32, 160]);  add_224 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:458 in forward, code: query_layer = query_layer.reshape(s_query, bz * self.num_heads, dim)
            view_450: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_448, [6, 32, 160]);  view_448 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:460 in forward, code: key_layer = key_layer.reshape(s_key, bz * self.num_heads, dim)
            view_451: "f32[6, 32, 160]" = torch.ops.aten.view.default(view_449, [6, 32, 160]);  view_449 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            transpose_186: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_450, 0, 1);  view_450 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:462 in forward, code: key_layer.transpose(0, 1).transpose(1, 2))
            transpose_187: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_451, 0, 1);  view_451 = None
            transpose_188: "f32[32, 160, 6]" = torch.ops.aten.transpose.int(transpose_187, 1, 2);  transpose_187 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:461 in forward, code: matmul_result = self.inv_norm_factor * torch.einsum('bik,bkj->bij', query_layer.transpose(0, 1),
            einsum_75: "f32[32, 6, 6]" = torch.ops.aten.einsum.default('bik,bkj->bij', [transpose_186, transpose_188]);  transpose_186 = transpose_188 = None
            mul_490: "f32[32, 6, 6]" = torch.ops.aten.mul.Tensor(einsum_75, 0.07905694150420949);  einsum_75 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:464 in forward, code: attention_scores = matmul_result.view(bz, self.num_heads, s_query, s_key)
            view_452: "f32[1, 32, 6, 6]" = torch.ops.aten.view.default(mul_490, [1, 32, 6, 6]);  mul_490 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:469 in forward, code: attn_weights = torch.masked_fill(attention_scores, attention_mask, torch.finfo(attention_scores.dtype).min)
            masked_fill_37: "f32[1, 32, 6, 6]" = torch.ops.aten.masked_fill.Scalar(view_452, or_1, -3.4028234663852886e+38);  view_452 = or_1 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:470 in forward, code: attention_probs = F.softmax(attn_weights, dim=-1).to(input_dtype)  ##dtype = torch.float32
            softmax_37: "f32[1, 32, 6, 6]" = torch.ops.aten.softmax.int(masked_fill_37, -1);  masked_fill_37 = None
            _to_copy_340: "f32[1, 32, 6, 6]" = torch.ops.aten._to_copy.default(softmax_37, dtype = torch.float32);  softmax_37 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:471 in forward, code: attention_probs = self.attention_dropout(attention_probs)
            dropout_111: "f32[1, 32, 6, 6]" = torch.ops.aten.dropout.default(_to_copy_340, 0.0, False);  _to_copy_340 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:472 in forward, code: attention_probs_reshaped = attention_probs.view(bz * self.num_heads, s_query, s_key)
            view_453: "f32[32, 6, 6]" = torch.ops.aten.view.default(dropout_111, [32, 6, 6]);  dropout_111 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:474 in forward, code: value_layer = value_layer.reshape(s_key, bz * self.num_heads, dim)
            view_454: "f32[6, 32, 160]" = torch.ops.aten.view.default(getitem_75, [6, 32, 160]);  getitem_75 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:475 in forward, code: context_layer = torch.bmm(attention_probs_reshaped, value_layer.transpose(0, 1))
            transpose_189: "f32[32, 6, 160]" = torch.ops.aten.transpose.int(view_454, 0, 1);  view_454 = None
            bmm_37: "f32[32, 6, 160]" = torch.ops.aten.bmm.default(view_453, transpose_189);  view_453 = transpose_189 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:391 in _merge_heads, code: x = x.view(batch_size, self.num_heads, seq_length, self.head_dim)
            view_455: "f32[1, 32, 6, 160]" = torch.ops.aten.view.default(bmm_37, [1, 32, 6, 160]);  bmm_37 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:392 in _merge_heads, code: x = x.permute(0, 2, 1, 3)
            permute_37: "f32[1, 6, 32, 160]" = torch.ops.aten.permute.default(view_455, [0, 2, 1, 3]);  view_455 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:393 in _merge_heads, code: return x.reshape(batch_size, seq_length, self.num_heads * self.head_dim)
            clone_37: "f32[1, 6, 32, 160]" = torch.ops.aten.clone.default(permute_37, memory_format = torch.contiguous_format);  permute_37 = None
            _unsafe_view_37: "f32[1, 6, 5120]" = torch.ops.aten._unsafe_view.default(clone_37, [1, 6, 5120]);  clone_37 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:478 in forward, code: output_tensor = self.dense(context_layer)
            linear_224: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(_unsafe_view_37, p_h_37_self_attention_dense_weight, p_h_37_self_attention_dense_bias);  _unsafe_view_37 = p_h_37_self_attention_dense_weight = p_h_37_self_attention_dense_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_112: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_224, 0.0, False);  linear_224 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_225: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_221, dropout_112);  add_221 = dropout_112 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_341: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_225, dtype = torch.float32)
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_114: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_341, 2)
            mean_75: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_114, [-1], True);  pow_114 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_226: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_75, 1e-05);  mean_75 = None
            rsqrt_75: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_226);  add_226 = None
            mul_491: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_341, rsqrt_75);  _to_copy_341 = rsqrt_75 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_342: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_491, dtype = torch.float32);  mul_491 = None
            mul_492: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_h_37_post_attention_layernorm_weight, _to_copy_342);  p_h_37_post_attention_layernorm_weight = _to_copy_342 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:499 in forward, code: intermediate_output = self.down_proj(F.silu(self.gate_proj(hidden_states)) * self.up_proj(hidden_states))
            linear_225: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_492, p_h_37_mlp_gate_proj_weight);  p_h_37_mlp_gate_proj_weight = None
            silu_37: "f32[1, 6, 12288]" = torch.ops.aten.silu.default(linear_225);  linear_225 = None
            linear_226: "f32[1, 6, 12288]" = torch.ops.aten.linear.default(mul_492, p_h_37_mlp_up_proj_weight);  mul_492 = p_h_37_mlp_up_proj_weight = None
            mul_493: "f32[1, 6, 12288]" = torch.ops.aten.mul.Tensor(silu_37, linear_226);  silu_37 = linear_226 = None
            linear_227: "f32[1, 6, 5120]" = torch.ops.aten.linear.default(mul_493, p_h_37_mlp_down_proj_weight, p_h_37_mlp_down_proj_bias);  mul_493 = p_h_37_mlp_down_proj_weight = p_h_37_mlp_down_proj_bias = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:258 in dropout_add, code: out = F.dropout(x, p=prob, training=training)
            dropout_113: "f32[1, 6, 5120]" = torch.ops.aten.dropout.default(linear_227, 0.0, False);  linear_227 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:259 in dropout_add, code: out = residual + out
            add_227: "f32[1, 6, 5120]" = torch.ops.aten.add.Tensor(add_225, dropout_113);  add_225 = dropout_113 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:146 in forward, code: hidden_states = hidden_states.to(torch.float32)
            _to_copy_343: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(add_227, dtype = torch.float32);  add_227 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:147 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
            pow_115: "f32[1, 6, 5120]" = torch.ops.aten.pow.Tensor_Scalar(_to_copy_343, 2)
            mean_76: "f32[1, 6, 1]" = torch.ops.aten.mean.dim(pow_115, [-1], True);  pow_115 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:148 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
            add_228: "f32[1, 6, 1]" = torch.ops.aten.add.Tensor(mean_76, 1e-05);  mean_76 = None
            rsqrt_76: "f32[1, 6, 1]" = torch.ops.aten.rsqrt.default(add_228);  add_228 = None
            mul_494: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(_to_copy_343, rsqrt_76);  _to_copy_343 = rsqrt_76 = None
            
             # File: /home/liuyang/project/rvbisai/telechat_src/modeling_telechat.py:149 in forward, code: return self.weight * hidden_states.to(input_dtype)
            _to_copy_344: "f32[1, 6, 5120]" = torch.ops.aten._to_copy.default(mul_494, dtype = torch.float32);  mul_494 = None
            mul_495: "f32[1, 6, 5120]" = torch.ops.aten.mul.Tensor(p_ln_f_weight, _to_copy_344);  p_ln_f_weight = _to_copy_344 = None
            return (mul_495,)
            
Graph signature: ExportGraphSignature(input_specs=[InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_0_input_layernorm_weight'), target='h.0.input_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_0_post_attention_layernorm_weight'), target='h.0.post_attention_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_1_input_layernorm_weight'), target='h.1.input_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_1_post_attention_layernorm_weight'), target='h.1.post_attention_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_2_input_layernorm_weight'), target='h.2.input_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_2_post_attention_layernorm_weight'), target='h.2.post_attention_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_3_input_layernorm_weight'), target='h.3.input_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_3_post_attention_layernorm_weight'), target='h.3.post_attention_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_4_input_layernorm_weight'), target='h.4.input_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_4_post_attention_layernorm_weight'), target='h.4.post_attention_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_5_input_layernorm_weight'), target='h.5.input_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_5_post_attention_layernorm_weight'), target='h.5.post_attention_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_6_input_layernorm_weight'), target='h.6.input_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_6_post_attention_layernorm_weight'), target='h.6.post_attention_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_7_input_layernorm_weight'), target='h.7.input_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_7_post_attention_layernorm_weight'), target='h.7.post_attention_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_8_input_layernorm_weight'), target='h.8.input_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_8_post_attention_layernorm_weight'), target='h.8.post_attention_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_9_input_layernorm_weight'), target='h.9.input_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_9_post_attention_layernorm_weight'), target='h.9.post_attention_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_10_input_layernorm_weight'), target='h.10.input_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_10_post_attention_layernorm_weight'), target='h.10.post_attention_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_11_input_layernorm_weight'), target='h.11.input_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_11_post_attention_layernorm_weight'), target='h.11.post_attention_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_12_input_layernorm_weight'), target='h.12.input_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_12_post_attention_layernorm_weight'), target='h.12.post_attention_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_13_input_layernorm_weight'), target='h.13.input_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_13_post_attention_layernorm_weight'), target='h.13.post_attention_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_14_input_layernorm_weight'), target='h.14.input_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_14_post_attention_layernorm_weight'), target='h.14.post_attention_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_15_input_layernorm_weight'), target='h.15.input_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_15_post_attention_layernorm_weight'), target='h.15.post_attention_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_16_input_layernorm_weight'), target='h.16.input_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_16_post_attention_layernorm_weight'), target='h.16.post_attention_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_17_input_layernorm_weight'), target='h.17.input_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_17_post_attention_layernorm_weight'), target='h.17.post_attention_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_18_input_layernorm_weight'), target='h.18.input_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_18_post_attention_layernorm_weight'), target='h.18.post_attention_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_19_input_layernorm_weight'), target='h.19.input_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_19_post_attention_layernorm_weight'), target='h.19.post_attention_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_20_input_layernorm_weight'), target='h.20.input_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_20_post_attention_layernorm_weight'), target='h.20.post_attention_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_21_input_layernorm_weight'), target='h.21.input_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_21_post_attention_layernorm_weight'), target='h.21.post_attention_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_22_input_layernorm_weight'), target='h.22.input_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_22_post_attention_layernorm_weight'), target='h.22.post_attention_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_23_input_layernorm_weight'), target='h.23.input_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_23_post_attention_layernorm_weight'), target='h.23.post_attention_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_24_input_layernorm_weight'), target='h.24.input_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_24_post_attention_layernorm_weight'), target='h.24.post_attention_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_25_input_layernorm_weight'), target='h.25.input_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_25_post_attention_layernorm_weight'), target='h.25.post_attention_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_26_input_layernorm_weight'), target='h.26.input_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_26_post_attention_layernorm_weight'), target='h.26.post_attention_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_27_input_layernorm_weight'), target='h.27.input_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_27_post_attention_layernorm_weight'), target='h.27.post_attention_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_28_input_layernorm_weight'), target='h.28.input_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_28_post_attention_layernorm_weight'), target='h.28.post_attention_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_29_input_layernorm_weight'), target='h.29.input_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_29_post_attention_layernorm_weight'), target='h.29.post_attention_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_30_input_layernorm_weight'), target='h.30.input_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_30_post_attention_layernorm_weight'), target='h.30.post_attention_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_31_input_layernorm_weight'), target='h.31.input_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_31_post_attention_layernorm_weight'), target='h.31.post_attention_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_32_input_layernorm_weight'), target='h.32.input_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_32_post_attention_layernorm_weight'), target='h.32.post_attention_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_33_input_layernorm_weight'), target='h.33.input_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_33_post_attention_layernorm_weight'), target='h.33.post_attention_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_34_input_layernorm_weight'), target='h.34.input_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_34_post_attention_layernorm_weight'), target='h.34.post_attention_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_35_input_layernorm_weight'), target='h.35.input_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_35_post_attention_layernorm_weight'), target='h.35.post_attention_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_36_input_layernorm_weight'), target='h.36.input_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_36_post_attention_layernorm_weight'), target='h.36.post_attention_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_37_input_layernorm_weight'), target='h.37.input_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_37_post_attention_layernorm_weight'), target='h.37.post_attention_layernorm.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_ln_f_weight'), target='ln_f.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_word_embeddings_weight'), target='word_embeddings.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_0_self_attention_query_weight'), target='h.0.self_attention.query.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_0_self_attention_key_value_weight'), target='h.0.self_attention.key_value.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_0_self_attention_dense_weight'), target='h.0.self_attention.dense.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_0_self_attention_dense_bias'), target='h.0.self_attention.dense.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_0_mlp_gate_proj_weight'), target='h.0.mlp.gate_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_0_mlp_up_proj_weight'), target='h.0.mlp.up_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_0_mlp_down_proj_weight'), target='h.0.mlp.down_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_0_mlp_down_proj_bias'), target='h.0.mlp.down_proj.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_1_self_attention_query_weight'), target='h.1.self_attention.query.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_1_self_attention_key_value_weight'), target='h.1.self_attention.key_value.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_1_self_attention_dense_weight'), target='h.1.self_attention.dense.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_1_self_attention_dense_bias'), target='h.1.self_attention.dense.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_1_mlp_gate_proj_weight'), target='h.1.mlp.gate_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_1_mlp_up_proj_weight'), target='h.1.mlp.up_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_1_mlp_down_proj_weight'), target='h.1.mlp.down_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_1_mlp_down_proj_bias'), target='h.1.mlp.down_proj.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_2_self_attention_query_weight'), target='h.2.self_attention.query.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_2_self_attention_key_value_weight'), target='h.2.self_attention.key_value.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_2_self_attention_dense_weight'), target='h.2.self_attention.dense.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_2_self_attention_dense_bias'), target='h.2.self_attention.dense.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_2_mlp_gate_proj_weight'), target='h.2.mlp.gate_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_2_mlp_up_proj_weight'), target='h.2.mlp.up_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_2_mlp_down_proj_weight'), target='h.2.mlp.down_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_2_mlp_down_proj_bias'), target='h.2.mlp.down_proj.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_3_self_attention_query_weight'), target='h.3.self_attention.query.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_3_self_attention_key_value_weight'), target='h.3.self_attention.key_value.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_3_self_attention_dense_weight'), target='h.3.self_attention.dense.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_3_self_attention_dense_bias'), target='h.3.self_attention.dense.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_3_mlp_gate_proj_weight'), target='h.3.mlp.gate_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_3_mlp_up_proj_weight'), target='h.3.mlp.up_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_3_mlp_down_proj_weight'), target='h.3.mlp.down_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_3_mlp_down_proj_bias'), target='h.3.mlp.down_proj.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_4_self_attention_query_weight'), target='h.4.self_attention.query.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_4_self_attention_key_value_weight'), target='h.4.self_attention.key_value.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_4_self_attention_dense_weight'), target='h.4.self_attention.dense.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_4_self_attention_dense_bias'), target='h.4.self_attention.dense.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_4_mlp_gate_proj_weight'), target='h.4.mlp.gate_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_4_mlp_up_proj_weight'), target='h.4.mlp.up_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_4_mlp_down_proj_weight'), target='h.4.mlp.down_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_4_mlp_down_proj_bias'), target='h.4.mlp.down_proj.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_5_self_attention_query_weight'), target='h.5.self_attention.query.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_5_self_attention_key_value_weight'), target='h.5.self_attention.key_value.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_5_self_attention_dense_weight'), target='h.5.self_attention.dense.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_5_self_attention_dense_bias'), target='h.5.self_attention.dense.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_5_mlp_gate_proj_weight'), target='h.5.mlp.gate_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_5_mlp_up_proj_weight'), target='h.5.mlp.up_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_5_mlp_down_proj_weight'), target='h.5.mlp.down_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_5_mlp_down_proj_bias'), target='h.5.mlp.down_proj.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_6_self_attention_query_weight'), target='h.6.self_attention.query.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_6_self_attention_key_value_weight'), target='h.6.self_attention.key_value.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_6_self_attention_dense_weight'), target='h.6.self_attention.dense.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_6_self_attention_dense_bias'), target='h.6.self_attention.dense.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_6_mlp_gate_proj_weight'), target='h.6.mlp.gate_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_6_mlp_up_proj_weight'), target='h.6.mlp.up_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_6_mlp_down_proj_weight'), target='h.6.mlp.down_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_6_mlp_down_proj_bias'), target='h.6.mlp.down_proj.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_7_self_attention_query_weight'), target='h.7.self_attention.query.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_7_self_attention_key_value_weight'), target='h.7.self_attention.key_value.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_7_self_attention_dense_weight'), target='h.7.self_attention.dense.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_7_self_attention_dense_bias'), target='h.7.self_attention.dense.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_7_mlp_gate_proj_weight'), target='h.7.mlp.gate_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_7_mlp_up_proj_weight'), target='h.7.mlp.up_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_7_mlp_down_proj_weight'), target='h.7.mlp.down_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_7_mlp_down_proj_bias'), target='h.7.mlp.down_proj.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_8_self_attention_query_weight'), target='h.8.self_attention.query.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_8_self_attention_key_value_weight'), target='h.8.self_attention.key_value.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_8_self_attention_dense_weight'), target='h.8.self_attention.dense.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_8_self_attention_dense_bias'), target='h.8.self_attention.dense.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_8_mlp_gate_proj_weight'), target='h.8.mlp.gate_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_8_mlp_up_proj_weight'), target='h.8.mlp.up_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_8_mlp_down_proj_weight'), target='h.8.mlp.down_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_8_mlp_down_proj_bias'), target='h.8.mlp.down_proj.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_9_self_attention_query_weight'), target='h.9.self_attention.query.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_9_self_attention_key_value_weight'), target='h.9.self_attention.key_value.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_9_self_attention_dense_weight'), target='h.9.self_attention.dense.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_9_self_attention_dense_bias'), target='h.9.self_attention.dense.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_9_mlp_gate_proj_weight'), target='h.9.mlp.gate_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_9_mlp_up_proj_weight'), target='h.9.mlp.up_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_9_mlp_down_proj_weight'), target='h.9.mlp.down_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_9_mlp_down_proj_bias'), target='h.9.mlp.down_proj.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_10_self_attention_query_weight'), target='h.10.self_attention.query.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_10_self_attention_key_value_weight'), target='h.10.self_attention.key_value.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_10_self_attention_dense_weight'), target='h.10.self_attention.dense.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_10_self_attention_dense_bias'), target='h.10.self_attention.dense.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_10_mlp_gate_proj_weight'), target='h.10.mlp.gate_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_10_mlp_up_proj_weight'), target='h.10.mlp.up_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_10_mlp_down_proj_weight'), target='h.10.mlp.down_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_10_mlp_down_proj_bias'), target='h.10.mlp.down_proj.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_11_self_attention_query_weight'), target='h.11.self_attention.query.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_11_self_attention_key_value_weight'), target='h.11.self_attention.key_value.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_11_self_attention_dense_weight'), target='h.11.self_attention.dense.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_11_self_attention_dense_bias'), target='h.11.self_attention.dense.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_11_mlp_gate_proj_weight'), target='h.11.mlp.gate_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_11_mlp_up_proj_weight'), target='h.11.mlp.up_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_11_mlp_down_proj_weight'), target='h.11.mlp.down_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_11_mlp_down_proj_bias'), target='h.11.mlp.down_proj.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_12_self_attention_query_weight'), target='h.12.self_attention.query.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_12_self_attention_key_value_weight'), target='h.12.self_attention.key_value.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_12_self_attention_dense_weight'), target='h.12.self_attention.dense.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_12_self_attention_dense_bias'), target='h.12.self_attention.dense.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_12_mlp_gate_proj_weight'), target='h.12.mlp.gate_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_12_mlp_up_proj_weight'), target='h.12.mlp.up_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_12_mlp_down_proj_weight'), target='h.12.mlp.down_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_12_mlp_down_proj_bias'), target='h.12.mlp.down_proj.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_13_self_attention_query_weight'), target='h.13.self_attention.query.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_13_self_attention_key_value_weight'), target='h.13.self_attention.key_value.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_13_self_attention_dense_weight'), target='h.13.self_attention.dense.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_13_self_attention_dense_bias'), target='h.13.self_attention.dense.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_13_mlp_gate_proj_weight'), target='h.13.mlp.gate_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_13_mlp_up_proj_weight'), target='h.13.mlp.up_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_13_mlp_down_proj_weight'), target='h.13.mlp.down_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_13_mlp_down_proj_bias'), target='h.13.mlp.down_proj.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_14_self_attention_query_weight'), target='h.14.self_attention.query.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_14_self_attention_key_value_weight'), target='h.14.self_attention.key_value.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_14_self_attention_dense_weight'), target='h.14.self_attention.dense.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_14_self_attention_dense_bias'), target='h.14.self_attention.dense.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_14_mlp_gate_proj_weight'), target='h.14.mlp.gate_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_14_mlp_up_proj_weight'), target='h.14.mlp.up_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_14_mlp_down_proj_weight'), target='h.14.mlp.down_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_14_mlp_down_proj_bias'), target='h.14.mlp.down_proj.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_15_self_attention_query_weight'), target='h.15.self_attention.query.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_15_self_attention_key_value_weight'), target='h.15.self_attention.key_value.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_15_self_attention_dense_weight'), target='h.15.self_attention.dense.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_15_self_attention_dense_bias'), target='h.15.self_attention.dense.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_15_mlp_gate_proj_weight'), target='h.15.mlp.gate_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_15_mlp_up_proj_weight'), target='h.15.mlp.up_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_15_mlp_down_proj_weight'), target='h.15.mlp.down_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_15_mlp_down_proj_bias'), target='h.15.mlp.down_proj.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_16_self_attention_query_weight'), target='h.16.self_attention.query.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_16_self_attention_key_value_weight'), target='h.16.self_attention.key_value.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_16_self_attention_dense_weight'), target='h.16.self_attention.dense.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_16_self_attention_dense_bias'), target='h.16.self_attention.dense.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_16_mlp_gate_proj_weight'), target='h.16.mlp.gate_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_16_mlp_up_proj_weight'), target='h.16.mlp.up_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_16_mlp_down_proj_weight'), target='h.16.mlp.down_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_16_mlp_down_proj_bias'), target='h.16.mlp.down_proj.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_17_self_attention_query_weight'), target='h.17.self_attention.query.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_17_self_attention_key_value_weight'), target='h.17.self_attention.key_value.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_17_self_attention_dense_weight'), target='h.17.self_attention.dense.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_17_self_attention_dense_bias'), target='h.17.self_attention.dense.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_17_mlp_gate_proj_weight'), target='h.17.mlp.gate_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_17_mlp_up_proj_weight'), target='h.17.mlp.up_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_17_mlp_down_proj_weight'), target='h.17.mlp.down_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_17_mlp_down_proj_bias'), target='h.17.mlp.down_proj.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_18_self_attention_query_weight'), target='h.18.self_attention.query.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_18_self_attention_key_value_weight'), target='h.18.self_attention.key_value.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_18_self_attention_dense_weight'), target='h.18.self_attention.dense.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_18_self_attention_dense_bias'), target='h.18.self_attention.dense.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_18_mlp_gate_proj_weight'), target='h.18.mlp.gate_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_18_mlp_up_proj_weight'), target='h.18.mlp.up_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_18_mlp_down_proj_weight'), target='h.18.mlp.down_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_18_mlp_down_proj_bias'), target='h.18.mlp.down_proj.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_19_self_attention_query_weight'), target='h.19.self_attention.query.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_19_self_attention_key_value_weight'), target='h.19.self_attention.key_value.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_19_self_attention_dense_weight'), target='h.19.self_attention.dense.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_19_self_attention_dense_bias'), target='h.19.self_attention.dense.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_19_mlp_gate_proj_weight'), target='h.19.mlp.gate_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_19_mlp_up_proj_weight'), target='h.19.mlp.up_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_19_mlp_down_proj_weight'), target='h.19.mlp.down_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_19_mlp_down_proj_bias'), target='h.19.mlp.down_proj.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_20_self_attention_query_weight'), target='h.20.self_attention.query.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_20_self_attention_key_value_weight'), target='h.20.self_attention.key_value.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_20_self_attention_dense_weight'), target='h.20.self_attention.dense.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_20_self_attention_dense_bias'), target='h.20.self_attention.dense.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_20_mlp_gate_proj_weight'), target='h.20.mlp.gate_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_20_mlp_up_proj_weight'), target='h.20.mlp.up_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_20_mlp_down_proj_weight'), target='h.20.mlp.down_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_20_mlp_down_proj_bias'), target='h.20.mlp.down_proj.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_21_self_attention_query_weight'), target='h.21.self_attention.query.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_21_self_attention_key_value_weight'), target='h.21.self_attention.key_value.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_21_self_attention_dense_weight'), target='h.21.self_attention.dense.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_21_self_attention_dense_bias'), target='h.21.self_attention.dense.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_21_mlp_gate_proj_weight'), target='h.21.mlp.gate_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_21_mlp_up_proj_weight'), target='h.21.mlp.up_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_21_mlp_down_proj_weight'), target='h.21.mlp.down_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_21_mlp_down_proj_bias'), target='h.21.mlp.down_proj.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_22_self_attention_query_weight'), target='h.22.self_attention.query.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_22_self_attention_key_value_weight'), target='h.22.self_attention.key_value.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_22_self_attention_dense_weight'), target='h.22.self_attention.dense.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_22_self_attention_dense_bias'), target='h.22.self_attention.dense.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_22_mlp_gate_proj_weight'), target='h.22.mlp.gate_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_22_mlp_up_proj_weight'), target='h.22.mlp.up_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_22_mlp_down_proj_weight'), target='h.22.mlp.down_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_22_mlp_down_proj_bias'), target='h.22.mlp.down_proj.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_23_self_attention_query_weight'), target='h.23.self_attention.query.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_23_self_attention_key_value_weight'), target='h.23.self_attention.key_value.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_23_self_attention_dense_weight'), target='h.23.self_attention.dense.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_23_self_attention_dense_bias'), target='h.23.self_attention.dense.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_23_mlp_gate_proj_weight'), target='h.23.mlp.gate_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_23_mlp_up_proj_weight'), target='h.23.mlp.up_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_23_mlp_down_proj_weight'), target='h.23.mlp.down_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_23_mlp_down_proj_bias'), target='h.23.mlp.down_proj.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_24_self_attention_query_weight'), target='h.24.self_attention.query.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_24_self_attention_key_value_weight'), target='h.24.self_attention.key_value.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_24_self_attention_dense_weight'), target='h.24.self_attention.dense.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_24_self_attention_dense_bias'), target='h.24.self_attention.dense.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_24_mlp_gate_proj_weight'), target='h.24.mlp.gate_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_24_mlp_up_proj_weight'), target='h.24.mlp.up_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_24_mlp_down_proj_weight'), target='h.24.mlp.down_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_24_mlp_down_proj_bias'), target='h.24.mlp.down_proj.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_25_self_attention_query_weight'), target='h.25.self_attention.query.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_25_self_attention_key_value_weight'), target='h.25.self_attention.key_value.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_25_self_attention_dense_weight'), target='h.25.self_attention.dense.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_25_self_attention_dense_bias'), target='h.25.self_attention.dense.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_25_mlp_gate_proj_weight'), target='h.25.mlp.gate_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_25_mlp_up_proj_weight'), target='h.25.mlp.up_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_25_mlp_down_proj_weight'), target='h.25.mlp.down_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_25_mlp_down_proj_bias'), target='h.25.mlp.down_proj.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_26_self_attention_query_weight'), target='h.26.self_attention.query.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_26_self_attention_key_value_weight'), target='h.26.self_attention.key_value.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_26_self_attention_dense_weight'), target='h.26.self_attention.dense.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_26_self_attention_dense_bias'), target='h.26.self_attention.dense.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_26_mlp_gate_proj_weight'), target='h.26.mlp.gate_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_26_mlp_up_proj_weight'), target='h.26.mlp.up_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_26_mlp_down_proj_weight'), target='h.26.mlp.down_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_26_mlp_down_proj_bias'), target='h.26.mlp.down_proj.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_27_self_attention_query_weight'), target='h.27.self_attention.query.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_27_self_attention_key_value_weight'), target='h.27.self_attention.key_value.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_27_self_attention_dense_weight'), target='h.27.self_attention.dense.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_27_self_attention_dense_bias'), target='h.27.self_attention.dense.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_27_mlp_gate_proj_weight'), target='h.27.mlp.gate_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_27_mlp_up_proj_weight'), target='h.27.mlp.up_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_27_mlp_down_proj_weight'), target='h.27.mlp.down_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_27_mlp_down_proj_bias'), target='h.27.mlp.down_proj.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_28_self_attention_query_weight'), target='h.28.self_attention.query.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_28_self_attention_key_value_weight'), target='h.28.self_attention.key_value.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_28_self_attention_dense_weight'), target='h.28.self_attention.dense.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_28_self_attention_dense_bias'), target='h.28.self_attention.dense.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_28_mlp_gate_proj_weight'), target='h.28.mlp.gate_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_28_mlp_up_proj_weight'), target='h.28.mlp.up_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_28_mlp_down_proj_weight'), target='h.28.mlp.down_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_28_mlp_down_proj_bias'), target='h.28.mlp.down_proj.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_29_self_attention_query_weight'), target='h.29.self_attention.query.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_29_self_attention_key_value_weight'), target='h.29.self_attention.key_value.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_29_self_attention_dense_weight'), target='h.29.self_attention.dense.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_29_self_attention_dense_bias'), target='h.29.self_attention.dense.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_29_mlp_gate_proj_weight'), target='h.29.mlp.gate_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_29_mlp_up_proj_weight'), target='h.29.mlp.up_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_29_mlp_down_proj_weight'), target='h.29.mlp.down_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_29_mlp_down_proj_bias'), target='h.29.mlp.down_proj.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_30_self_attention_query_weight'), target='h.30.self_attention.query.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_30_self_attention_key_value_weight'), target='h.30.self_attention.key_value.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_30_self_attention_dense_weight'), target='h.30.self_attention.dense.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_30_self_attention_dense_bias'), target='h.30.self_attention.dense.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_30_mlp_gate_proj_weight'), target='h.30.mlp.gate_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_30_mlp_up_proj_weight'), target='h.30.mlp.up_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_30_mlp_down_proj_weight'), target='h.30.mlp.down_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_30_mlp_down_proj_bias'), target='h.30.mlp.down_proj.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_31_self_attention_query_weight'), target='h.31.self_attention.query.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_31_self_attention_key_value_weight'), target='h.31.self_attention.key_value.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_31_self_attention_dense_weight'), target='h.31.self_attention.dense.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_31_self_attention_dense_bias'), target='h.31.self_attention.dense.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_31_mlp_gate_proj_weight'), target='h.31.mlp.gate_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_31_mlp_up_proj_weight'), target='h.31.mlp.up_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_31_mlp_down_proj_weight'), target='h.31.mlp.down_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_31_mlp_down_proj_bias'), target='h.31.mlp.down_proj.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_32_self_attention_query_weight'), target='h.32.self_attention.query.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_32_self_attention_key_value_weight'), target='h.32.self_attention.key_value.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_32_self_attention_dense_weight'), target='h.32.self_attention.dense.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_32_self_attention_dense_bias'), target='h.32.self_attention.dense.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_32_mlp_gate_proj_weight'), target='h.32.mlp.gate_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_32_mlp_up_proj_weight'), target='h.32.mlp.up_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_32_mlp_down_proj_weight'), target='h.32.mlp.down_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_32_mlp_down_proj_bias'), target='h.32.mlp.down_proj.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_33_self_attention_query_weight'), target='h.33.self_attention.query.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_33_self_attention_key_value_weight'), target='h.33.self_attention.key_value.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_33_self_attention_dense_weight'), target='h.33.self_attention.dense.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_33_self_attention_dense_bias'), target='h.33.self_attention.dense.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_33_mlp_gate_proj_weight'), target='h.33.mlp.gate_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_33_mlp_up_proj_weight'), target='h.33.mlp.up_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_33_mlp_down_proj_weight'), target='h.33.mlp.down_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_33_mlp_down_proj_bias'), target='h.33.mlp.down_proj.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_34_self_attention_query_weight'), target='h.34.self_attention.query.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_34_self_attention_key_value_weight'), target='h.34.self_attention.key_value.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_34_self_attention_dense_weight'), target='h.34.self_attention.dense.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_34_self_attention_dense_bias'), target='h.34.self_attention.dense.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_34_mlp_gate_proj_weight'), target='h.34.mlp.gate_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_34_mlp_up_proj_weight'), target='h.34.mlp.up_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_34_mlp_down_proj_weight'), target='h.34.mlp.down_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_34_mlp_down_proj_bias'), target='h.34.mlp.down_proj.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_35_self_attention_query_weight'), target='h.35.self_attention.query.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_35_self_attention_key_value_weight'), target='h.35.self_attention.key_value.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_35_self_attention_dense_weight'), target='h.35.self_attention.dense.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_35_self_attention_dense_bias'), target='h.35.self_attention.dense.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_35_mlp_gate_proj_weight'), target='h.35.mlp.gate_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_35_mlp_up_proj_weight'), target='h.35.mlp.up_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_35_mlp_down_proj_weight'), target='h.35.mlp.down_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_35_mlp_down_proj_bias'), target='h.35.mlp.down_proj.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_36_self_attention_query_weight'), target='h.36.self_attention.query.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_36_self_attention_key_value_weight'), target='h.36.self_attention.key_value.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_36_self_attention_dense_weight'), target='h.36.self_attention.dense.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_36_self_attention_dense_bias'), target='h.36.self_attention.dense.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_36_mlp_gate_proj_weight'), target='h.36.mlp.gate_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_36_mlp_up_proj_weight'), target='h.36.mlp.up_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_36_mlp_down_proj_weight'), target='h.36.mlp.down_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_36_mlp_down_proj_bias'), target='h.36.mlp.down_proj.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_37_self_attention_query_weight'), target='h.37.self_attention.query.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_37_self_attention_key_value_weight'), target='h.37.self_attention.key_value.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_37_self_attention_dense_weight'), target='h.37.self_attention.dense.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_37_self_attention_dense_bias'), target='h.37.self_attention.dense.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_37_mlp_gate_proj_weight'), target='h.37.mlp.gate_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_37_mlp_up_proj_weight'), target='h.37.mlp.up_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_37_mlp_down_proj_weight'), target='h.37.mlp.down_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_h_37_mlp_down_proj_bias'), target='h.37.mlp.down_proj.bias', persistent=None), InputSpec(kind=<InputKind.CONSTANT_TENSOR: 4>, arg=TensorArgument(name='c_lifted_tensor_0'), target='lifted_tensor_0', persistent=None), InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='input_ids'), target=None, persistent=None), InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='past_key_values'), target=None, persistent=None)], output_specs=[OutputSpec(kind=<OutputKind.USER_OUTPUT: 1>, arg=TensorArgument(name='mul_495'), target=None)])
Range constraints: {}

